{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd33b02",
   "metadata": {},
   "source": [
    "Read sample table from Terra workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dalmatian\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "namespace = \"broad-firecloud-ccle\"\n",
    "workspaces = [\"DepMap_WES_CN_hg38\", \"DepMap_WGS_CN\"]\n",
    "dest_dataset = \"depmap-omics.maf_staging_0916\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac52eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Transfer:\n",
    "    srcs : str\n",
    "    dest_table : str\n",
    "    cds_id : str\n",
    "\n",
    "def get_transfers(workspace):\n",
    "    wm = dalmatian.WorkspaceManager(f\"{namespace}/{workspace}\")\n",
    "\n",
    "    sample = wm.get_entities(\"sample\")\n",
    "    sample = sample.reset_index()\n",
    "\n",
    "    transfers = []\n",
    "    for rec in sample.to_dict(\"records\"):\n",
    "        if isinstance(rec['full_file'], list):\n",
    "            dest_table = f\"{dest_dataset}.stage_maf_{rec['sample_id'].replace('-', '_')}\"\n",
    "            transfers.append(Transfer(rec['full_file'], dest_table, rec[\"sample_id\"]))\n",
    "\n",
    "    return transfers\n",
    "\n",
    "transfers = []\n",
    "for workspace in workspaces:\n",
    "    transfers.extend(get_transfers(workspace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00899f91-644e-446a-a20e-f9665a7f1bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transfers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060ef69",
   "metadata": {},
   "source": [
    "Create \"external\" tables, one per cds_id from the associated uris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "import re\n",
    "\n",
    "from google.api_core.exceptions import Conflict\n",
    "\n",
    "def create_ext_table(srcs, dest_table, job_prefix):\n",
    "    table = bigquery.Table(dest_table)\n",
    "\n",
    "    external_config = bigquery.ExternalConfig(bigquery.external_config.ExternalSourceFormat.PARQUET)\n",
    "    external_config.source_uris = srcs\n",
    "    table.external_data_configuration = external_config\n",
    "\n",
    "    client.create_table(table, exists_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53533c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def cleanup_uris(uris):\n",
    "    result = []\n",
    "    for uri in uris:\n",
    "        result.extend([x.strip() for x in uri.split(\",\")])\n",
    "    return list(set(result))\n",
    "\n",
    "#transfers = transfers[:10]\n",
    "\n",
    "# create a bunch of tables which correspond to CDS_IDs, because we want to add CDS_ID as a column\n",
    "for transfer in tqdm(transfers):\n",
    "    uris = cleanup_uris(transfer.srcs) # at least one row has a entry which looks like a string containing a comma seperated list instead of a real list\n",
    "    create_ext_table(uris, transfer.dest_table, \"t2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64de8e-fe81-4be0-92ef-905c1f093019",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = client.get_table(table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae9b4d-654d-409e-b5c3-858c18b363cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609ecde",
   "metadata": {},
   "source": [
    "Copy from the external table into a single table adding the cds_id to each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c502fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def create_batches(data, batch_size):\n",
    "    return [data[x:x+batch_size] for x in range(0, len(data), batch_size)]\n",
    "\n",
    "# Concatenate table adding CDS_ID to the dest table\n",
    "def concatenate_tables(dest_table, transfers, job_prefix, parallelism):\n",
    "    create_table_stmt = f\"create table if not exists {dest_table} as select 'invalid' CDS_ID, * from `{transfers[0].dest_table}` limit 0\"\n",
    "    job = client.query(create_table_stmt)\n",
    "    job.result() # wait for completion\n",
    "\n",
    "    # figure out which cds_ids have already been loaded\n",
    "    already_loaded = set(pd.read_gbq(f\"\"\"select distinct cds_id from `{dest_dataset}.merged_maf` \"\"\")[\"cds_id\"])\n",
    "    \n",
    "    # drop transfers already loaded\n",
    "    remaining_transfers = [x for x in transfers if x.cds_id not in already_loaded]\n",
    "    print(f\"{len(already_loaded)} CDS IDs already loaded. {len(remaining_transfers)} of {len(transfers)} tables need to be loaded\")\n",
    "    transfers = remaining_transfers\n",
    "          \n",
    "    batches = create_batches(transfers, parallelism)\n",
    "    #return\n",
    "    \n",
    "    for batch in tqdm(batches, desc=\"batch\", position=0):\n",
    "        jobs = []\n",
    "\n",
    "        # submit a batch to run in parallel\n",
    "        for transfer in tqdm(batch, desc=\" submit\", position=1, leave=False):\n",
    "            append_stmt = f\"insert into {dest_table} select '{transfer.cds_id}' CDS_ID, * from {transfer.dest_table} where hugo_symbol != '' and hugo_symbol is not NULL\"\n",
    "\n",
    "            job = client.query(append_stmt)            \n",
    "            jobs.append(job)\n",
    "        \n",
    "        # wait for batch to complete\n",
    "        for job in tqdm(jobs, desc=\" wait\", position=1, leave=False):            \n",
    "            job.result() # wait for completion\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34dca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_tables(f\"{dest_dataset}.merged_maf\", transfers, \"t5\", parallelism=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a8904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "q = client.query(f\"\"\"select * from (\n",
    "  SELECT chrom, pos, variant_type, count(1) mut_count FROM `{dest_dataset}.merged_maf` \n",
    "  where hugo_symbol='BRAF'\n",
    "  group by chrom, pos, variant_type) where mut_count > 10\n",
    "  LIMIT 1000\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = q.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267d61df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabc9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0dfbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_gbq(f\"\"\"select * from (\n",
    "  SELECT chrom, pos, variant_type, count(1) mut_count FROM `{dest_dataset}.merged_maf` \n",
    "  group by chrom, pos, variant_type)\n",
    "  LIMIT 1000\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b607cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3e7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "import re\n",
    "\n",
    "from google.api_core.exceptions import Conflict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c7d292-c8c0-4bc9-a0ac-60a28f8d0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = list(client.list_tables(\"depmap-omics.maf_staging_0916\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42041c63-3a60-4287-a8dd-25c3fe17bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_mafs = [x.table_id[len(\"stage_maf_\"):].replace(\"_\", \"-\") for x in tables if x.table_id.startswith(\"stage_maf_CDS\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3abe32f-820f-4e62-97bb-e274271b4c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_gbq(f\"\"\"select distinct cds_id from `{dest_dataset}.merged_maf` \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5461a3fe-f75e-43d2-994a-a6b0c2dc7ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(stage_mafs).difference(set(df[\"cds_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d891873-30b3-454f-8e20-55138ce3ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = list(client.list_jobs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9983eb-27dc-4da1-b7ff-3e479267d2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = [x for x in jobs if \"CDS_dhBHhw\" in x.job_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23167b7c-fe5f-464a-b16d-4378623f2cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing[0].errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31824496-264c-4a6c-804f-bd20a6b98e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71c0d0a-ce5d-41d8-8a35-4d4bfbce41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "just_one_transfer = [x for x in transfers if \"CDS_dhBHhw\" in x.dest_table]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0d5615-21dc-4913-8c59-0fea6ff43bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_tables(f\"{dest_dataset}.merged_maf\", just_one_transfer, \"t6\", parallelism=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d697e9-df86-4b80-8184-16071f81414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ef8b81-4b31-49ab-a337-6ebbb141f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing[0].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a2db7-b6e4-4fdd-9f81-17b9235f6ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a370a-2002-4de7-8732-c77976b206da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
