{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you need to have installed JKBio in the same folder as ccle_processing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n",
       "  var css_urls = [];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n  var css_urls = [];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import dalmatian as dm\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.CCLE_postp_function import *\n",
    "\n",
    "from JKBio import Datanalytics as da \n",
    "from JKBio import TerraFunction as terra\n",
    "from JKBio.Helper import * \n",
    "import gseapy\n",
    "from taigapy import TaigaClient\n",
    "from gsheets import Sheets\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from bokeh.plotting import *\n",
    "from bokeh.models import HoverTool\n",
    "from collections import OrderedDict\n",
    "from IPython.display import Image,display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext rpy2.ipython\n",
    "tc = TaigaClient()\n",
    "output_notebook()\n",
    "sheets = Sheets.from_files('~/.client_secret.json', '~/.storage.json')\n",
    "replace = {'T': 'Tumor', 'N': 'Normal', 'm': 'Unknown', 'L': 'Unknown'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesetname=\"20Q3\"\n",
    "prevname=\"20Q2\"\n",
    "prevversion=14 #TOTAL\n",
    "prevprevname = \"20Q1\"\n",
    "prevprevversion=22 #INTERNAL\n",
    "virtual_public='public-20q3-3d35'\n",
    "virtual_dmc='dmc-20q3-033d'\n",
    "virtual_internal='internal-20q3-00d0'\n",
    "\n",
    "workspace2=\"broad-firecloud-ccle/CCLE_DepMap_RNAseq\"\n",
    "workspace4=\"broad-genomics-delivery/Cancer_Cell_Line_Factory_CCLF_RNAseq\"\n",
    "workspace5=\"nci-mimoun-bi-org/CCLF_RNA_2_0\"\n",
    "\n",
    "workspace3=\"broad-genomics-delivery/CCLE_DepMap_RNAseq\"\n",
    "workspace1=\"broad-genomics-delivery/Getz_IBM_CellLines_RNASeqData\"\n",
    "\n",
    "workspace6=\"terra-broad-cancer-prod/CCLE_DepMap_RNAseq\"\n",
    "workspace7=\"terra-broad-cancer-prod/Getz_IBM_CellLines_RNASeqData\"\n",
    "\n",
    "refworkspace=\"broad-firecloud-ccle/DepMap_hg38_RNAseq\"\n",
    "source1=\"ibm\"\n",
    "source2=\"ccle\"\n",
    "source3=\"ccle\"\n",
    "source4=\"cclf\"\n",
    "source5=\"cclf\"\n",
    "source6=\"ccle\"\n",
    "source7=\"ibm\"\n",
    "release = samplesetname\n",
    "\n",
    "refsheet_url = \"https://docs.google.com/spreadsheets/d/1XkZypRuOEXzNLxVk9EOHeWRE98Z8_DBvL4PovyM01FE\"\n",
    "privacy_release_url = \"https://docs.google.com/spreadsheets/d/115TUgA1t_mD32SnWAGpW9OKmJ2W5WYAOs3SuSdedpX4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "genome_version <- 'hg38'\n",
    "release <- '20Q2'\n",
    "hg38_cyto_band_reference <- '../JKBio/data/hg38_cytoband.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sample set from new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_to_change = {'from_arxspan_id': 'participant',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm1 = dm.WorkspaceManager(workspace1)\n",
    "#wm2 = dm.WorkspaceManager(workspace2)\n",
    "wm3 = dm.WorkspaceManager(workspace3)\n",
    "#wm4 = dm.WorkspaceManager(workspace4)\n",
    "#wm5 = dm.WorkspaceManager(workspace5)\n",
    "wm6 = dm.WorkspaceManager(workspace6)\n",
    "wm7 = dm.WorkspaceManager(workspace7)\n",
    "refwm = dm.WorkspaceManager(refworkspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccle_refsamples = sheets.get(refsheet_url).sheets[0].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be missing \"primary disease\",\"sm_id\", \"cellosaurus_id\", \"gender, \"age\", \"primary_site\", \"primary_disease\", \"subtype\", \"subsubtype\", \"origin\", \"comments\"\n",
    "#when SMid: match==\n",
    "samples, pairs, noarxspan = GetNewCellLinesFromWorkspaces(refworkspace, stype='rna', refurl=refsheet_url, wmfroms = [workspace1, workspace3, workspace6, workspace7], sources=[source1, source3,source6, source7], match=['ACH-','CDS-'], participantslicepos=10, accept_unknowntypes=True, extract=extract_to_change, recomputedate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am trying to remove duplicates from samples without arxspan ids to then look more into them and see if I have to get data for them or if I should just throw them out\n",
    "toremov=set()\n",
    "for k, val in noarxspan.iterrows():\n",
    "    withsamesize = noarxspan[noarxspan[\"sample_id\"] == val[\"sample_id\"]]\n",
    "    if len(withsamesize) > 1:\n",
    "        for l, v in withsamesize.iloc[1:].iterrows():\n",
    "            toremov.add(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in toremov:\n",
    "    noarxspan = noarxspan.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noarxspan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noarxspan.sample_id = [i.split('NA_O')[0] for i in noarxspan.sample_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noarxspan['ccle_name'] = [''.join(i.split('_')[1:-1]).split('v')[0] for i in noarxspan.sample_id]\n",
    "noarxspan['readgroup'] = [i.split('_')[0] for i in noarxspan.sample_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noarxspan['ccle_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in noarxspan.iterrows():\n",
    "    if not gcp.exists(v['cram_or_bam_path']):\n",
    "        print(v.ccle_name)\n",
    "        noarxspan = noarxspan.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toupdate = {\"gender\":[],\n",
    "\"primary_disease\":[],\n",
    "\"sm_id\":[],\n",
    "\"cellosaurus_id\":[],\n",
    "\"age\":[],\n",
    "\"primary_site\":[],\n",
    "\"subtype\":[],\n",
    "\"subsubtype\":[],\n",
    "\"origin\":[],\n",
    "\"comments\":[],\n",
    "\"patient_id\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refwm.get_bucket_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I have a previous samples I can update unknown data directly\n",
    "index=[]\n",
    "notfound=[]\n",
    "for k, val in samples.iterrows():\n",
    "    dat = ccle_refsamples[ccle_refsamples['arxspan_id']==val['arxspan_id']]\n",
    "    if len(dat)>0:\n",
    "        index.append(k)\n",
    "        for k, v in toupdate.items():\n",
    "            toupdate[k].append(dat[k].tolist()[0])\n",
    "    else:\n",
    "        notfound.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(toupdate['patient_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing so..\n",
    "for k, v in toupdate.items():\n",
    "    samples.loc[index,k] =v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples.loc[notfound].patient_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for these samples I will need to check and manually add the data in the list \n",
    "samples.loc[notfound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found same patient\n",
    "a = [\"ACH-000635\",\"ACH-000717\", \"ACH-000864\", \"ACH-001042\", \"ACH-001547\"]\n",
    "b = [\"ACH-002291\",\"ACH-001672\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccle_refsamples[ccle_refsamples.arxspan_id.isin(a)].patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we found we had cell lines from different patients\n",
    "ccle_refsamples.loc[ccle_refsamples[ccle_refsamples.arxspan_id.isin(a)].index,\"patient_id\"]=\"PT-B15oIzKN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate ach-id\n",
    "dup = {\"ACH-001620\": \"ACH-001605\",\n",
    "\"ACH-001621\": \"ACH-001606\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccle_refsamples[ccle_refsamples.arxspan_id==\"ACH-001605\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.loc[\"CDS-R2h6fu\",\"arxspan_id\"]=\"ACH-001620\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = changeCellLineNameInNewSet(new = samples, ref=ccle_refsamples, datatype=\"rna\", dupdict=dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notfound.remove(\"CDS-R2h6fu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename ccle_name TODO: ask becky what to do\n",
    "rename = {\"PEDS117\": \"CCLFPEDS0009T\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(notfound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the addtional data and writing it here in the right order 'as shown above'\n",
    "- use the stripped_cell_line_name to find the samples on https://docs.google.com/spreadsheets/d/1uqCOos-T9EMQU7y2ZUw4Nm84opU5fIT1y7jet1vnScE/edit#gid=356471436. \n",
    "- Make sure that we don't have duplicate cell lines in there. Otherwise, use the duplicate renaming function\n",
    "- copy Primary Site, Primary Disease, Subtype, Comments, Disease Sub-subtype, if they exist. (sometimes subtype and subsubtype are the same.. don't use subsubtype then.\n",
    "- look for the cell line in cellosaurus, you might need to use one of the aliases given in master depmap pv..\n",
    "- copy  cellosaurus_id gender age info or write 'U' if they don't exist. 'can be a number or {Embryonic, Children, Adult, Fetus, U} \n",
    "- check that it does not say this cell line is not a duplicate from another cell line\n",
    "- check that if it says this cell line is derived/children/father/samepatient from other cell lines, and that if we have any of the other cell lines, that the patient id is changed to be the same one for all (be sure that you are updating everywhere these patient ids are used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sample info file has different age annotations\n",
    "toupdate = {\"sm_id\":[\"\"]*30,\n",
    "            \n",
    "\"cellosaurus_id\":['CVCL_H248',\"\",\"CVCL_S505\",\"CVCL_2277\",\"CVCL_1966\",\"CVCL_M147\",\"CVCL_2024\",\"CVCL_A344\",\"CVCL_8354\",\"CVCL_S506\",\"CVCL_M258\",\"CVCL_2581\",\"CVCL_2604\",\"CVCL_4449\",\"CVCL_2610\",\"CVCL_M266\",\"CVCL_4D12\",\"CVCL_3118\",\"\",\"CVCL_T041\",\"CVCL_1852\",\"CVCL_2777\",\"CVCL_B371\", \"CVCL_3220\",\"CVCL_1854\",\"CVCL_2223\",\"CVCL_2226\",\"CVCL_1903\",\"CVCL_3423\",\"CVCL_1898\"],\n",
    "\"gender\":['Female',\"Male\",\"Male\",\"Male\",\"Male\",\"Male\",\"Female\",\"Male\",\"Female\",\"Male\",\"Male\",\"Male\",\"Male\",\"Female\",\"Male\",\"U\",\"Female\",\"Male\",\"U\",'Female',\"Male\",\"Male\",\"Male\",\"Female\",\"Male\",\"Female\",\"Female\",\"Male\",\"Female\",\"Male\"],\n",
    "            \n",
    "\"age\":[2,'U',\"Embryo\",\"Embryo\",7,12,49,\"Embryo\",52,3,56,57,\"U\",\"U\",\"U\",18,49,71,\"U\",55,11,22,10,22,57,61,69,37,\"U\",42],\n",
    "            \n",
    "\"primary_site\":[\"haematopoietic_and_lymphoid_tissue\",\"skin\",\"Embryonal\",\"Embryonal\",\"haematopoietic_and_lymphoid_tissue\",\"central_nervous_system\",\"pleural_effusion\",\"Embryonal\",\"ascites\",\"central_nervous_system\",\"biliary_tract\",\"pleural_effusion\",\"lymph_node\",\"lymph_node\",\"lymph_node\",\"biliary_tract\",\"uvea\",\"biliary_tract\",\"bone\",\"pleural_effusion\",\"haematopoietic_and_lymphoid_tissue\",\"lung\",\"haematopoietic_and_lymphoid_tissue\",\"Thymus\",\"ascites\",\"upper_aerodigestive_tract\",\"upper_aerodigestive_tract\",\"haematopoietic_and_lymphoid_tissue\",\"breast\",\"haematopoietic_and_lymphoid_tissue\"],\n",
    "            \n",
    "\"primary_disease\":['Leukemia',\"Skin Cancer\",\"Embryonal Cancer\",\"Embryonal Cancer\",\"Lymphoma\",\"Brain Cancer\",\"Lymphoma\",\"Teratoma\",\"Ovarian Cancer\",\"Brain Cancer\",\"Bile Duct Cancer\",\"Lung Cancer\",\"Skin Cancer\",\"Skin Cancer\",\"Skin Cancer\",\"Bile Duct Cancer\",\"Eye Cancer\",\"Bile Duct Cancer\",\"Bone Cancer\",\"Breast Cancer\",\"Leukemia\",\"Embryonal Cancer\",\"Lymphoma\",\"Thymic Cancer\",\"Lymphoma\",\"Head and Neck Cancer\",\"Head and Neck Cancer\",\"Lymphoma\",\"Breast Cancer\",\"Lymphoma\"],\n",
    "            \n",
    "\"subtype\":['AML',\"Melanoma\",\"Carcinoma\",\"Carcinoma\",\"B-cell, Non-Hodgkins, Burkitts\",\"Glioblastoma\",\"B-cell, Non-Hodgkins, Burkitts\",\"Testicular\",\"Adenocarcinoma, clear cell\",\"Medulloblastoma\",\"Cholangiocarcinoma\",\"Mesothelioma\",\"Melanoma\",\"Melanoma\",\"Melanoma\",\"Engineered\",\"Uveal Melanoma\",\"Cholangiocarcinoma\",\"Ewings Sarcoma\",\"Breast Ductal Carcinoma\",\"Acute Lymphoblastic Leukemia (ALL), B-cell\",\"Carcinoma\",\"B-cell, Non-Hodgkins, Burkitts\",\"Carcinoma\",\"B-cell\",\"Squamous Cell Carcinoma\",\"Squamous Cell Carcinoma\",\"B-cell\",\"Carcinoma\",\"B-cell\"],\n",
    "            \n",
    "\"subsubtype\":[\"\",'acral',\"\",\"\",\"b_cell_burkitt\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"engineered_bile_duct\",\"\",\"intrahepatic\",\"\",\"\",\"b_cells\",\"\",\"b_cell_burkitt\",\"anaplastic\",\"\",\"\",\"\",\"\",\"ERneg_HER2pos\",\"\"],\n",
    "            \n",
    "\"origin\":['']*30,\n",
    "            \n",
    "\"comments\":[\"Gifted to Fred Hutchinson Cancer Center by Dr. Jeff Taub at Wayne State University; Received from Academic lab (FHCRC and Taub, Wayne State University)\",\"M1610113 (p9); Mutation: NRASG12R; biopsy site - right dorsal heel; Received from Academic lab (Hayward, QIMRB)\",\"cytoreductive thoracotomy specimen of a man with primary retroperitoneal embryonal carcinoma\",\"retro-peritoneal metastasis of a non seminomatous germ cell carcinoma\",\"\",\"Received from Academic lab (Hong, Broad)\",\"\",\"malignant testicular teratoma (intermediate grade)\",\"ovarian clear cell adenocarcinoma\",\"\",\"Human poorly differentiated cholangiocarcinoma cell line established from biliary tract.\",\"\",\"taken from metastatic site: lymph node\",\"taken from metastatic site: lymph node\",\"taken from metastatic site: lymph node\",\"Received from Academic lab (Bardeesy, MGH)/DSMZ\",\"display GNAQ or GNA11 activating mutations\",\"Received from Academic lab (Bardeesy, MGH)/DSMZ\",\"(PCR) EWS/FLI\",\"breast cancer, Infiltrating ductal carcinoma\",\"ST & GR-ST are subclones of Tanoue (JCRB). GR-ST are ST cells transformed with human G-CSF receptor cDNA. Responds to G-CSF.\",\"malignant embryonal carcinoma derived from lung\",\"Human lymphoid cell line derived from Burkitt's lymphoma. Tumorigenic in nude mouse (meta from ileocecal lymph node)\",\"Thymic carcinoma cell line.\",\"\",\"\",\"\",\"\",\"HER2Amp; basal\",\"\"],\n",
    "            \n",
    "\"patient_id\":[\"PT-IMn7eqLZ\",\"PT-pUBtKkYR\",\"PT-El9YBuz5\",\"PT-P6Iueia9\", \"PT-2gssvols\", \"PT-NczSWWPv\", \"PT-RY8p3mdI\", \"PT-olH0fsQe\", \"PT-3CksfnKs\", \"PT-OrIcgwcE\", \"PT-W3uUJEi1\", \"PT-B15oIzKN\", \"PT-fo7MYTEA\", \"PT-ley1jk9c\", \"PT-3f6vZIv9\", \"PT-MdR8meiJ\", \"PT-z5KcZzaT\", \"PT-3wNFFmJl\", \"PT-gpiijlSj\", \"PT-bdAvWDQz\", \"PT-uDGwdjJW\", \"PT-L63btI6L\", \"PT-uXJgRBuh\", \"PT-GkdLfVhc\", \"PT-UGLaOKH7\", \"PT-hw8KmxDu\", \"PT-G3kJesC0\", \"PT-zFOjyoHz\", \"PT-A8FG8EcM\", \"PT-98h5OTqP\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = pd.DataFrame(toupdate)\n",
    "a['name'] = samples.loc[notfound,\"stripped_cell_line_name\"].tolist()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating..\n",
    "for k, v in toupdate.items():\n",
    "    samples.loc[notfound,k] =v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading to our bucket (now a new function)\n",
    "a = changeToBucket(samples,'gs://cclebams/rna/', values=['internal_bam_filepath','internal_bai_filepath'], catchdup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and updating the spreadsheet with these\n",
    "print(\"YOU NOW NEED TO UPDATE THE GOOGLE SHEET!\")\n",
    "pd.concat([ccle_refsamples,samples],sort=False).to_csv('temp/updated_ccle_samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['arxspan_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.rename(columns={'patient_id':'participant_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairs.participant_id = samples.participant_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairs = pairs.rename(columns={'patient_id':'participant_id'}).set_index('pair_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploading new samples\n",
    "refwm = refwm.disable_hound()\n",
    "#refwm.upload_samples(samples)\n",
    "#refwm.upload_entities('pairs', pairs)\n",
    "#creating a sample set\n",
    "refwm.update_sample_set(sample_set_id=samplesetname, sample_ids=samples.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that we have all the cell lines we expect for this release\n",
    "This involves comparing to the list in the Google sheet \"Cell Line Profiling Status.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function may not work - it hasn't been tested\n",
    "url = 'https://docs.google.com/spreadsheets/d/1qus-9TKzqzwUMNWp8S1QP4s4-3SsMo2vuQRZrNXf7ag'\n",
    "\n",
    "compareToCuratedGS(url, sample = samples, samplesetname = samplesetname, colname = 'RNA New to internal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the pipeline\n",
    "\n",
    "We are using Dalmatian to send request to Terra, we are running a set of 6 functions To generate the expression/fusion dataset:\n",
    "\n",
    "We use the GTEx pipeline ([https://github.com/broadinstitute/gtex-pipeline/blob/v9/TOPMed_RNAseq_pipeline.md](https://github.com/broadinstitute/gtex-pipeline/blob/v9/TOPMed_RNAseq_pipeline.md)).\n",
    "\n",
    "To generate the expression dataset, run the following tasks on all samples that you need, in this order:\n",
    "\n",
    "\n",
    "\n",
    "*   samtofastq_v1-0_BETA_cfg \n",
    "\n",
    "    (broadinstitute_gtex/samtofastq_v1-0_BETA Snapshot ID: 5)\n",
    "\n",
    "*   star_v1-0_BETA_cfg\n",
    "\n",
    "(broadinstitute_gtex/star_v1-0_BETA Snapshot ID: 7)\n",
    "\n",
    "\n",
    "\n",
    "*   rsem_v1-0_BETA_cfg \n",
    "\n",
    "    (broadinstitute_gtex/rsem_v1-0_BETA Snapshot ID: 4)\n",
    "\n",
    "*   rsem_aggregate_results_v1-0_BETA_cfg (broadinstitute_gtex/rsem_aggregate_results_v1-0_BETA Snapshot ID: 3)\n",
    "\n",
    "The outputs to be downloaded will be saved under the sample set that you ran. The outputs we use for the release are:\n",
    "\n",
    "\n",
    "\n",
    "*   rsem_genes_expected_count\n",
    "*   rsem_genes_tpm\n",
    "*   rsem_transcripts_tpm\n",
    "\n",
    "****Make sure that you delete the intermediate files. These files are quite large so cost a lot to store. To delete, you can either write a task that deletes them or use gsutil rm*****\n",
    "\n",
    "\n",
    "##### Fusions {#fusions}\n",
    "\n",
    "We use STAR-Fusion [https://github.com/STAR-Fusion/STAR-Fusion/wiki](https://github.com/STAR-Fusion/STAR-Fusion/wiki). The fusions are generated by running the following tasks\n",
    "\n",
    "\n",
    "\n",
    "*   hg38_STAR_fusion (gkugener/STAR_fusion Snapshot ID: 14)\n",
    "*   Aggregate_Fusion_Calls (gkugener/Aggregate_files_set Snapshot ID: 2)\n",
    "\n",
    "The outputs to be downloaded will be saved under the sample set you ran. The outputs we use for the release are: \n",
    "\n",
    "\n",
    "\n",
    "*   fusions_star\n",
    "\n",
    "This task uses the same samtofastq_v1-0_BETA_cfg task as in the expression pipeline, although in the current implementation, this task will be run twice. It might be worth combing the expression/fusion calling into a single workflow. This task also contains a flag that lets you specify if you want to delete the intermediates (fastqs). \n",
    "\n",
    "There are several other tasks in this workspace. In brief:\n",
    "\n",
    "\n",
    "\n",
    "*   Tasks prefixed with **EXPENSIVE** or **CHEAP** are identical to their non-prefixed version, except that they specify different memory, disk space, etc. parameters. These versions can be used when samples fail the normal version of the task due to memory errors.\n",
    "*   The following tasks are part of the GTEx pipeline but we do not use them (we use RSEM exclusively): markduplicates_v1-0_BETA_cfg (broadinstitute_gtex/markduplicates_v1-0_BETA Snapshot ID: 2), rnaseqc2_v1-0_BETA_cfg (broadinstitute_gtex/rnaseqc2_v1-0_BETA Snapshot ID: 2)\n",
    "*   **ExonUsage_hg38_fixed** (gkugener/ExonUsage_fixed Snapshot ID: 1): this task calculates exon usage ratios. The non-fixed version contains a bug in the script that is not able to handle chromosome values prefixed with ‘chr’. The ‘fixed’ version resolves this issue.\n",
    "*   **AggregateExonUsageRObj_hg38** (ccle_mg/AggregateExonUsageRObj Snapshot ID: 2): combines the exon usage ratios into a matrices that are saved in an R object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Terra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id = refwm.create_submission(\"samtofastq_v1-0_BETA_cfg\", samplesetname,'sample_set',expression='this.samples')\n",
    "terra.waitForSubmission(refworkspace, submission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id = refwm.create_submission(\"star_v1-0_BETA_cfg\", samplesetname,'sample_set',expression='this.samples')\n",
    "terra.waitForSubmission(refworkspace, submission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id1 = refwm.create_submission(\"rsem_v1-0_BETA_cfg\", samplesetname,'sample_set',expression='this.samples')\n",
    "terra.waitForSubmission(refworkspace, [submission_id1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update a samples set with another sampleset\n",
    "terra.updateAllSampleSet(refworkspace, samplesetname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id1 = refwm.create_submission(\"rsem_aggregate_results\", 'All_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id3 = refwm.create_submission(\"hg38_STAR_fusion\", samplesetname,'sample_set',expression='this.samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id_snp = refwm.create_submission(\"rnaseq-germline-snps-indels\", samplesetname,'sample_set',expression='this.samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra.waitForSubmission(refworkspace, [submission_id1])#submission_id_snp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id_snp = refwm.create_submission(\"merge_vcfs\", 'All_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the workflow configurations used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra.saveConfigs(refworkspace,'data/'+samplesetname+'/RNAconfig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load QC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starlogs = getQC(workspace=refworkspace ,only=[], qcname=\"star_logs\",match=\".Log.final.out\")\n",
    "rnaqc = getQC(workspace=refworkspace ,only=[], qcname=\"rnaseqc2_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ref = pd.read_csv('temp/newrefWES.csv',index_col=\"cds_sample_id\")\n",
    "for k,v in starlogs.items():\n",
    "    if k =='nan':\n",
    "        continue\n",
    "    new_ref.loc[k,'bam_qc']=v+\" \"+rnaqc[k]\n",
    "new_ref.to_csv('temp/newrefAll.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove some datafile to save money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstoremove = ['fastq1', 'fastq2', 'star_chimeric_bam_file','star_chimeric_bam_index', 'star_transcriptome_bam','recalibrated_bam','recalibrated_bam_index']\n",
    "for val in colstoremove:\n",
    "    refwm.disable_hound().delete_entity_attributes('sample', res[val], delete_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.iloc[1]['star_bam_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## copy star bam file to our cclebams/rnasq_hg38/ bucket\n",
    "renamed, _ = terra.changeGSlocation(workspacefrom=refworkspace, newgs=\"gs://cclebams/rnasq_hg38/\", onlycol=[\"star_bam_file\",'star_bam_index'], entity=\"sample\", keeppath=False,dry_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expression post processing\n",
    "\n",
    "Here we get all data and remove the duplicates directly with the function `removeDuplicates`\n",
    "\n",
    "we then run:\n",
    "\n",
    "- readTranscripts\n",
    "- readCounts\n",
    "- readTPM\n",
    "- renameFunction\n",
    "\n",
    "- Allie's gene renaming / filtering and log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = refwm.get_sample_sets().loc['All_samples']\n",
    "rsem_genes_expected_count = res['rsem_genes_expected_count']\n",
    "rsem_genes_tpm = res['rsem_genes_tpm']\n",
    "rsem_transcripts_tpm = res['rsem_transcripts_tpm']\n",
    "! gsutil cp $rsem_genes_expected_count \"temp/expression.expectedcount\" & gsutil cp $rsem_genes_tpm \"temp/expression.genes.tpm\" & gsutil cp $rsem_transcripts_tpm \"temp/expression.transcript.tpm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in [\"temp/expression.expectedcount\"]:#,\"temp/expression.transcript.tpm\",\"temp/expression.genes.tpm\"]:\n",
    "    file = pd.read_csv(val, compression='gzip', header=0, sep='\\t', quotechar='\"', error_bad_lines=False)\n",
    "    print(file.columns[:10])\n",
    "    renaming,_ = removeOlderVersions(names=file.columns[2:], refsamples=refwm.get_samples(), arxspan_id=\"arxspan_id\", version=\"version\")\n",
    "    file[file.columns[:2].tolist()+[i for i in file.columns if i in renaming.keys()]].rename(columns=renaming).to_csv(val, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saving samples used for 20Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ref.loc[renaming.keys(),'20q2']=1\n",
    "new_ref.to_csv('temp/newrefAll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library('taigr')\n",
    "source('../gkugener/RScripts/load_libraries_and_annotations.R')\n",
    "source('src/CCLE_postp_function.R')\n",
    "library('cdsomics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# These files are downloaded from FireCloud/Terra\n",
    "download_paths <- list(\n",
    "  tpm_genes='temp/expression.genes.tpm',\n",
    "  tpm_transcripts='temp/expression.transcript.tpm',\n",
    "  counts_genes='temp/expression.expectedcount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "tpm_transcripts = readTranscripts(download_paths$tpm_transcripts)\n",
    "counts_genes = readCounts(download_paths$counts_genes)\n",
    "tpm_genes = readTPM(download_paths$tpm_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "head(tpm_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Fix the colnames (for cases where there are mixed values (CCLE_name and DepMap_IDs))\n",
    "colnames(counts_genes) %<>% renameFunction(.)\n",
    "colnames(tpm_genes) %<>% renameFunction(.)\n",
    "colnames(tpm_transcripts) %<>% renameFunction(.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data exploration and QC\n",
    "\n",
    "- we QC on the amount of genes with 0 counts for each samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "# Quick QC\n",
    "# We are looking for samples with a worrying amount of zeros\n",
    "zero_threshold <- 39000\n",
    "number_zeros <- apply(tpm_genes[,2:ncol(tpm_genes)-1] ==0, 2, FUN = sum)\n",
    "nzdf <- data.frame(CL=names(number_zeros), nz=number_zeros, stringsAsFactors = F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "number_zeros <- number_zeros[order(-number_zeros)]\n",
    "number_zeros <- number_zeros[number_zeros < zero_threshold]\n",
    "pass <- number_zeros %>% names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "counts_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "colnames(tpm_transcripts)[which(colnames(tpm_transcripts)=='transcript')] <- 'transcript_id(s)'\n",
    "colnames(tpm_genes)[which(colnames(tpm_genes)=='transcript')] <- 'transcript_id(s)'\n",
    "colnames(counts_genes)[which(colnames(counts_genes)=='transcript')] <- 'transcript_id(s)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# These samples failed\n",
    "failed <- setdiff(colnames(tpm_genes), pass) %>% .[!(. %in% c('gene_id', 'transcript_id(s)'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "counts_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "counts_genes %<>% dplyr::select(c(\"gene_id\",\"transcript_id(s)\", pass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "head(tpm_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "pass <- setdiff(pass,c(\"gene_id\",\"transcript_id(s)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "tpm_genes %<>% dplyr::select(c(\"gene_id\",\"transcript_id(s)\", pass))\n",
    "tpm_transcripts %<>% dplyr::select(c(\"transcript_id\", \"gene_id\", pass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Plot of the samples that fail\n",
    "plot <- ggplot(nzdf, aes(nz)) +\n",
    "  geom_histogram(bins = 100, color='black', fill='white') +\n",
    "  geom_vline(xintercept = zero_threshold, linetype=2) +\n",
    "  geom_label_repel(data = nzdf %>% filter(nz > zero_threshold), aes(x=nz, y=0, label=CL), size=5, fill=rgb(1,1,1,0.5))\n",
    "\n",
    "ggsave(plot, filename ='data/zero_transcripts_distribution.png', width=20, height = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='data/zero_transcripts_distribution.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allie post processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "colnames(tpm_transcripts)[which(colnames(tpm_transcripts)=='transcript')] <- 'transcript_id(s)'\n",
    "colnames(tpm_genes)[which(colnames(tpm_genes)=='transcript')] <- 'transcript_id(s)'\n",
    "colnames(counts_genes)[which(colnames(counts_genes)=='transcript')] <- 'transcript_id(s)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "print(colnames(tpm_genes)[1:10])\n",
    "tpm_gene_ids <- gsub(\"\\\\..*\", \"\", tpm_genes$gene_id)\n",
    "if(nrow(tpm_genes) != length(unique(tpm_gene_ids))){\n",
    "  print(\"Duplicated ensembl ids\")\n",
    "  print(nrow(tpm_genes) - length(unique(tpm_gene_ids)))\n",
    "  tpm_genes <- tpm_genes[-which(duplicated(tpm_gene_ids)==T),]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "annotations<-download.raw.from.taiga(data.name='gene-annotations-e737', data.version=1, data.file='gencode.v29.annotation')\n",
    "tpm_protein_coding <- prepare_depmap_TPM_for_taiga(tpm_genes, log_transform=T, just_protein_coding=T, gencode_annotations=annotations)\n",
    "tpm_genes <- prepare_depmap_TPM_for_taiga(tpm_genes, log_transform=T, just_protein_coding=F, gencode_annotations=annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "### transcripts\n",
    "library('cdsomics')\n",
    "print(colnames(tpm_transcripts)[1:10])\n",
    "tpm_transcript_id <- gsub(\"\\\\..*\", \"\", tpm_transcripts$`transcript_id`)\n",
    "if(nrow(tpm_transcripts) != length(unique(tpm_transcript_id))) {\n",
    "  print(\"Duplicated transcript ids\")\n",
    "  print(nrow(tpm_transcripts) - length(unique(tpm_transcript_id)))\n",
    "  tpm_transcript <- tpm_transcripts[-which(duplicated(tpm_transcript_id)==T),]\n",
    "}\n",
    "tpm_transcripts <- prepare_depmap_transcripts_for_taiga(tpm_transcript, gencode_annotations = annotations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "### counts\n",
    "print(colnames(counts_genes)[1:10])\n",
    "counts_gene_ids <- gsub(\"\\\\..*\", \"\", counts_genes$gene_id)\n",
    "if(nrow(counts_genes) != length(unique(counts_gene_ids))) {\n",
    "  print(\"Duplicate ensembl ids\")\n",
    "  print(length(which(duplicated(counts_gene_ids)==T)))\n",
    "  counts_genes <- counts_genes[-which(duplicated(counts_gene_ids)==T),]\n",
    "}\n",
    "counts_genes <- prepare_depmap_TPM_for_taiga(counts_genes, gencode_annotations =annotations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "write.table(\n",
    "  counts_genes, \n",
    "  file = paste0('temp/expression_', release,'_counts'), \n",
    "  sep = ',', quote = F)\n",
    "write.table(\n",
    "  tpm_genes, \n",
    "  file = paste0('temp/expression_', release,'_genes'), \n",
    "  sep = ',', quote = F)\n",
    "write.table(\n",
    "  tpm_protein_coding, \n",
    "  file = paste0('temp/expression_', release,'_protein_coding'), \n",
    "  sep = ',', quote = F)\n",
    "write.table(\n",
    "  tpm_transcripts, \n",
    "  file = paste0('temp/expression_', release,'_transcripts'),\n",
    "  sep = ',', quote = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts_genes = pd.read_csv('temp/expression.'+ prevname + '.counts',index_col=0, sep='\\t')\n",
    "tpm_genes = pd.read_csv('temp/expression.'+ prevname + '.genes',index_col=0, sep='\\t')\n",
    "tpm_proteincoding = pd.read_csv('temp/expression.'+ prevname + '.protein_coding',index_col=0, sep='\\t')\n",
    "tpm_transcripts = pd.read_csv('temp/expression.'+ prevname + '.transcripts', index_col=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ssGSEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genesets = [\"GO_Biological_Process_2015\", \"GO_Cellular_Component_2015\", \"GO_Molecular_Function_2015\", \"Reactome_2013\", \"BioCarta_2013\", \"KEGG_2016\", \"CORUM\", \"TRANSFAC_and_JASPAR_PWMs\", \"Chromosome_Location\",\"data/oncogenic.gmt\",\n",
    "\"data/positional.gmt\",\"data/hallmark.gmt\", \"data/cancer_modules.gmt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_genes.columns = [i.split(' (')[0] for i in counts_genes.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([val.split('.')[0] for val in counts_genes.columns if '.' in val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[val for val in counts_genes.columns if '.' in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94%93%94%\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96%\r"
     ]
    }
   ],
   "source": [
    "#### merging splicing variants into the same gene\n",
    "counts_genes_merged= mergeSplicingVariants(counts_genes.T,defined='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_genes_merged['NAME'] = counts_genes_merged.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_genes_merged = counts_genes_merged.drop(columns='genes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ACH-001097        0.0\n",
       "ACH-002466        0.0\n",
       "ACH-002467        0.0\n",
       "ACH-001636        0.0\n",
       "ACH-001679        0.0\n",
       "               ...   \n",
       "ACH-000904    20564.0\n",
       "ACH-000110       23.0\n",
       "ACH-000261       10.0\n",
       "ACH-000031      348.0\n",
       "ACH-000682      619.0\n",
       "Name: C14orf132, Length: 1419, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_genes_merged.loc['C14orf132']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_genes_merged.to_csv('temp/counts_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts_genes_merged= pd.read_csv('temp/counts_merged.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACH-001097</th>\n",
       "      <th>ACH-002466</th>\n",
       "      <th>ACH-002467</th>\n",
       "      <th>ACH-001636</th>\n",
       "      <th>ACH-001679</th>\n",
       "      <th>ACH-001698</th>\n",
       "      <th>ACH-002022</th>\n",
       "      <th>ACH-001804</th>\n",
       "      <th>ACH-000534</th>\n",
       "      <th>ACH-002464</th>\n",
       "      <th>...</th>\n",
       "      <th>ACH-000305</th>\n",
       "      <th>ACH-000603</th>\n",
       "      <th>ACH-000916</th>\n",
       "      <th>ACH-000296</th>\n",
       "      <th>ACH-000978</th>\n",
       "      <th>ACH-000904</th>\n",
       "      <th>ACH-000110</th>\n",
       "      <th>ACH-000261</th>\n",
       "      <th>ACH-000031</th>\n",
       "      <th>ACH-000682</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>A1BG</td>\n",
       "      <td>4.13</td>\n",
       "      <td>76.52</td>\n",
       "      <td>79.47</td>\n",
       "      <td>395.71</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23.44</td>\n",
       "      <td>9.76</td>\n",
       "      <td>267.80</td>\n",
       "      <td>600.84</td>\n",
       "      <td>87.59</td>\n",
       "      <td>...</td>\n",
       "      <td>148.25</td>\n",
       "      <td>3.93</td>\n",
       "      <td>50.51</td>\n",
       "      <td>117.20</td>\n",
       "      <td>581.13</td>\n",
       "      <td>377.95</td>\n",
       "      <td>108.29</td>\n",
       "      <td>295.21</td>\n",
       "      <td>18.04</td>\n",
       "      <td>92.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>A1BG-AS1</td>\n",
       "      <td>45.96</td>\n",
       "      <td>453.13</td>\n",
       "      <td>344.58</td>\n",
       "      <td>439.84</td>\n",
       "      <td>136.8</td>\n",
       "      <td>121.70</td>\n",
       "      <td>30.96</td>\n",
       "      <td>399.59</td>\n",
       "      <td>243.46</td>\n",
       "      <td>239.03</td>\n",
       "      <td>...</td>\n",
       "      <td>265.42</td>\n",
       "      <td>5.35</td>\n",
       "      <td>55.76</td>\n",
       "      <td>421.83</td>\n",
       "      <td>810.86</td>\n",
       "      <td>585.60</td>\n",
       "      <td>299.54</td>\n",
       "      <td>325.75</td>\n",
       "      <td>28.41</td>\n",
       "      <td>147.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>A1CF</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.03</td>\n",
       "      <td>1950.40</td>\n",
       "      <td>89.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>A2M</td>\n",
       "      <td>12.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>25.29</td>\n",
       "      <td>23.00</td>\n",
       "      <td>9.41</td>\n",
       "      <td>36.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>14.65</td>\n",
       "      <td>9.85</td>\n",
       "      <td>90.00</td>\n",
       "      <td>129.78</td>\n",
       "      <td>69.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>A2M-AS1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>197.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>60.71</td>\n",
       "      <td>14.00</td>\n",
       "      <td>179.59</td>\n",
       "      <td>11.00</td>\n",
       "      <td>42.00</td>\n",
       "      <td>3.35</td>\n",
       "      <td>70.39</td>\n",
       "      <td>43.00</td>\n",
       "      <td>44.22</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Z99129</td>\n",
       "      <td>151.41</td>\n",
       "      <td>269.00</td>\n",
       "      <td>300.00</td>\n",
       "      <td>116.00</td>\n",
       "      <td>137.0</td>\n",
       "      <td>197.00</td>\n",
       "      <td>571.05</td>\n",
       "      <td>258.00</td>\n",
       "      <td>75.00</td>\n",
       "      <td>279.00</td>\n",
       "      <td>...</td>\n",
       "      <td>265.71</td>\n",
       "      <td>216.11</td>\n",
       "      <td>194.49</td>\n",
       "      <td>347.74</td>\n",
       "      <td>198.13</td>\n",
       "      <td>157.51</td>\n",
       "      <td>320.18</td>\n",
       "      <td>346.14</td>\n",
       "      <td>177.00</td>\n",
       "      <td>411.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Z99289</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Z99497</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Z99755</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Z99916</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>6.00</td>\n",
       "      <td>11.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53228 rows × 1419 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ACH-001097  ACH-002466  ACH-002467  ACH-001636  ACH-001679  \\\n",
       "A1BG            4.13       76.52       79.47      395.71        30.0   \n",
       "A1BG-AS1       45.96      453.13      344.58      439.84       136.8   \n",
       "A1CF            1.00        0.00        0.00        0.00         0.0   \n",
       "A2M            12.00        0.00        0.00        0.00         0.0   \n",
       "A2M-AS1         2.00        0.00        0.00        0.00         0.0   \n",
       "...              ...         ...         ...         ...         ...   \n",
       "Z99129        151.41      269.00      300.00      116.00       137.0   \n",
       "Z99289          0.00        0.00        0.00        0.00         0.0   \n",
       "Z99497          0.00        0.00        0.00        0.00         0.0   \n",
       "Z99755          0.00        0.00        0.00        0.00         0.0   \n",
       "Z99916          0.00        0.00        0.00        0.00         0.0   \n",
       "\n",
       "          ACH-001698  ACH-002022  ACH-001804  ACH-000534  ACH-002464  ...  \\\n",
       "A1BG           23.44        9.76      267.80      600.84       87.59  ...   \n",
       "A1BG-AS1      121.70       30.96      399.59      243.46      239.03  ...   \n",
       "A1CF            0.00        8.00        0.00        0.00        0.00  ...   \n",
       "A2M            26.00        0.00        0.00       16.00        1.00  ...   \n",
       "A2M-AS1         5.00        0.00      197.00        1.00        0.00  ...   \n",
       "...              ...         ...         ...         ...         ...  ...   \n",
       "Z99129        197.00      571.05      258.00       75.00      279.00  ...   \n",
       "Z99289          0.00        0.00        0.00        0.00        2.00  ...   \n",
       "Z99497          0.00        0.00        0.00        0.00        0.00  ...   \n",
       "Z99755          0.00        0.00        0.00        0.00        0.00  ...   \n",
       "Z99916          6.00        0.00        0.00        0.00        0.00  ...   \n",
       "\n",
       "          ACH-000305  ACH-000603  ACH-000916  ACH-000296  ACH-000978  \\\n",
       "A1BG          148.25        3.93       50.51      117.20      581.13   \n",
       "A1BG-AS1      265.42        5.35       55.76      421.83      810.86   \n",
       "A1CF            5.03     1950.40       89.00       12.00        8.00   \n",
       "A2M            25.29       23.00        9.41       36.00        9.00   \n",
       "A2M-AS1        60.71       14.00      179.59       11.00       42.00   \n",
       "...              ...         ...         ...         ...         ...   \n",
       "Z99129        265.71      216.11      194.49      347.74      198.13   \n",
       "Z99289          0.00        1.00        0.00        0.00        0.00   \n",
       "Z99497          0.00        0.00        0.00        0.00        0.00   \n",
       "Z99755          1.00        0.00        1.00        0.00        0.00   \n",
       "Z99916          6.00       11.00        3.00        2.00        3.00   \n",
       "\n",
       "          ACH-000904  ACH-000110  ACH-000261  ACH-000031  ACH-000682  \n",
       "A1BG          377.95      108.29      295.21       18.04       92.95  \n",
       "A1BG-AS1      585.60      299.54      325.75       28.41      147.18  \n",
       "A1CF            6.00        3.00        4.00        9.00       10.00  \n",
       "A2M            14.65        9.85       90.00      129.78       69.00  \n",
       "A2M-AS1         3.35       70.39       43.00       44.22       15.00  \n",
       "...              ...         ...         ...         ...         ...  \n",
       "Z99129        157.51      320.18      346.14      177.00      411.40  \n",
       "Z99289          0.00        1.00        0.00        0.00       12.00  \n",
       "Z99497          0.00        0.00        0.00        0.00        0.00  \n",
       "Z99755          5.00        5.00        2.00        2.00        1.00  \n",
       "Z99916          0.00        5.00        1.00        2.00        3.00  \n",
       "\n",
       "[53228 rows x 1419 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_genes_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACH-002065']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in counts_genes.columns if 'ACH-002065' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-13 16:51:32,145 Warning: dropping duplicated gene names, only keep the first values\n"
     ]
    }
   ],
   "source": [
    "#for val in genesets:\n",
    "res = gseapy.ssgsea(data=counts_genes_merged[counts_genes_merged.columns], gene_sets=\"KEGG_2016\",\n",
    "    outdir='temp/',\n",
    "    sample_norm_method='rank', # choose 'custom' for your own rank list\n",
    "    permutation_num=0, # skip permutation procedure, because you don't need it\n",
    "    no_plot=True, # skip plotting, because you don't need these figures\n",
    "    processes=8, \n",
    "    format='png',\n",
    "    seed=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19Q4.mergedMAF.txt                  fusions.20Q2.unfiltered\n",
      "ACH-001421_2.hg38.modeled.png       ggplot.png\n",
      "ACH-001533_1.hg38.modeled.png       gseapy.ssgsea.chromosome_location.log\n",
      "ACH-001574_1.hg38.modeled.png       hotspot_mutation\n",
      "ACH-001678_1.hg38.modeled.png       hotspot_mutation.20Q1.tsv\n",
      "ACH-001711_3.hg38.modeled.png       hotspot_mutation.20Q2.all.tsv\n",
      "ACH-001847_1.hg38.modeled.png       hotspot_mutation.all\n",
      "ACH-002021_1.hg38.modeled.png       internal_20Q1_counts\n",
      "ACH-002048_1.hg38.modeled.png       internal_20Q1_proteincoding_tpm\n",
      "ACH-002065_1.hg38.modeled.png       internal_20Q1_tpm\n",
      "ACH-003000_1.hg38.modeled.png       internal_20Q1_transcripts_tpm\n",
      "cnv_ccle.called.seg                 internal_20Q2_counts\n",
      "damaging_mutation                   internal_20Q2_gene_cn\n",
      "damaging_mutation.20Q1.tsv          internal_20Q2_proteincoding_tpm\n",
      "damaging_mutation.20Q2.all.tsv      internal_20Q2_segs_cn\n",
      "damaging_mutation.all               internal_20Q2_tpm\n",
      "depmap_20Q1_mutation_calls          internal_20Q2_transcripts_tpm\n",
      "depmap_20Q2_mutation_calls.all      mutation_filtered_terra_merged.txt\n",
      "dmc_20Q1_counts                     mutations.20Q1.tsv\n",
      "dmc_20Q1_gene_cn                    mutations.20Q2.all.tsv\n",
      "dmc_20Q1_proteincoding_tpm          mutation_unfiltered_terra_merged.txt\n",
      "dmc_20Q1_segs_cn                    other_mutation\n",
      "dmc_20Q1_tpm                        other_mutation.20Q1.tsv\n",
      "dmc_20Q1_transcripts_tpm            other_mutation.20Q2.all.tsv\n",
      "dmc_20Q2_counts                     other_mutation.all\n",
      "dmc_20Q2_gene_cn                    public_20Q1_counts\n",
      "dmc_20Q2_proteincoding_tpm          public_20Q1_gene_cn\n",
      "dmc_20Q2_segs_cn                    public_20Q1_proteincoding_tpm\n",
      "dmc_20Q2_tpm                        public_20Q1_segs_cn\n",
      "dmc_20Q2_transcripts_tpm            public_20Q1_tpm\n",
      "expression.20Q1.counts.tsv          public_20Q1_transcripts_tpm\n",
      "expression.20Q1.genes.tsv           public_20Q2_counts\n",
      "expression.20Q1.protein_coding.tsv  public_20Q2_gene_cn\n",
      "expression.20Q1.transcripts.tsv     public_20Q2_proteincoding_tpm\n",
      "expression.20Q2.counts              public_20Q2_segs_cn\n",
      "expression.20Q2.genes               public_20Q2_tpm\n",
      "expression.20Q2.protein_coding      public_20Q2_transcripts_tpm\n",
      "expression.20Q2.transcripts         q3_mutation_filtered_terra_merged.txt\n",
      "expression.expectedcount            q3_mutation_unfiltered_terra_merged.txt\n",
      "expression.fusion.tsv               q4_mutation_filtered_terra_merged.txt\n",
      "expression.genes.tpm                q4_mutation_unfiltered_terra_merged.txt\n",
      "expression.transcript.tpm           unfiltered_fusions_20Q1\n",
      "filtered_fusions_20Q1               unfiltered_fusions_20Q2\n",
      "filtered_fusions_20Q2               wes.20Q1.gene.cn\n",
      "fusions.20Q1.filtered.tsv           wes.20Q1.segmented.cn\n",
      "fusions.20Q1.unfiltered.tsv         wes.20Q2.gene.cn\n",
      "fusions.20Q2.filtered               wes.20Q2.segment.cn\n"
     ]
    }
   ],
   "source": [
    "ls temp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1419"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tpm_proteincoding.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACH-000052', 'ACH-000561', 'ACH-001425'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(prevproteincoding.index) - set(tpm_proteincoding.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(prevproteincoding.index) - set(tpm_proteincoding.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'14'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##################]100% |  30.9 MiB/s | 637.0 MiB / 637.0 MiB | Time:  0:00:20\n",
      "[##################]100% |  23.0 MiB/s |   2.2 GiB /   2.2 GiB | Time:  0:01:37\n"
     ]
    }
   ],
   "source": [
    "#prevcounts = tc.get(name='depmap-rnaseq-expression-data-363a', version=24, file=\"internal_\"+prevname+'_counts')\n",
    "prevgenes = tc.get(name='depmap-rnaseq-expression-data-363a', version=24, file=\"internal_\"+prevname+'_tpm')\n",
    "prevtranscripts = tc.get(name='depmap-rnaseq-expression-data-363a', version=24, file=\"internal_\"+prevname+'_transcripts_tpm')\n",
    "prevproteincoding = tc.get(name='depmap-rnaseq-expression-data-363a', version=24, file=\"internal_\"+prevname+'_proteincoding_tpm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do we have the same cell lines as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1415, 1413)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prevcounts), len(counts_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACH-000052', 'ACH-000561'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(prevproteincoding.index) - set(counts_genes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_genes\n",
    "tpm_genes\n",
    "tpm_proteincoding\n",
    "tpm_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tpm_proteincoding = pd.concat([tpm_proteincoding,  prevproteincoding.loc[['ACH-000052', 'ACH-000561']]])\n",
    "tpm_transcripts = pd.concat([tpm_transcripts, prevtranscripts.loc[['ACH-000052', 'ACH-000561']]])\n",
    "tpm_genes = pd.concat([tpm_genes, prevgenes.loc[['ACH-000052', 'ACH-000561']]])\n",
    "counts_genes = pd.concat([counts_genes, prevcounts.loc[['ACH-000052', 'ACH-000561']]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do we have new duplicates that don't correlate well and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNonZeroCorrelations(df1,df2):\n",
    "    res=[]\n",
    "    for k,val in df1.iterrows():\n",
    "        if k in df2.index:\n",
    "            corr = np.corrcoef(df1.loc[k],df2.loc[k])\n",
    "            if corr[0,1]<0.99999999999999:\n",
    "                res.append((k,corr[0,1]))\n",
    "        else:\n",
    "            print(k+\" not in second df\")\n",
    "    print(\"found \"+str(len(res))+\" samples that did not match\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1419, 58676)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_genes.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACH-001679 not in second df\n",
      "ACH-001696 not in second df\n",
      "ACH-001293 not in second df\n",
      "ACH-001449 not in second df\n",
      "ACH-001393 not in second df\n",
      "ACH-001703 not in second df\n",
      "ACH-002010 not in second df\n",
      "ACH-002709 not in second df\n",
      "ACH-001854 not in second df\n",
      "ACH-001036 not in second df\n",
      "ACH-001519 not in second df\n",
      "ACH-001605 not in second df\n",
      "ACH-001384 not in second df\n",
      "ACH-001493 not in second df\n",
      "ACH-001708 not in second df\n",
      "ACH-001855 not in second df\n",
      "ACH-001971 not in second df\n",
      "ACH-001349 not in second df\n",
      "ACH-001973 not in second df\n",
      "ACH-001438 not in second df\n",
      "ACH-001669 not in second df\n",
      "ACH-001512 not in second df\n",
      "ACH-001662 not in second df\n",
      "ACH-001820 not in second df\n",
      "ACH-001716 not in second df\n",
      "ACH-001537 not in second df\n",
      "ACH-001678 not in second df\n",
      "ACH-002512 not in second df\n",
      "ACH-001603 not in second df\n",
      "ACH-001626 not in second df\n",
      "ACH-002021 not in second df\n",
      "ACH-001547 not in second df\n",
      "ACH-001693 not in second df\n",
      "ACH-002475 not in second df\n",
      "ACH-001385 not in second df\n",
      "ACH-001676 not in second df\n",
      "ACH-001578 not in second df\n",
      "ACH-002048 not in second df\n",
      "ACH-001437 not in second df\n",
      "ACH-002065 not in second df\n",
      "ACH-001970 not in second df\n",
      "ACH-001672 not in second df\n",
      "ACH-001502 not in second df\n",
      "ACH-001429 not in second df\n",
      "ACH-002067 not in second df\n",
      "found 26 samples that did not match\n"
     ]
    }
   ],
   "source": [
    "unmatched = getNonZeroCorrelations(counts_genes ,prevcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unmatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACH-001515 new reprocessing 0.9946915477726004 MERGE\n",
    "ACH-001520 new reprocessing 0.9957699212101433 MERGE\n",
    "ACH-001740 new version (but first one seems much better in terms of reads, second one seems much better in RNAseqQC) 0.8015848372375373 TAKE SECOND\n",
    "ACH-001608 new version 0.9945092691624465 MERGE\n",
    "ACH-001334 new version same as 1740 but first one has much more reads TAKE FIRST 0.9767280911431145 MERGE\n",
    "ACH-001745 new version same as 1740 0.9829779076971114 MERGE\n",
    "ACH-000206 new version same as 1740 but in the end the total amount of mapped read is better in second version 0.9872061994453488 MERGE\n",
    "ACH-001841 new version 0.9891607464895766 MERGE\n",
    "ACH-001541 new version 0.9903054150416472 MERGE\n",
    "ACH-000597 new version 0.9807940519711182 MERGE\n",
    "ACH-000143 new version 0.9775750844260739 MERGE\n",
    "ACH-001765 new version 0.9665153948393865 MERGE\n",
    "ACH-001422 new version 0.993737865220167 MERGE\n",
    "ACH-000095 new version same as 1740 0.9307949055051599 TAKE FIRST\n",
    "ACH-000468 0.9574272807923139 TAKE FIRST\n",
    "ACH-001192 0.9786159108766187 MERGE\n",
    "ACH-000556 0.9658561991609396 MERGE\n",
    "ACH-000700 0.9724575358855265 MERGE\n",
    "ACH-001495 0.9933487615618818 MERGE\n",
    "ACH-000975 0.9670679001304227 MERGE\n",
    "ACH-000517 0.9674345128976869 MERGE\n",
    "ACH-000337 0.975427126351326 MERGE\n",
    "ACH-000029 first seems a bit better quality and more reads 0.9740794074881175 MERGE\n",
    "ACH-000455 0.9694405381565491 MERGE\n",
    "ACH-001321 0.720323357250316 BOTH are the same cell line but GIVEN how bad these lines are, I would advice to use the newest version nonetheless as it is almost the same as tthe first one in terms of read counts and bettter in QC\n",
    "ACH-000328 0.9716845943633539 MERGE\n",
    "\n",
    "merge everything above .96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ACH-001515', 0.9946915477726004),\n",
       " ('ACH-001520', 0.9957699212101433),\n",
       " ('ACH-001740', 0.8015848372375373),\n",
       " ('ACH-001608', 0.9945092691624465),\n",
       " ('ACH-001334', 0.9767280911431145),\n",
       " ('ACH-001745', 0.9829779076971114),\n",
       " ('ACH-000206', 0.9872061994453488),\n",
       " ('ACH-001841', 0.9891607464895766),\n",
       " ('ACH-001541', 0.9903054150416472),\n",
       " ('ACH-000597', 0.9807940519711182),\n",
       " ('ACH-000143', 0.9775750844260739),\n",
       " ('ACH-001765', 0.9665153948393865),\n",
       " ('ACH-001422', 0.993737865220167),\n",
       " ('ACH-000095', 0.9307949055051599),\n",
       " ('ACH-000468', 0.9574272807923139),\n",
       " ('ACH-001192', 0.9786159108766187),\n",
       " ('ACH-000556', 0.9658561991609396),\n",
       " ('ACH-000700', 0.9724575358855265),\n",
       " ('ACH-001495', 0.9933487615618818),\n",
       " ('ACH-000975', 0.9670679001304227),\n",
       " ('ACH-000517', 0.9674345128976869),\n",
       " ('ACH-000337', 0.975427126351326),\n",
       " ('ACH-000029', 0.9740794074881175),\n",
       " ('ACH-000455', 0.9694405381565491),\n",
       " ('ACH-001321', 0.720323357250316),\n",
       " ('ACH-000328', 0.9716845943633539)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it because of  duplicate version?\n",
    "rnasamples = ccle_refsamples[ccle_refsamples.datatype=='rna']\n",
    "for i,val in unmatched:\n",
    "    print(len(rnasamples[rnasamples.arxspan_id==i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <20million (~10million)\n",
    "removedfromPiPelineNotEnoughReads = ['CDS-ABH0uZ','CDS-fk564T','CDS-kU30H5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notinnew = set(prevcounts.index) - set(counts_genes.index)\n",
    "notinnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removedFromListNotEnoughExpressedGenes = [\"ACH-001577\", \"ACH-001686\", \"ACH-002463\", \"ACH-002055\", \"ACH-001388\", \"ACH-000052\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replacing different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_genes = counts_genes.T\n",
    "tpm_genes = tpm_genes.T\n",
    "tpm_proteincoding = tpm_proteincoding.T\n",
    "tpm_transcripts = tpm_transcripts.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, _ in unmatched+[('ACH-000561',''),('ACH-000052','')]:\n",
    "    counts_genes[k] = prevcounts.loc[k].values\n",
    "    tpm_genes[k] = prevgenes.loc[k].values\n",
    "    tpm_proteincoding[k] = prevproteincoding.loc[k].values\n",
    "    tpm_transcripts[k] = prevtranscripts.loc[k].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_genes = counts_genes.T\n",
    "tpm_proteincoding = tpm_proteincoding.T\n",
    "tpm_genes = tpm_genes.T\n",
    "tpm_transcripts = tpm_transcripts.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = tc.get(name='depmap-a0ab', file='sample_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding train and test set\n",
    "trainame = [val for val in new1&prev if val[:3] == 'ACH']\n",
    "testname = [val for val in new1-prev if val[:3] == 'ACH']\n",
    "\n",
    "#looking at the 2000 most variable genes in the two sets\n",
    "genetolookfor = 2000\n",
    "gene_var = counts_genes[trainame].var(1).values\n",
    "print(len(gene_var))\n",
    "sorting = np.argsort(gene_var)[-genetolookfor:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unregistered = set(testname) - set(metadata[\"DepMap_ID\"].values.tolist())\n",
    "unregistered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counts_genes['ACH-001767']) - np.count_nonzero(counts_genes['ACH-001767'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and reodering train and test sets\n",
    "traindata = counts_genes[set(trainame)-unregistered].values[sorting].T\n",
    "trainlabels = [metadata[metadata[\"DepMap_ID\"]==val][\"disease\"].values[0] for val in counts_genes[set(trainame)-unregistered].columns.tolist() if val not in unregistered]\n",
    "\n",
    "testdata = counts_genes[set(testname)-unregistered].values[sorting].T\n",
    "testlabels = [metadata[metadata[\"DepMap_ID\"]==val][\"disease\"].values[0] for val in counts_genes[set(testname)-unregistered].columns.tolist() if val not in unregistered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn KNN classifier to the metadata diseases\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(traindata, trainlabels) \n",
    "predicted = neigh.predict(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = trainlabels + testlabels\n",
    "colors=[0]*len(trainlabels)\n",
    "colors.extend([1,2,2,2,2,1,2,2,2,1,2])\n",
    "data = np.vstack([traindata,testdata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot them with TSNE, highlight the points that failed and show colors for diseases\n",
    "dimred = TSNE(2,10).fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(dimred, labels=labels,colors=colors, radi=1.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files for taiga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls temp/expression.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR 20Q3 KNOW THAT WE WILL RETRIEVE DATA FROM 20Q2 WITH THE VERY DFFERENT CELL LINES!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'temp/expression_20Q2_genes' does not exist: b'temp/expression_20Q2_genes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-38deebafee92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'temp/expression_20Q2_genes'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'temp/expression_20Q2_genes' does not exist: b'temp/expression_20Q2_genes'"
     ]
    }
   ],
   "source": [
    "a = pd.read_csv('temp/expression_20Q2_genes',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.loc['ACH-001632']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading expression_20Q3_transcripts...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'temp/expression_20Q3_transcripts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-5d5f34c9c7ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m  \u001b[0mgene\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mformat\u001b[0m \u001b[0mHGNC\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0m_symbol\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mEntrez\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0m_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mDepMap\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0m_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChromosome\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNum\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0m_Probes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSegment\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0m_Mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \"\"\")\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/taigapy/__init__.py\u001b[0m in \u001b[0;36mupdate_dataset\u001b[0;34m(self, dataset_id, dataset_permaname, dataset_version, dataset_description, changes_description, upload_file_path_dict, add_taiga_ids, add_all_existing_files)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m         new_session_id = self.upload_session_files(\n\u001b[0;32m-> 1255\u001b[0;31m             \u001b[0mupload_file_path_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupload_file_path_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_taiga_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_taiga_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         )\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/taigapy/__init__.py\u001b[0m in \u001b[0;36mupload_session_files\u001b[0;34m(self, upload_file_path_dict, add_taiga_ids)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m             s3_client.upload_file(\n\u001b[0;32m-> 1033\u001b[0;31m                 \u001b[0mupload_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupload_file_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix_and_file_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             )\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    129\u001b[0m         return transfer.upload_file(\n\u001b[1;32m    130\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             extra_args=ExtraArgs, callback=Callback)\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    277\u001b[0m             filename, bucket, key, extra_args, subscribers)\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;31m# If a client error was raised, add the backwards compatibility layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# that raises a S3UploadFailedError. These specific errors were only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# out of this and propogate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# final result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;31m# Call the submit method to start submitting tasks to execute the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;31m# transfer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_submit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;31m# If there was an exception raised during the submission of task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36m_submit\u001b[0;34m(self, client, config, osutil, request_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Determine the size if it was not provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtransfer_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m             \u001b[0mupload_input_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovide_transfer_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_future\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;31m# Do a multipart upload if needed, otherwise do a regular put object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/s3transfer/upload.py\u001b[0m in \u001b[0;36mprovide_transfer_size\u001b[0;34m(self, transfer_future)\u001b[0m\n\u001b[1;32m    235\u001b[0m         transfer_future.meta.provide_transfer_size(\n\u001b[1;32m    236\u001b[0m             self._osutil.get_file_size(\n\u001b[0;32m--> 237\u001b[0;31m                 transfer_future.meta.call_args.fileobj))\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequires_multipart_upload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransfer_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/s3transfer/utils.py\u001b[0m in \u001b[0;36mget_file_size\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mOSUtils\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_file_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_file_chunk_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_byte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/genericpath.py\u001b[0m in \u001b[0;36mgetsize\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m\"\"\"Return the size of a file, reported by os.stat().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'temp/expression_20Q3_transcripts'"
     ]
    }
   ],
   "source": [
    "tc.update_dataset(dataset_permaname=\"depmap-expression-87f8\",\n",
    "                 upload_file_path_dict={'temp/expression_'+release+'_transcripts': 'NumericMatrixCSV',\n",
    "                                       'temp/expression_'+release+'_genes': 'NumericMatrixCSV',\n",
    "                                       'temp/expression_'+release+'_counts': 'NumericMatrixCSV',\n",
    "                                       'temp/expression_'+release+'_protein_coding': 'NumericMatrixCSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# RNAseq\n",
    "\n",
    "Combined segment and gene-level CN calls from Broad WES, Sanger WES, and Broad SNP. Relative CN, not log2 transformed.\n",
    "\n",
    "PORTAL TEAM SHOULD NOT USE THIS: There are lines here that should not make it even to internal. Must use subsetted dataset instead. These data will not make it on the portal starting 19Q1. With the DMC portal, there is new cell line release prioritization as to which lines can be included, so a new taiga dataset will be created containing CN for the portal.\n",
    "\n",
    "version 1-8: guillaume releases\n",
    "version 9: 19Q3 release\n",
    "version 10:  adding missing samples in Terra merge files\n",
    "version 11: 19Q4 new release.\n",
    "\n",
    "Adding 93 new cell lines. \n",
    "Some cells lines have been removed because they:\n",
    "\n",
    " - had too many 0 values = [\"ACH-001388\" \"ACH-001577\" \"ACH-001767\" \"ACH-002463\"] \n",
    "\n",
    "Some cells lines have been flagged as:\n",
    "\n",
    " - having too many..\n",
    " \n",
    "version 12:\n",
    "    uploading as matrices \n",
    "    \n",
    "version 13:\n",
    "    20Q1 new release. \n",
    "    \n",
    "version 14:\n",
    "    20Q2 new release, adding 50 new samples\n",
    " \n",
    "transcriptions (Transcripts rpkm):\n",
    "\n",
    "genes (gene rpkm):\n",
    "__Rows__:\n",
    "__Columns__:\n",
    "Counts (gene counts):\n",
    "__Rows__:\n",
    "__Columns__:\n",
    "Gene level CN data:\n",
    "__Rows__:\n",
    "__Columns__:\n",
    " DepMap cell line IDs\n",
    " gene names in the format HGNC\\_symbol (Entrez\\_ID)\n",
    "DepMap\\_ID, Chromosome, Start, End, Num\\_Probes, Segment\\_Mean\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsheets = sheets.get(privacy_release_url).sheets[6].to_frame()\n",
    "rna_ibm_embargo = [i for i in gsheets['RNAseq_IBM_embargo'].values.tolist() if str(i) != 'nan']\n",
    "rna_dmc_embargo = [i for i in gsheets['RNAseq_DMC_embargo'].values.tolist() if str(i) != 'nan']\n",
    "blacklist = [i for i in gsheets['blacklist'].values.tolist() if str(i) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ACH-001006',\n",
       "  'ACH-001007',\n",
       "  'ACH-001008',\n",
       "  'ACH-001009',\n",
       "  'ACH-001010',\n",
       "  'ACH-001062',\n",
       "  'ACH-001082',\n",
       "  'ACH-001083',\n",
       "  'ACH-001153',\n",
       "  'ACH-001154',\n",
       "  'ACH-001155',\n",
       "  'ACH-001156',\n",
       "  'ACH-001157',\n",
       "  'ACH-001158',\n",
       "  'ACH-001159',\n",
       "  'ACH-001160',\n",
       "  'ACH-001161',\n",
       "  'ACH-001178',\n",
       "  'ACH-001179',\n",
       "  'ACH-001181',\n",
       "  'ACH-001204',\n",
       "  'ACH-001218',\n",
       "  'ACH-001219',\n",
       "  'ACH-001220',\n",
       "  'ACH-001221',\n",
       "  'ACH-001222',\n",
       "  'ACH-001223',\n",
       "  'ACH-001251',\n",
       "  'ACH-001252',\n",
       "  'ACH-001253',\n",
       "  'ACH-001254',\n",
       "  'ACH-001255',\n",
       "  'ACH-001256',\n",
       "  'ACH-001257',\n",
       "  'ACH-001258',\n",
       "  'ACH-001259',\n",
       "  'ACH-001269',\n",
       "  'ACH-001434',\n",
       "  'ACH-001752',\n",
       "  'ACH-001753',\n",
       "  'ACH-001754',\n",
       "  'ACH-001755',\n",
       "  'ACH-001756',\n",
       "  'ACH-001757',\n",
       "  'ACH-001758',\n",
       "  'ACH-001759',\n",
       "  'ACH-001760',\n",
       "  'ACH-001780',\n",
       "  'ACH-001781',\n",
       "  'ACH-001782',\n",
       "  'ACH-001783',\n",
       "  'ACH-001784',\n",
       "  'ACH-001785',\n",
       "  'ACH-002446-311cas9',\n",
       "  'ACH-002476',\n",
       "  'ACH-003000',\n",
       "  'ACH-002874',\n",
       "  'ACH-002875',\n",
       "  'ACH-001140',\n",
       "  'ACH-001546'],\n",
       " ['ACH-001024',\n",
       "  'ACH-001036',\n",
       "  'ACH-001293',\n",
       "  'ACH-001349',\n",
       "  'ACH-001385',\n",
       "  'ACH-001429',\n",
       "  'ACH-001437',\n",
       "  'ACH-001438',\n",
       "  'ACH-001449',\n",
       "  'ACH-001493',\n",
       "  'ACH-001502',\n",
       "  'ACH-001537',\n",
       "  'ACH-001547',\n",
       "  'ACH-001577',\n",
       "  'ACH-001578',\n",
       "  'ACH-001603',\n",
       "  'ACH-001620',\n",
       "  'ACH-001626',\n",
       "  'ACH-001662',\n",
       "  'ACH-001669',\n",
       "  'ACH-001672',\n",
       "  'ACH-001678',\n",
       "  'ACH-001686',\n",
       "  'ACH-001693',\n",
       "  'ACH-001696',\n",
       "  'ACH-001703',\n",
       "  'ACH-001716',\n",
       "  'ACH-001767',\n",
       "  'ACH-001820',\n",
       "  'ACH-001854',\n",
       "  'ACH-001855',\n",
       "  'ACH-001970',\n",
       "  'ACH-001971',\n",
       "  'ACH-001973',\n",
       "  'ACH-002010',\n",
       "  'ACH-002021',\n",
       "  'ACH-002048',\n",
       "  'ACH-002055',\n",
       "  'ACH-002065',\n",
       "  'ACH-002475',\n",
       "  'ACH-002508',\n",
       "  'ACH-002510',\n",
       "  'ACH-002511',\n",
       "  'ACH-002512',\n",
       "  'ACH-002709',\n",
       "  'ACH-001676',\n",
       "  'ACH-002462',\n",
       "  'ACH-002463',\n",
       "  'ACH-002464',\n",
       "  'ACH-002465',\n",
       "  'ACH-002466',\n",
       "  'ACH-002467'],\n",
       " ['ACH-001194',\n",
       "  'ACH-001453',\n",
       "  'ACH-001518',\n",
       "  'ACH-001627',\n",
       "  'ACH-001687',\n",
       "  'ACH-001690',\n",
       "  'ACH-001692',\n",
       "  'ACH-001719',\n",
       "  'ACH-001991',\n",
       "  'ACH-002019',\n",
       "  'ACH-002024',\n",
       "  'ACH-002025',\n",
       "  'ACH-002041',\n",
       "  'ACH-002046',\n",
       "  'ACH-001384',\n",
       "  'ACH-001519'])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blacklist, rna_dmc_embargo, rna_ibm_embargo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1419, 0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpm_genes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1419"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tpm_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1415\n",
      "1415\n",
      "1421\n",
      "1415\n",
      "1421\n",
      "1415\n",
      "1421\n",
      "1415\n"
     ]
    }
   ],
   "source": [
    "## removing first blacklisted, then embargoed, to create two datasets\n",
    "print(len(counts_genes))\n",
    "counts_genes = counts_genes[~counts_genes.index.isin(blacklist)]\n",
    "print(len(counts_genes))\n",
    "counts_genes.to_csv('temp/internal_'+release+'_counts')\n",
    "print(len(tpm_genes))\n",
    "tpm_genes = tpm_genes[~tpm_genes.index.isin(blacklist)]\n",
    "print(len(tpm_genes))\n",
    "tpm_genes.to_csv('temp/internal_'+release+'_tpm')\n",
    "print(len(tpm_proteincoding))\n",
    "tpm_proteincoding = tpm_proteincoding[~tpm_proteincoding.index.isin(blacklist)]\n",
    "print(len(tpm_proteincoding))\n",
    "tpm_proteincoding.to_csv('temp/internal_'+release+'_proteincoding_tpm')\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts = tpm_transcripts[~tpm_transcripts.index.isin(blacklist)]\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts.to_csv('temp/internal_'+release+'_transcripts_tpm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_counts_genes = tc.get(name='depmap-rnaseq-expression-data-363a', version=26, file='internal_'+prevname+'_counts')\n",
    "print('shoud be None')\n",
    "print(set(prev_counts_genes.index) - set(counts_genes.index))\n",
    "print(\"new lines\")\n",
    "newlines = set(counts_genes.index) - set(prev_counts_genes.index) \n",
    "newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading internal_20Q3_counts...\n",
      "hitting https://cds.team/taiga/api/datafile/7e00c071007049999c47607db08bf88d\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: internal_20Q3_counts properly converted and uploaded\n",
      "Uploading internal_20Q3_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/7e00c071007049999c47607db08bf88d\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: internal_20Q3_tpm properly converted and uploaded\n",
      "Uploading internal_20Q3_proteincoding_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/7e00c071007049999c47607db08bf88d\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: internal_20Q3_proteincoding_tpm properly converted and uploaded\n",
      "Uploading internal_20Q3_transcripts_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/7e00c071007049999c47607db08bf88d\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: internal_20Q3_transcripts_tpm properly converted and uploaded\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id c9cca827e51f4692aa53f5c4de6a23e9 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/c9cca827e51f4692aa53f5c4de6a23e9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c9cca827e51f4692aa53f5c4de6a23e9'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.update_dataset(dataset_permaname=\"depmap-rnaseq-expression-data-363a\",\n",
    "                 upload_file_path_dict={'temp/internal_'+release+'_counts': 'NumericMatrixCSV',\n",
    "                                       'temp/internal_'+release+'_tpm': 'NumericMatrixCSV',\n",
    "                                       'temp/internal_'+release+'_proteincoding_tpm': 'NumericMatrixCSV',\n",
    "                                       'temp/internal_'+release+'_transcripts_tpm': 'NumericMatrixCSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# INTERNAL RNA\n",
    "\n",
    "* Version 1-3 Internal 18Q1*\n",
    "\n",
    "All CCLE cell lines with RNAseq data.\n",
    "\n",
    "Original data source:\n",
    "\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_depMap_18Q1_RNAseq_reads_20180201.gct`\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_depMap_18Q1_RNAseq_RPKM_20180201.gct`\n",
    "\n",
    "Version 2 of RPKM data are log2-transformed with a noisy floor at -3 (-3 + N(0, 0.1))\n",
    "\n",
    "* Version 4-6 Internal 18Q2*\n",
    "\n",
    "Original data source:\n",
    "\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_depMap_18q2_RNAseq_reads_20180420.gct` `/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_depMap_18q2_RNAseq_RPKM_20180420.gct`\n",
    "\n",
    "RPKM data are log2-transformed with a noisy floor at -3 (-3 + N(0, 0.1)). Reads file unaltered aside from formatting row / column names.\n",
    "\n",
    "* Version 7 Internal 18Q2*\n",
    "\n",
    "Includes a matrix with genes filtered by HGNC protein-coding gene locus group.\n",
    "\n",
    "* Version 8-10 Internal 18Q3*\n",
    "\n",
    "use version 10\n",
    "\n",
    "Original data source:\n",
    "\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_depMap_18q3_RNAseq_reads_20180716.gct` `/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_depMap_18q3_RNAseq_RPKM_20180716.gct`\n",
    "\n",
    "RPKM data are log2-transformed with a noisy floor at -3 (-3 + N(0, 0.1)). Reads file unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are Broad (Arxspan) cell line IDs.\n",
    "\n",
    "Columns: In the complete RPKM and read datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding RPKM and read datasets column names are HGNC_symbol (Entrez_ID)\n",
    "\n",
    "version 9 updates names, and slightly different RPKM values due to randomly added noisy floor (using a seed of 4)\n",
    "\n",
    "version 10 removes duplicate gene names from the protein coding datasets\n",
    "\n",
    "* Version 11-12 Internal 18Q4*\n",
    "\n",
    "18Q4 transcript level data is found in version 14. (In versions 1-13 transcript data contains only gene level not transcript level data)\n",
    "\n",
    "changing to TPM expression\n",
    "\n",
    "Original data source:\n",
    "\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_DepMap_18Q4_rsem_genes_tpm_20181029.txt` `/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_DepMap_18Q4_rsem_transcripts_tpm_20181029.gct` `/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_DepMap_18Q4_RNAseq_reads_20181029.gct` `/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_DepMap_18Q4_RNAseq_RPKM_20181029.gct`\n",
    "\n",
    "TPM data is the primary expression data now. It is log2-transformed with a pseudo count of 1 added. The TPM data contains 4 cell lines not included in the RPKM data.\n",
    "\n",
    "RPKM data are log2-transformed with a pseudo count of 1 added. RPKM values are no longer thresholded.\n",
    "\n",
    "Reads/transcript files unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are DepMap (Arxspan) cell line IDs\n",
    "\n",
    "Columns: In the complete TPM, TPM transcripts, RPKM and read datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding TPM dataset column names are HGNC_symbol (Entrez_ID)\n",
    "\n",
    "* Version 13-15 Internal 19Q1*\n",
    "\n",
    "Version 15 contains the correct data sets for 19Q1 - 2 cell lines are removed\n",
    "\n",
    "Version 14 contains the correct transcript level data for 18Q4\n",
    "\n",
    "* Version 16 Internal 19Q2*\n",
    "\n",
    "* Version 17 Internal 19Q3*\n",
    "\n",
    "* Version 18 Internal 19Q4*\n",
    "\n",
    "Adding 93 new cell lines - Blacklisted\n",
    "Some cells lines have been removed because they:\n",
    "\n",
    " - had too many 0 values = [\"ACH-001388\" \"ACH-001577\" \"ACH-001767\" \"ACH-002463\"] \n",
    "\n",
    "Some cells lines have been flagged as:\n",
    "\n",
    " - None\n",
    "* Version 19 Internal 19Q4\n",
    "removing blacklisted\n",
    "\n",
    "* Version 20 Internal 19Q4\n",
    "removing blacklisted in transcripts\n",
    "\n",
    "* Version 21 Internal 19Q4\n",
    "uploading as matrices \n",
    "\n",
    "* Version 22 Internal 20Q1\n",
    "adding 6 new cell lines\n",
    "\n",
    "* Version 23 Internal 20Q2\n",
    "adding  new cell lines\n",
    "\n",
    "* Version 24 Internal 20Q2\n",
    "adding  back ACH-000052\n",
    "\n",
    "* Version 25 Internal 20Q3\n",
    "nothing different from  20Q2. no new cell lines added\n",
    "\n",
    "* Version 26 Internal 20Q3\n",
    "some lines were wrongly added to the blacklist\n",
    "\n",
    "data is aligned to hg38\n",
    "\n",
    "read count data is created using RSEM, which, when a read maps to multiple places, splits the counts between genes, with the weight based on the likelihood that it came from one gene or the other, so counts data may not be integers\n",
    "\n",
    "TPM data is log2-transformed with a pseudo count of 1 added. log2(X+1)\n",
    "\n",
    "Reads/transcript files unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are DepMap cell line IDs Mapping between Broad IDs and CCLE IDs can be done using a R or python package\n",
    "\n",
    "To install R implementation: options(repos = c(\"https://iwww.broadinstitute.org/~datasci/R-packages\", \"https://cran.cnr.berkeley.edu\")) install.packages('celllinemapr')\n",
    "\n",
    "To install python implementation: pip install https://intranet.broadinstitute.org/~datasci/python-packages/cell_line_mapper-0.1.9.tar.gz)\n",
    "\n",
    "Columns: In the complete TPM and read counts datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding TPM dataset column names are HGNC_symbol (Entrez_ID). In the TPM transcript dataset column names are HGNC_symbol (Transcript_ID) - the HGNC symbols are not unique, use the transcript IDs for unique identifiers.\n",
    "\n",
    "\n",
    "NEW LINES:\n",
    "\"\"\"+newlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CCLE_RNAseq_reads', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_counts'), ('CCLE_expression_full', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_tpm'), ('CCLE_expression', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_proteincoding_tpm'), ('CCLE_RNAseq_transcripts', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_transcripts_tpm'), ('SCORE_gene_effect', 'sanger-crispr-project-score--e20b.4/gene_effect'), ('Achilles_raw_readcounts', 'avana-internal-20q2-3b07.1/raw_readcounts'), ('SCORE_raw_readcounts', 'sanger-crispr-readcounts-2fa9.1/SangerRawReadcounts_filtered'), ('SCORE_essential_genes', 'sanger-crispr-project-score--e20b.4/pan_dependent_genes'), ('Achilles_gene_effect', 'avana-internal-20q2-3b07.1/gene_effect'), ('Achilles_guide_map', 'avana-internal-20q2-3b07.1/guide_gene_map'), ('Achilles_gene_effect_unscaled', 'avana-internal-20q2-3b07.1/gene_effect_unscaled'), ('Achilles_README', 'avana-internal-19q3-67a9.3/README'), ('CCLE_gene_cn', 'depmap-wes-cn-data-81a7.24/internal_20Q3_gene_cn'), ('D2_common_essentials', 'demeter2-combined-dc9c.16/pan_dependent_genes'), ('CCLE_fusions_unfiltered', 'gene-fusions-8b7a.12/unfiltered_fusions_20Q2'), ('Achilles_logfold_change_failures', 'avana-internal-20q2-3b07.1/logfold_change_failures'), ('SCORE_guide_efficacy', 'sanger-crispr-project-score--e20b.4/guide_efficacy'), ('SCORE_logfold_change', 'sanger-crispr-project-score--e20b.4/logfold_change'), ('Achilles_replicate_map', 'avana-internal-20q2-3b07.1/replicate_map'), ('CCLE_mutations', 'depmap-mutation-calls-9be3.24/depmap_20Q3_mutation_calls'), ('Achilles_high_variance_genes', 'avana-internal-20q2-3b07.1/high_variance_genes'), ('Achilles_common_essentials', 'avana-internal-20q2-3b07.1/pan_dependent_genes'), ('SCORE_replicate_map', 'sanger-crispr-project-score--e20b.4/replicate_map'), ('Achilles_raw_readcounts_failures', 'avana-internal-20q2-3b07.1/raw_readcounts_failing'), ('Achilles_logfold_change', 'avana-internal-20q2-3b07.1/logfold_change'), ('TD_summary_table', 'master-table-26f2.3/summary_table'), ('CCLE_segment_cn', 'depmap-wes-cn-data-81a7.24/internal_20Q3_segs_cn'), ('CCLE_fusions', 'gene-fusions-8b7a.12/filtered_fusions_20Q2'), ('SCORE_gene_effect_unscaled', 'sanger-crispr-project-score--e20b.4/gene_effect_unscaled'), ('Achilles_dropped_guides', 'avana-internal-20q2-3b07.1/dropped_guides'), ('Achilles_replicate_QC_report_failing', 'avana-internal-19q4-4c36.4/replicate_QC_report_failing'), ('D2_gene_effect', 'demeter2-combined-dc9c.16/gene_effect'), ('D2_hairpin_parameters', 'demeter2-combined-dc9c.16/hp_data_comb'), ('CCLE_README', 'ccle-readmes-b1a1.4/Internal_CCLE_README'), ('Achilles_guide_efficacy', 'avana-internal-20q2-3b07.1/guide_efficacy'), ('D2_gene_dependency', 'demeter2-combined-dc9c.16/gene_dependency'), ('Achilles_gene_dependency', 'avana-internal-20q2-3b07.1/gene_dependency'), ('SCORE_guide_map', 'sanger-crispr-project-score--e20b.4/guide_gene_map'), ('D2_cell_parameters', 'demeter2-combined-dc9c.16/CL_data_comb'), ('SCORE_gene_dependency', 'sanger-crispr-project-score--e20b.4/gene_dependency'), ('nonessentials', 'avana-internal-20q2-3b07.1/nonessential_genes'), ('sample_info', 'depmap-a0ab.6/sample_info'), ('common_essentials', 'avana-internal-20q2-3b07.1/essential_genes')]\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datafile/bf74b640f26f44b9a26cf02f83a366ab\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 486566421b234972985dc78f0f768d0d created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/486566421b234972985dc78f0f768d0d\n",
      "[('CCLE_RNAseq_reads', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_counts'), ('CCLE_expression_full', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_tpm'), ('CCLE_expression', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_proteincoding_tpm'), ('CCLE_RNAseq_transcripts', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_transcripts_tpm'), ('CCLE_mutations', 'depmap-mutation-calls-9be3.24/depmap_20Q3_mutation_calls'), ('CCLE_segment_cn', 'depmap-wes-cn-data-81a7.24/internal_20Q3_segs_cn'), ('CCLE_gene_cn', 'depmap-wes-cn-data-81a7.24/internal_20Q3_gene_cn')]\n",
      "hitting https://cds.team/taiga/api/datafile/a8943237c51d40958db1125f3862493d\n",
      "hitting https://cds.team/taiga/api/datafile/a8943237c51d40958db1125f3862493d\n",
      "hitting https://cds.team/taiga/api/datafile/a8943237c51d40958db1125f3862493d\n",
      "hitting https://cds.team/taiga/api/datafile/a8943237c51d40958db1125f3862493d\n",
      "hitting https://cds.team/taiga/api/datafile/a8943237c51d40958db1125f3862493d\n",
      "hitting https://cds.team/taiga/api/datafile/a8943237c51d40958db1125f3862493d\n",
      "hitting https://cds.team/taiga/api/datafile/a8943237c51d40958db1125f3862493d\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 1f383009b3d14e789d32db81d0f712be created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/1f383009b3d14e789d32db81d0f712be\n"
     ]
    }
   ],
   "source": [
    "AddToVirtual('depmap-a0ab', \"depmap-rnaseq-expression-data-363a\", files=[('CCLE_RNAseq_reads', 'internal_'+release+'_counts'),('CCLE_expression_full', 'internal_'+release+'_tpm'),('CCLE_expression', 'internal_'+release+'_proteincoding_tpm'),('CCLE_RNAseq_transcripts', 'internal_'+release+'_transcripts_tpm')])\n",
    "\n",
    "AddToVirtual(virtual_internal, \"depmap-rnaseq-expression-data-363a\", files=[('CCLE_RNAseq_reads', 'internal_'+release+'_counts'),('CCLE_expression_full', 'internal_'+release+'_tpm'),('CCLE_expression', 'internal_'+release+'_proteincoding_tpm'),('CCLE_RNAseq_transcripts', 'internal_'+release+'_transcripts_tpm')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM\n",
    "\n",
    "like internal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1415\n",
      "1371\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-4754ac425164>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcounts_genes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounts_genes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mcounts_genes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrna_ibm_embargo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_genes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcounts_genes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'temp/dmc_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_counts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpm_genes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtpm_genes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpm_genes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtpm_genes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrna_ibm_embargo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3226\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m         )\n\u001b[0;32m-> 3228\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnicodeWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mwriter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0;31m# self.data is a preallocated list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_loc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         ix = data_index.to_native_types(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## removing first blacklisted, then embargoed, to create two datasets\n",
    "print(len(counts_genes))\n",
    "counts_genes = counts_genes[~counts_genes.index.isin(rna_ibm_embargo)]\n",
    "print(len(counts_genes))\n",
    "counts_genes.to_csv('temp/dmc_'+release+'_counts')\n",
    "print(len(tpm_genes))\n",
    "tpm_genes = tpm_genes[~tpm_genes.index.isin(rna_ibm_embargo)]\n",
    "print(len(tpm_genes))\n",
    "tpm_genes.to_csv('temp/dmc_'+release+'_tpm')\n",
    "print(len(tpm_proteincoding))\n",
    "tpm_proteincoding = tpm_proteincoding[~tpm_proteincoding.index.isin(rna_ibm_embargo)]\n",
    "print(len(tpm_proteincoding))\n",
    "tpm_proteincoding.to_csv('temp/dmc_'+release+'_proteincoding_tpm')\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts = tpm_transcripts[~tpm_transcripts.index.isin(rna_ibm_embargo)]\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts.to_csv('temp/dmc_'+release+'_transcripts_tpm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_counts_genes = tc.get(name='depmap-rnaseq-expression-data-80ef', version=17, file='dmc_'+prevname+'_counts')\n",
    "print('shoud be None')\n",
    "print(set(prev_counts_genes.index) - set(counts_genes.index))\n",
    "print(\"new lines\")\n",
    "newlines = set(counts_genes.index) - set(prev_counts_genes.index) \n",
    "newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading dmc_20Q3_counts...\n",
      "hitting https://cds.team/taiga/api/datafile/ead2963bd9944894bcafdbf9f8dd29b5\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: dmc_20Q3_counts properly converted and uploaded\n",
      "Uploading dmc_20Q3_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/ead2963bd9944894bcafdbf9f8dd29b5\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: dmc_20Q3_tpm properly converted and uploaded\n",
      "Uploading dmc_20Q3_proteincoding_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/ead2963bd9944894bcafdbf9f8dd29b5\n",
      "Conversion and upload...:\n",
      "\t Waiting in the task queue\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: dmc_20Q3_proteincoding_tpm properly converted and uploaded\n",
      "Uploading dmc_20Q3_transcripts_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/ead2963bd9944894bcafdbf9f8dd29b5\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: dmc_20Q3_transcripts_tpm properly converted and uploaded\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 8dfe1e60a50b4fb78e93f2a75f61da12 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/8dfe1e60a50b4fb78e93f2a75f61da12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8dfe1e60a50b4fb78e93f2a75f61da12'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.update_dataset(dataset_permaname=\"depmap-rnaseq-expression-data-80ef\",\n",
    "                 upload_file_path_dict={'temp/dmc_'+release+'_counts': 'NumericMatrixCSV',\n",
    "                                       'temp/dmc_'+release+'_tpm': 'NumericMatrixCSV',\n",
    "                                       'temp/dmc_'+release+'_proteincoding_tpm': 'NumericMatrixCSV',\n",
    "                                       'temp/dmc_'+release+'_transcripts_tpm': 'NumericMatrixCSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# DMC RNA\n",
    "\n",
    "* Version 1-3 DMC 19Q1*\n",
    "\n",
    "version 3 contains the correct data for 19Q1\n",
    "\n",
    "version 2 contains correct TPM transcript data (in version 1 transcript data contains only gene level not transcript level data)\n",
    "\n",
    "* Version 4 DMC 19Q2*\n",
    "\n",
    "* Version 5 DMC 19Q3*\n",
    "\n",
    "* Version 6 DMC 19Q4*\n",
    "\n",
    "Adding 93 new cell lines - Blacklisted - IBM\n",
    "Some cells lines have been removed because they:\n",
    "\n",
    " - had too many 0 values = [\"ACH-001388\" \"ACH-001577\" \"ACH-001767\" \"ACH-002463\"] \n",
    "\n",
    "Some cells lines have been flagged as:\n",
    "\n",
    " - None\n",
    "\n",
    "* Version 7 DMC 19Q4\n",
    "\n",
    "removing blacklisted\n",
    "\n",
    "* Version 8 DMC 19Q4\n",
    "\n",
    "removing blacklisted in transcripts\n",
    "\n",
    "* Version 9 DMC 19Q4\n",
    "\n",
    "uploading as numeric matrix\n",
    "\n",
    "* Version 10 DMC 20Q1\n",
    "unknown reupload\n",
    "\n",
    "* Version 11 DMC 20Q2\n",
    "adding  new cell lines\n",
    "\n",
    "* Version 12 DMC 20Q2\n",
    "unknown reupload\n",
    "\n",
    "* Version 13 DMC 20Q2\n",
    "removing some missed blacklisted lines\n",
    "\n",
    "* Version 14 DMC 20Q2\n",
    "Adding one missing line\n",
    "\n",
    "* Version 15 DMC 20Q2\n",
    "nothing different from  20Q2. no new cell lines added\n",
    "\n",
    "* Version 16 DMC 20Q3\n",
    "Some wrong annotations in the blacklists\n",
    "\n",
    "* Version 17 DMC 20Q3\n",
    "Updated annotations in the blacklists\n",
    "\n",
    "\n",
    "data is aligned to hg38\n",
    "\n",
    "read count data is created using RSEM, which, when a read maps to multiple places, splits the counts between genes, with the weight based on the likelihood that it came from one gene or the other, so counts data may not be integers\n",
    "\n",
    "TPM data is log2-transformed with a pseudo count of 1 added.\n",
    "\n",
    "Reads/transcript files unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are DepMap cell line IDs\n",
    "\n",
    "Columns: In the complete TPM and read counts datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding TPM dataset column names are HGNC_symbol (Entrez_ID). In the TPM transcript dataset column names are HGNC_symbol (Transcript_ID) - the HGNC symbols are not unique, use the transcript IDs for unique identifiers.\n",
    "\n",
    "NEW LINES:\n",
    "\"\"\"+newlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CCLE_RNAseq_reads', 'depmap-rnaseq-expression-data-80ef.17/dmc_20Q3_counts'), ('CCLE_expression_full', 'depmap-rnaseq-expression-data-80ef.17/dmc_20Q3_tpm'), ('CCLE_expression', 'depmap-rnaseq-expression-data-80ef.17/dmc_20Q3_proteincoding_tpm'), ('CCLE_RNAseq_transcripts', 'depmap-rnaseq-expression-data-80ef.17/dmc_20Q3_transcripts_tpm'), ('Achilles_raw_readcounts_failures', 'avana-consortium-20q3-902b.1/raw_readcounts_failing'), ('Achilles_logfold_change_failures', 'avana-consortium-20q3-902b.1/logfold_change_failures'), ('Achilles_dropped_guides', 'avana-consortium-20q3-902b.1/dropped_guides'), ('CCLE_segment_cn', 'depmap-cn-data-9b9d.15/dmc_20Q3_segs_cn'), ('Achilles_replicate_map', 'avana-consortium-20q3-902b.1/replicate_map'), ('Achilles_guide_map', 'avana-consortium-20q3-902b.1/guide_gene_map'), ('CCLE_fusions', 'gene-fusions-375f.9/filtered_fusions_20Q3'), ('Achilles_gene_effect', 'avana-consortium-20q3-902b.1/gene_effect'), ('Achilles_common_essentials', 'avana-consortium-20q3-902b.1/pan_dependent_genes'), ('Achilles_high_variance_genes', 'avana-consortium-20q3-902b.1/high_variance_genes'), ('Achilles_gene_effect_unscaled', 'avana-consortium-20q3-902b.1/gene_effect_unscaled'), ('CCLE_gene_cn', 'depmap-cn-data-9b9d.15/dmc_20Q3_gene_cn'), ('CCLE_fusions_unfiltered', 'gene-fusions-375f.9/unfiltered_fusions_20Q3'), ('common_essentials', 'avana-consortium-20q3-902b.1/essential_genes'), ('Achilles_logfold_change', 'avana-consortium-20q3-902b.1/logfold_change'), ('Achilles_guide_efficacy', 'avana-consortium-20q3-902b.1/guide_efficacy'), ('CCLE_mutations', 'depmap-mutation-calls-dfce.17/depmap_20Q3_mutation_calls'), ('Achilles_raw_readcounts', 'avana-consortium-20q3-902b.1/raw_readcounts'), ('Achilles_gene_dependency', 'avana-consortium-20q3-902b.1/gene_dependency'), ('nonessentials', 'avana-consortium-20q3-902b.1/nonessential_genes')]\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datafile/380b7e129fae45d683cdcd07f6cdb943\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id b0369b982e604e65a06a6426a558843f created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/b0369b982e604e65a06a6426a558843f\n"
     ]
    }
   ],
   "source": [
    "AddToVirtual(virtual_dmc, \"depmap-rnaseq-expression-data-80ef\", files=[('CCLE_RNAseq_reads', 'dmc_'+release+'_counts'),('CCLE_expression_full', 'dmc_'+release+'_tpm'),('CCLE_expression', 'dmc_'+release+'_proteincoding_tpm'),('CCLE_RNAseq_transcripts', 'dmc_'+release+'_transcripts_tpm')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20Q1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevprevname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevprevcells = set(tc.get(name='depmap-rnaseq-expression-data-363a', file='internal_'+prevprevname+'_proteincoding_tpm',version=prevprevversion).index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1371\n",
      "1333\n",
      "1333\n",
      "1333\n",
      "1371\n",
      "1333\n",
      "1325\n"
     ]
    }
   ],
   "source": [
    "## removing first blacklisted, then embargoed, to create two datasets\n",
    "print(len(counts_genes))\n",
    "counts_genes = counts_genes[counts_genes.index.isin(prevprevcells)]\n",
    "print(len(counts_genes))\n",
    "counts_genes[~counts_genes.index.isin(rna_dmc_embargo)].to_csv('temp/public_'+release+'_counts')\n",
    "tpm_genes = tpm_genes[tpm_genes.index.isin(prevprevcells)]\n",
    "print(len(tpm_genes))\n",
    "tpm_genes[~tpm_genes.index.isin(rna_dmc_embargo)].to_csv('temp/public_'+release+'_tpm')\n",
    "tpm_proteincoding = tpm_proteincoding[tpm_proteincoding.index.isin(prevprevcells)]\n",
    "print(len(tpm_proteincoding))\n",
    "tpm_proteincoding[~tpm_proteincoding.index.isin(rna_dmc_embargo)].to_csv('temp/public_'+release+'_proteincoding_tpm')\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts = tpm_transcripts[tpm_transcripts.index.isin(prevprevcells)]\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts = tpm_transcripts[~tpm_transcripts.index.isin(rna_dmc_embargo)]\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts.to_csv('temp/public_'+release+'_transcripts_tpm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##################]100% |  20.9 MiB/s | 596.7 MiB / 596.7 MiB | Time:  0:00:28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shoud be None\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'counts_genes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3710d23b860a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprev_counts_genes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'depmap-rnaseq-expression-data-ccd0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'public_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mprevname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_counts'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shoud be None'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_counts_genes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_genes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new lines\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnewlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_genes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_counts_genes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'counts_genes' is not defined"
     ]
    }
   ],
   "source": [
    "prev_counts_genes = tc.get(name='depmap-rnaseq-expression-data-ccd0', version=24, file='public_'+prevname+'_counts')\n",
    "print('shoud be None')\n",
    "print(set(prev_counts_genes.index) - set(counts_genes.index))\n",
    "print(\"new lines\")\n",
    "newlines = set(counts_genes.index) - set(prev_counts_genes.index) \n",
    "newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading public_20Q3_counts...\n",
      "hitting https://cds.team/taiga/api/datafile/ae88fc7204d04ba18cc77038b495738b\n",
      "Conversion and upload...:\n",
      "\t Waiting in the task queue\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: public_20Q3_counts properly converted and uploaded\n",
      "Uploading public_20Q3_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/ae88fc7204d04ba18cc77038b495738b\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: public_20Q3_tpm properly converted and uploaded\n",
      "Uploading public_20Q3_proteincoding_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/ae88fc7204d04ba18cc77038b495738b\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: public_20Q3_proteincoding_tpm properly converted and uploaded\n",
      "Uploading public_20Q3_transcripts_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/ae88fc7204d04ba18cc77038b495738b\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: public_20Q3_transcripts_tpm properly converted and uploaded\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 7b5dc3f906ee426c922b1a8914734731 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/7b5dc3f906ee426c922b1a8914734731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'7b5dc3f906ee426c922b1a8914734731'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.update_dataset(dataset_permaname=\"depmap-rnaseq-expression-data-ccd0\",\n",
    "                 upload_file_path_dict={'temp/public_'+release+'_counts': 'NumericMatrixCSV',\n",
    "                                       'temp/public_'+release+'_tpm': 'NumericMatrixCSV',\n",
    "                                       'temp/public_'+release+'_proteincoding_tpm': 'NumericMatrixCSV',\n",
    "                                       'temp/public_'+release+'_transcripts_tpm': 'NumericMatrixCSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# PUBLIC RNA\n",
    "\n",
    "* Version 1-2 Public 18Q1*\n",
    "\n",
    "Original source (`CCLE_DepMap_18Q1_RNAseq_reads_20180214.gct`, `CCLE_DepMap_18Q1_RNAseq_RPKM_20180214.gct`) downloaded from portals.broadinstitute.org/ccle\n",
    "RPKM file is log2(RPKM) with a \"noisy floor\" around -3 (-3 + N(0, 0.1))\n",
    "\n",
    "* Version 3-5 Public 18Q2*\n",
    "\n",
    "gene expression data (RNAseq for1,076 cell lines, including data for 28 newly released cell lines)\n",
    "\n",
    "original source: (`/xchip/ccle_dist/public/DepMap_18Q2/CCLE_DepMap_18Q2_RNAseq_RPKM_20180502.gct`, `/xchip/ccle_dist/public/DepMap_18Q2/CCLE_DepMap_18Q2_RNAseq_reads_20180502.gct`)\n",
    "* Version 6-7 Public 18Q3*\n",
    "\n",
    "gene expression data (80 newly released cell lines)\n",
    "\n",
    "Original data source:\n",
    "\n",
    "`/xchip/ccle_dist/public/DepMap_18Q3/CCLE_DepMap_18q3_RNAseq_reads_20180718.gct`\n",
    "`/xchip/ccle_dist/public/DepMap_18Q3/CCLE_DepMap_18q3_RNAseq_RPKM_20180718.gct`\n",
    "\n",
    "RPKM data are log2-transformed with a noisy floor at -3 (-3 + N(0, 0.1)). Reads file unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are Broad (Arxspan) cell line IDs. Mapping between Broad IDs and CCLE IDs can be done using a R or python package\n",
    "\n",
    "To install R implementation: options(repos = c(\"https://iwww.broadinstitute.org/~datasci/R-packages\", \"https://cran.cnr.berkeley.edu\")) install.packages('celllinemapr')\n",
    "\n",
    "To install python implementation: pip install https://intranet.broadinstitute.org/~datasci/python-packages/cell_line_mapper-0.1.9.tar.gz)\n",
    "\n",
    "Columns: In the complete RPKM and read datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding RPKM and read datasets column names are HGNC_symbol (Entrez_ID)\n",
    "\n",
    "version 7 removes duplicate gene names from the protein coding datasets\n",
    "\n",
    "* Version 8-9 Public 18Q4*\n",
    "\n",
    "_ 18Q4 transcript level data is found in version 11. (In versions 8-9 transcript data contains only gene level not transcript level data)\n",
    "\n",
    "changing to TPM expression\n",
    "\n",
    "Original data source:\n",
    "\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_DepMap_18Q4_rsem_genes_tpm_20181029.txt`\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_DepMap_18Q4_rsem_transcripts_tpm_20181029.gct`\n",
    "`/xchip/ccle_dist/public/DepMap_18Q4/CCLE_DepMap_18q4_RNAseq_reads_20181029.gct`\n",
    "\n",
    "`/xchip/ccle_dist/public/DepMap_18Q4/CCLE_DepMap_18q4_RNAseq_RPKM_20181029.gct`\n",
    "\n",
    "TPM data is subsetted to just public cell lines using the cell line found in the RPKM dataset.\n",
    "\n",
    "TPM data is the primary expression data now. It is log2-transformed with a pseudo count of 1 added\n",
    "\n",
    "RPKM data are log2-transformed with a pseudo count of 1 added. RPKM values are no longer thresholded.\n",
    "\n",
    "Reads/transcript files unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are DepMap (Arxspan) cell line IDs\n",
    "\n",
    "Columns: In the complete TPM, TPM transcripts, RPKM and read datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding TPM and read datasets column names are HGNC_symbol (Entrez_ID)\n",
    "\n",
    "* Version 10-12 Public 19Q1*\n",
    "\n",
    "version 12 contains the correct data for 19Q1\n",
    "\n",
    "version 11 contains the correct transcript level data for 19Q1 and 18Q4\n",
    "\n",
    "* Version 13 Public 19Q2*\n",
    "\n",
    "* Version 14 Public 19Q3*\n",
    "\n",
    "* Version 15 Public 19Q4*\n",
    "Adding 93 new cell lines - Blacklisted - IBM - DMC\n",
    "Some cells lines have been removed because they:\n",
    "\n",
    " - had too many 0 values = [\"ACH-001388\" \"ACH-001577\" \"ACH-001767\" \"ACH-002463\"] \n",
    "\n",
    "Some cells lines have been flagged as:\n",
    "\n",
    " - None\n",
    " \n",
    "* Version 16 Public 19Q4*\n",
    "removing unauthorized cell lines\n",
    "\n",
    "* Version 17 Public 20Q1*\n",
    "adding 6 new lines\n",
    "\n",
    "* Version 18 Public 20Q1\n",
    "Unknown reupdate\n",
    "\n",
    "* Version 19 Public 20Q2\n",
    "adding  new cell lines\n",
    "\n",
    "* Version 20 Public 20Q2\n",
    "unknown reupload\n",
    "\n",
    "* Version 21 Public 20Q2\n",
    "removing some missed blacklisted lines\n",
    "\n",
    "* Version 22 Public 20Q3\n",
    "nothing different from  20Q2. no new cell lines added\n",
    "\n",
    "* Version 23 Public 20Q3\n",
    "Some wrong annotations in the blacklists\n",
    "\n",
    "* Version 24 Public 20Q3\n",
    "Updated annotations in the blacklists\n",
    "\n",
    "data is hg38 aligned\n",
    "\n",
    "read count data is created using RSEM, which, when a read maps to multiple places, splits the counts between genes, with the weight based on the likelihood that it came from one gene or the other, so counts data may not be integers\n",
    "\n",
    "TPM data is log2-transformed with a pseudo count of 1 added. log2(X+1)\n",
    "\n",
    "Reads/transcript files unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are DepMap cell line IDs\n",
    "\n",
    "Columns: In the complete TPM and read counts datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding TPM dataset column names are HGNC_symbol (Entrez_ID). In the TPM transcript dataset column names are HGNC_symbol (Transcript_ID) - the HGNC symbols are not unique, use the transcript IDs for unique identifiers.\n",
    "\n",
    "NEW LINES:\n",
    "\"\"\"+newlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'public-20q3-3d35'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virtual_public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CCLE_RNAseq_reads', 'depmap-rnaseq-expression-data-ccd0.24/public_20Q3_counts'), ('CCLE_expression_full', 'depmap-rnaseq-expression-data-ccd0.24/public_20Q3_tpm'), ('CCLE_expression', 'depmap-rnaseq-expression-data-ccd0.24/public_20Q3_proteincoding_tpm'), ('CCLE_RNAseq_transcripts', 'depmap-rnaseq-expression-data-ccd0.24/public_20Q3_transcripts_tpm'), ('Achilles_replicate_map', 'avana-public-tentative-20q3-3e73.4/replicate_map'), ('Achilles_guide_map', 'avana-public-tentative-20q3-3e73.4/guide_gene_map'), ('CCLE_gene_cn', 'depmap-wes-cn-data-97cc.33/public_20Q3_gene_cn'), ('Achilles_gene_dependency', 'avana-public-tentative-20q3-3e73.4/gene_dependency'), ('common_essentials', 'avana-public-tentative-20q3-3e73.4/essential_genes'), ('Achilles_high_variance_genes', 'avana-public-tentative-20q3-3e73.4/high_variance_genes'), ('Achilles_logfold_change_failures', 'avana-public-tentative-20q3-3e73.4/logfold_change_failures'), ('Achilles_dropped_guides', 'avana-public-tentative-20q3-3e73.4/dropped_guides'), ('Achilles_logfold_change', 'avana-public-tentative-20q3-3e73.4/logfold_change'), ('CCLE_fusions_unfiltered', 'gene-fusions-6212.13/unfiltered_fusions_20Q3'), ('CCLE_segment_cn', 'depmap-wes-cn-data-97cc.33/public_20Q3_segs_cn'), ('Achilles_raw_readcounts', 'avana-public-tentative-20q3-3e73.4/raw_readcounts'), ('CCLE_mutations', 'depmap-mutation-calls-9a1a.20/depmap_20Q3_mutation_calls'), ('CCLE_fusions', 'gene-fusions-6212.13/filtered_fusions_20Q3'), ('Achilles_gene_effect', 'avana-public-tentative-20q3-3e73.4/gene_effect'), ('nonessentials', 'avana-public-tentative-20q3-3e73.4/nonessential_genes'), ('Achilles_gene_effect_unscaled', 'avana-public-tentative-20q3-3e73.4/gene_effect_unscaled'), ('Achilles_raw_readcounts_failures', 'avana-public-tentative-20q3-3e73.4/raw_readcounts_failing'), ('Achilles_common_essentials', 'avana-public-tentative-20q3-3e73.4/pan_dependent_genes'), ('Achilles_guide_efficacy', 'avana-public-tentative-20q3-3e73.4/guide_efficacy')]\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datafile/3c7a14e6f8c9482fa13f6105113ec520\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 823531cbee3d4c47a7ed0234d52faadc created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/823531cbee3d4c47a7ed0234d52faadc\n"
     ]
    }
   ],
   "source": [
    "AddToVirtual(virtual_public, \"depmap-rnaseq-expression-data-ccd0\", files=[('CCLE_RNAseq_reads', 'public_'+release+'_counts'),('CCLE_expression_full', 'public_'+release+'_tpm'),('CCLE_expression', 'public_'+release+'_proteincoding_tpm'),('CCLE_RNAseq_transcripts', 'public_'+release+'_transcripts_tpm')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra.waitForSubmission(refworkspace, submission_id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id2 = refwm.create_submission(\"Aggregate_Fusion_Calls\", 'All_samples')\n",
    "terra.waitForSubmission(refworkspace, submission_id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = refwm.get_sample_sets().loc['All_samples']['fusions_star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp $aggregated \"temp/expression.fusion.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('temp/expression.fusion.tsv', header=0, sep='\\t', quotechar='\"', error_bad_lines=False, index_col=None)\n",
    "file[\"DepMap_ID\"] = [i.split('.')[0] for i in file['DepMap_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(file))\n",
    "renaming = removeOlderVersions(names=set(file['DepMap_ID']), refsamples=refwm.get_samples(), arxspan_id=\"arxspan_id\", version=\"version\")\n",
    "file = file[file['DepMap_ID'].isin(renaming.keys())].replace({'DepMap_ID':renaming}).reset_index(drop=True)\n",
    "print(len(file))\n",
    "print(file['DepMap_ID'][:10])\n",
    "file.to_csv('temp/expression.fusion.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This document contains the code used to generate the unfiltered and filtered versions of the fusion datasets for the release. The bottom of the document also contains some comparisons between the release fusion dataset, CCLE2 fusion calls, and the translocation data from CCLE2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "source('../gkugener/RScripts/load_libraries_and_annotations.R')\n",
    "source(\"src/CCLE_postp_function.R\")\n",
    "library('cdsomics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate filtered fusion table\n",
    "\n",
    "Release: `r release`\n",
    "\n",
    "We want to apply filters to the fusion table to reduce the number of artifacts in the dataset. Specifically, we filter the following:\n",
    "\n",
    "* Remove fusions involving mitochondrial chromosomes, or HLA genes, or immunoglobulin genes\n",
    "* Remove red herring fusions (from STAR-Fusion annotations column)\n",
    "* Remove recurrent in CCLE (>= 25 samples)\n",
    "* Remove fusion with (SpliceType=\" INCL_NON_REF_SPLICE\" and LargeAnchorSupport=\"No\" and FFPM < 0.1)\n",
    "* Remove fusions with FFPM < 0.05 (STAR-Fusion suggests using 0.1, but looking at the translocation data, this looks like it might be too aggressive)\n",
    "\n",
    "with the functions:\n",
    "- readFusions\n",
    "- filterFusions\n",
    "- prepare_depmap_fusion_data_for_taiga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "unfiltered_fusions <- readFusions('temp/expression.fusion.tsv')\n",
    "filtered_fusions <- filterFusions(unfiltered_fusions)\n",
    "\n",
    "filtered_fusions <- prepare_depmap_fusion_data_for_taiga(filtered_fusions)\n",
    "unfiltered_fusions <- prepare_depmap_fusion_data_for_taiga(unfiltered_fusions)\n",
    "\n",
    "filtered_fusions <- filtered_fusions[,2:ncol(filtered_fusions)]\n",
    "unfiltered_fusions <- unfiltered_fusions[,2:ncol(unfiltered_fusions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Save the files (to be uploaded to taiga)\n",
    "write.table(\n",
    "  unfiltered_fusions,\n",
    "  file = paste0('temp/fusions_',release, '_unfiltered'),\n",
    "  sep = ',', quote = F, row.names = F\n",
    ")\n",
    "write.table(\n",
    "  filtered_fusions,\n",
    "  file = paste0('temp/fusions_', release, '_filtered'),\n",
    "  sep = ',', quote = F, row.names = F\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to Taiga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.update_dataset(dataset_permaname=\"depmap-fusions-7990\",\n",
    "                     upload_file_path_dict={'temp/fusions_'+release+'_filtered': 'TableCSV',\n",
    "                                        'temp/fusions_'+release+'_unfiltered': 'TableCSV'},\n",
    "                 dataset_description=\"\"\"\n",
    "# Fusions\n",
    "\n",
    "filtered and unfiltered fusion files from Broad RNAseq data mapped to hg38\n",
    "\n",
    "PORTAL TEAM SHOULD NOT USE THIS: There are lines here that should not make it even to internal. Must use subsetted dataset instead. These data will not make it on the portal starting 19Q1. With the DMC portal, there is new cell line release prioritization as to which lines can be included, so a new taiga dataset will be created containing CN for the portal.\n",
    "\n",
    "version 1-4: guillaume releases\n",
    "version 5: 19Q3 release\n",
    "version 6:  adding missing samples in Terra merge files\n",
    "\n",
    "## ** version 7 Internal 19Q4****\n",
    "Adding 17 new cell lines, 3 reprioritized cell lines. log2(COPY RATIO+1). \n",
    "Some cells lines have been flagged as:\n",
    "\n",
    "## ** version 8 Internal 20Q1****\n",
    "Adding 6 new lines\n",
    "\n",
    "## ** version 9 Internal 20Q2****\n",
    "Adding 50 new lines\n",
    "\n",
    "## ** version 10 Internal 20Q3****\n",
    "nothing different from  20Q2. no new cell lines added\n",
    "\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines, IDs contained in the column DepMap_ID\n",
    "LeftGene and RightGene separated by an ampersand (\"&\").\n",
    "\n",
    "Unfiltered data contains all output fusions, while the filtered data uses the filters suggested by the star fusion docs. These filters are:\n",
    "- FFPM > 0.1 -  a cutoff of 0.1 means&nbsp;at least 1 fusion-supporting RNAseq fragment per 10M total reads\n",
    "- Remove known false positives, such as GTEx recurrent fusions and certain paralogs\n",
    "- Genes that are next to each other\n",
    "- Fusions with mitochondrial breakpoints\n",
    "- Removing fusion involving mitochondrial chromosomes or HLA genes\n",
    "- Removed common false positive fusions (red herring annotations as described in the STAR-Fusion docs)\n",
    "- Recurrent fusions observed in CCLE across cell lines (in 25 or more samples)\n",
    "- Removed fusions where SpliceType='INCL_NON_REF_SPLICE' and LargeAnchorSupport='NO_LDAS' and FFPM < 0.1\n",
    "- FFPM < 0.05\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered = pd.read_csv('temp/fusions_'+release+'_unfiltered', sep='\\t')#,index_col=None)\n",
    "filtered = pd.read_csv('temp/fusions._+ release+'_filtered', sep='\\t')#,index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35770\n",
      "35478\n",
      "359778\n",
      "358959\n"
     ]
    }
   ],
   "source": [
    "## removing first blacklisted, then embargoed, to create two datasets\n",
    "print(len(filtered))\n",
    "filtered = filtered[~filtered.DepMap_ID.isin(blacklist)]\n",
    "print(len(filtered))\n",
    "filtered.to_csv('temp/filtered_fusions_'+release, index=False)\n",
    "print(len(unfiltered))\n",
    "unfiltered= unfiltered[~unfiltered.DepMap_ID.isin(blacklist)]\n",
    "print(len(unfiltered))\n",
    "unfiltered.to_csv('temp/unfiltered_fusions_'+release, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_unfilt = tc.get(name='gene-fusions-8b7a', version=14, file='unfiltered_fusions_'+prevname)\n",
    "print('shoud be None')\n",
    "print(set(prev_unfilt.DepMap_ID) - set(unfiltered.DepMap_ID))\n",
    "print(\"new lines\")\n",
    "newlines = set(unfiltered.DepMap_ID) - set(prev_unfilt.DepMap_ID) \n",
    "newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading filtered_fusions_20Q3...\n",
      "hitting https://cds.team/taiga/api/datafile/812cf0eaeee448b29a35d769f8fe47be\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: filtered_fusions_20Q3 properly converted and uploaded\n",
      "Uploading unfiltered_fusions_20Q3...\n",
      "hitting https://cds.team/taiga/api/datafile/812cf0eaeee448b29a35d769f8fe47be\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: unfiltered_fusions_20Q3 properly converted and uploaded\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 2cb8fc0b4b1a4bb2883157e3048b6ac1 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/2cb8fc0b4b1a4bb2883157e3048b6ac1\n",
      "[('CCLE_fusions_unfiltered', 'gene-fusions-8b7a.14/unfiltered_fusions_20Q3'), ('CCLE_fusions', 'gene-fusions-8b7a.14/filtered_fusions_20Q3'), ('CCLE_RNAseq_transcripts', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_transcripts_tpm'), ('Achilles_replicate_QC_report_failing', 'avana-internal-19q4-4c36.4/replicate_QC_report_failing'), ('Achilles_gene_effect_unscaled', 'avana-internal-20q2-3b07.1/gene_effect_unscaled'), ('Achilles_dropped_guides', 'avana-internal-20q2-3b07.1/dropped_guides'), ('SCORE_gene_dependency', 'sanger-crispr-project-score--e20b.4/gene_dependency'), ('Achilles_logfold_change', 'avana-internal-20q2-3b07.1/logfold_change'), ('D2_gene_dependency', 'demeter2-combined-dc9c.16/gene_dependency'), ('Achilles_guide_map', 'avana-internal-20q2-3b07.1/guide_gene_map'), ('D2_cell_parameters', 'demeter2-combined-dc9c.16/CL_data_comb'), ('sample_info', 'depmap-a0ab.6/sample_info'), ('CCLE_mutations', 'depmap-mutation-calls-9be3.24/depmap_20Q3_mutation_calls'), ('Achilles_logfold_change_failures', 'avana-internal-20q2-3b07.1/logfold_change_failures'), ('CCLE_README', 'ccle-readmes-b1a1.4/Internal_CCLE_README'), ('common_essentials', 'avana-internal-20q2-3b07.1/essential_genes'), ('SCORE_guide_map', 'sanger-crispr-project-score--e20b.4/guide_gene_map'), ('Achilles_raw_readcounts_failures', 'avana-internal-20q2-3b07.1/raw_readcounts_failing'), ('CCLE_expression', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_proteincoding_tpm'), ('CCLE_gene_cn', 'depmap-wes-cn-data-81a7.24/internal_20Q3_gene_cn'), ('Achilles_gene_effect', 'avana-internal-20q2-3b07.1/gene_effect'), ('SCORE_replicate_map', 'sanger-crispr-project-score--e20b.4/replicate_map'), ('SCORE_gene_effect_unscaled', 'sanger-crispr-project-score--e20b.4/gene_effect_unscaled'), ('D2_hairpin_parameters', 'demeter2-combined-dc9c.16/hp_data_comb'), ('TD_summary_table', 'master-table-26f2.3/summary_table'), ('nonessentials', 'avana-internal-20q2-3b07.1/nonessential_genes'), ('Achilles_common_essentials', 'avana-internal-20q2-3b07.1/pan_dependent_genes'), ('CCLE_RNAseq_reads', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_counts'), ('SCORE_logfold_change', 'sanger-crispr-project-score--e20b.4/logfold_change'), ('SCORE_gene_effect', 'sanger-crispr-project-score--e20b.4/gene_effect'), ('Achilles_guide_efficacy', 'avana-internal-20q2-3b07.1/guide_efficacy'), ('Achilles_replicate_map', 'avana-internal-20q2-3b07.1/replicate_map'), ('SCORE_essential_genes', 'sanger-crispr-project-score--e20b.4/pan_dependent_genes'), ('D2_gene_effect', 'demeter2-combined-dc9c.16/gene_effect'), ('SCORE_guide_efficacy', 'sanger-crispr-project-score--e20b.4/guide_efficacy'), ('Achilles_raw_readcounts', 'avana-internal-20q2-3b07.1/raw_readcounts'), ('SCORE_raw_readcounts', 'sanger-crispr-readcounts-2fa9.1/SangerRawReadcounts_filtered'), ('D2_common_essentials', 'demeter2-combined-dc9c.16/pan_dependent_genes'), ('Achilles_README', 'avana-internal-19q3-67a9.3/README'), ('CCLE_segment_cn', 'depmap-wes-cn-data-81a7.24/internal_20Q3_segs_cn'), ('Achilles_high_variance_genes', 'avana-internal-20q2-3b07.1/high_variance_genes'), ('Achilles_gene_dependency', 'avana-internal-20q2-3b07.1/gene_dependency'), ('CCLE_expression_full', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_tpm')]\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datafile/a3b0bb31d4ee45db8b72fec850f69c4b\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id ce9ca358fdce4c3ab1a1a03f74d6178a created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/ce9ca358fdce4c3ab1a1a03f74d6178a\n",
      "[('CCLE_fusions_unfiltered', 'gene-fusions-8b7a.14/unfiltered_fusions_20Q3'), ('CCLE_fusions', 'gene-fusions-8b7a.14/filtered_fusions_20Q3'), ('CCLE_gene_cn', 'depmap-wes-cn-data-81a7.24/internal_20Q3_gene_cn'), ('CCLE_segment_cn', 'depmap-wes-cn-data-81a7.24/internal_20Q3_segs_cn'), ('CCLE_RNAseq_reads', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_counts'), ('CCLE_expression_full', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_tpm'), ('CCLE_expression', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_proteincoding_tpm'), ('CCLE_mutations', 'depmap-mutation-calls-9be3.24/depmap_20Q3_mutation_calls'), ('CCLE_RNAseq_transcripts', 'depmap-rnaseq-expression-data-363a.26/internal_20Q3_transcripts_tpm')]\n",
      "hitting https://cds.team/taiga/api/datafile/80630534ae0443fb8d64685f4e225bcf\n",
      "hitting https://cds.team/taiga/api/datafile/80630534ae0443fb8d64685f4e225bcf\n",
      "hitting https://cds.team/taiga/api/datafile/80630534ae0443fb8d64685f4e225bcf\n",
      "hitting https://cds.team/taiga/api/datafile/80630534ae0443fb8d64685f4e225bcf\n",
      "hitting https://cds.team/taiga/api/datafile/80630534ae0443fb8d64685f4e225bcf\n",
      "hitting https://cds.team/taiga/api/datafile/80630534ae0443fb8d64685f4e225bcf\n",
      "hitting https://cds.team/taiga/api/datafile/80630534ae0443fb8d64685f4e225bcf\n",
      "hitting https://cds.team/taiga/api/datafile/80630534ae0443fb8d64685f4e225bcf\n",
      "hitting https://cds.team/taiga/api/datafile/80630534ae0443fb8d64685f4e225bcf\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 205f9667b03f4b4686d480bee9c26334 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/205f9667b03f4b4686d480bee9c26334\n"
     ]
    }
   ],
   "source": [
    "tc.update_dataset(dataset_permaname=\"gene-fusions-8b7a\",\n",
    "                 upload_file_path_dict={'temp/filtered_fusions_'+release: 'TableCSV',\n",
    "                                       'temp/unfiltered_fusions_'+release: 'TableCSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# Internal Fusions\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines\n",
    "\n",
    "Original Raw Data: Generated by Mahmoud Ghandi on April 25, 2017. Can be found at xchip_ccle_dist/broad_only/unpublished_Novartis_data/RNAseq/fusions.txt\n",
    "\n",
    "Version 3: added a column containing the Broad_ID\n",
    "\n",
    "* Version 4-5 Internal 19Q1*\n",
    "\n",
    "version 5 contains the correct data for 19Q1\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines\n",
    "\n",
    "* Version 6 Internal 19Q2*\n",
    "\n",
    "* Version 7*\n",
    "\n",
    "Josh D added \"common_fusion_matrix\".\n",
    "\n",
    "Binary matrix of the most common gene fusions (those where the two involved genes are fused in at least 5 cell lines) with no additional filtering. Use at your own risk.\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines\n",
    "\n",
    "* Version 8 Internal 19Q3*\n",
    "\n",
    "* Version 9 Internal 19Q4*\n",
    "\n",
    "* Version 10 Internal 20Q1*\n",
    "Adding new lines\n",
    "\n",
    "* Version 11 Internal 20Q1*\n",
    "Adding a file\n",
    "\n",
    "* Version 12 Internal 20Q2*\n",
    "Adding 50 new lines\n",
    "\n",
    "* Version 13 Internal 20Q3*\n",
    "nothing different from  20Q2. no new cell lines added\n",
    "\n",
    "* Version 14 Internal 20Q3*\n",
    "issues with the blacklists\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines, IDs contained in the column DepMap_ID\n",
    "LeftGene and RightGene separated by an ampersand (\"&\").\n",
    "\n",
    "Unfiltered data contains all output fusions, while the filtered data uses the filters suggested by the star fusion docs. These filters are:\n",
    "- FFPM > 0.1 -  a cutoff of 0.1 means&nbsp;at least 1 fusion-supporting RNAseq fragment per 10M total reads\n",
    "- Remove known false positives, such as GTEx recurrent fusions and certain paralogs\n",
    "- Genes that are next to each other\n",
    "- Fusions with mitochondrial breakpoints\n",
    "- Removing fusion involving mitochondrial chromosomes or HLA genes\n",
    "- Removed common false positive fusions (red herring annotations as described in the STAR-Fusion docs)\n",
    "- Recurrent fusions observed in CCLE across cell lines (in 25 or more samples)\n",
    "- Removed fusions where SpliceType='INCL_NON_REF_SPLICE' and LargeAnchorSupport='NO_LDAS' and FFPM < 0.1\n",
    "- FFPM < 0.05\n",
    "\n",
    "NEW LINES:\n",
    "\"\"\"+newlines)\n",
    "\n",
    "AddToVirtual('depmap-a0ab', \"gene-fusions-8b7a\", files=[('CCLE_fusions_unfiltered', 'unfiltered_fusions_'+release),('CCLE_fusions', 'filtered_fusions_'+release)])\n",
    "\n",
    "AddToVirtual(virtual_internal, \"gene-fusions-8b7a\", files=[('CCLE_fusions_unfiltered', 'unfiltered_fusions_'+release),('CCLE_fusions', 'filtered_fusions_'+release)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM\n",
    "\n",
    "same as internal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35478\n",
      "34874\n",
      "358959\n",
      "353446\n"
     ]
    }
   ],
   "source": [
    "## removing first blacklisted, then embargoed, to create two datasets\n",
    "print(len(filtered))\n",
    "filtered = filtered[~filtered.DepMap_ID.isin(rna_ibm_embargo)]\n",
    "print(len(filtered))\n",
    "filtered.to_csv('temp/filtered_fusions_'+release, index=False)\n",
    "\n",
    "print(len(unfiltered))\n",
    "unfiltered  = unfiltered[~unfiltered.DepMap_ID.isin(rna_ibm_embargo)]\n",
    "unfiltered.to_csv('temp/unfiltered_fusions_'+release, index=False)\n",
    "print(len(unfiltered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_unfilt = tc.get(name='gene-fusions-375f', version=11, file='unfiltered_fusions_'+prevname)\n",
    "print('shoud be None')\n",
    "print(set(prev_unfilt.DepMap_ID) - set(unfiltered.DepMap_ID))\n",
    "print(\"new lines\")\n",
    "newlines = set(unfiltered.DepMap_ID) - set(prev_unfilt.DepMap_ID) \n",
    "newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading filtered_fusions_20Q3...\n",
      "hitting https://cds.team/taiga/api/datafile/59897090bfaa4ec9af221bf4c5755b9e\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: filtered_fusions_20Q3 properly converted and uploaded\n",
      "Uploading unfiltered_fusions_20Q3...\n",
      "hitting https://cds.team/taiga/api/datafile/59897090bfaa4ec9af221bf4c5755b9e\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: unfiltered_fusions_20Q3 properly converted and uploaded\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 618f6215e2154be2be6b49fb8de97cb2 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/618f6215e2154be2be6b49fb8de97cb2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'618f6215e2154be2be6b49fb8de97cb2'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.update_dataset(dataset_permaname=\"gene-fusions-375f\",\n",
    "                 upload_file_path_dict={'temp/filtered_fusions_'+release: 'TableCSV',\n",
    "                                       'temp/unfiltered_fusions_'+release: 'TableCSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# DMC Fusions\n",
    "\n",
    "* Version 1-2 DMC 19Q1*\n",
    "\n",
    "version 2 contains the correct data for 19Q1\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines\n",
    "\n",
    "Unfiltered data contains all output fusions, while the filtered data uses the filters suggested by the star fusion docs. These filters are:\n",
    "- FFPM > 0.1 -  a cutoff of 0.1 means&nbsp;at least 1 fusion-supporting RNAseq fragment per 10M total reads\n",
    "- Remove known false positives, such as GTEx recurrent fusions and certain paralogs\n",
    "- Genes that are next to each other\n",
    "- Fusions with mitochondrial breakpoints\n",
    "\n",
    "* Version 3 DMC 19Q2*\n",
    "\n",
    "* Version 4 DMC 19Q3*\n",
    "\n",
    "* Version 5 DMC 19Q4*\n",
    "\n",
    "* Version 6 DMC 20Q1*\n",
    "\n",
    "* Version 7 DMC 20Q2*\n",
    "adding 50 new lines\n",
    "\n",
    "* Version 7 DMC 20Q2*\n",
    "Unknown change\n",
    "\n",
    "* Version 9 DMC 20Q3*\n",
    "nothing different from  20Q2. no new cell lines added\n",
    "\n",
    "* Version 10 DMC 20Q3*\n",
    "updating the blacklists\n",
    "\n",
    "* Version 11 DMC 20Q3*\n",
    "issues with the blacklists resolved\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines, IDs contained in the column DepMap_ID\n",
    "LeftGene and RightGene separated by an ampersand (\"&\").\n",
    "\n",
    "Unfiltered data contains all output fusions, while the filtered data uses the filters suggested by the star fusion docs. These filters are:\n",
    "- FFPM > 0.1 -  a cutoff of 0.1 means&nbsp;at least 1 fusion-supporting RNAseq fragment per 10M total reads\n",
    "- Remove known false positives, such as GTEx recurrent fusions and certain paralogs\n",
    "- Genes that are next to each other\n",
    "- Fusions with mitochondrial breakpoints\n",
    "- Removing fusion involving mitochondrial chromosomes or HLA genes\n",
    "- Removed common false positive fusions (red herring annotations as described in the STAR-Fusion docs)\n",
    "- Recurrent fusions observed in CCLE across cell lines (in 25 or more samples)\n",
    "- Removed fusions where SpliceType='INCL_NON_REF_SPLICE' and LargeAnchorSupport='NO_LDAS' and FFPM < 0.1\n",
    "- FFPM < 0.05\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CCLE_fusions', 'gene-fusions-375f.11/filtered_fusions_20Q3'), ('CCLE_fusions_unfiltered', 'gene-fusions-375f.11/unfiltered_fusions_20Q3'), ('Achilles_raw_readcounts', 'avana-consortium-20q3-902b.1/raw_readcounts'), ('Achilles_common_essentials', 'avana-consortium-20q3-902b.1/pan_dependent_genes'), ('Achilles_raw_readcounts_failures', 'avana-consortium-20q3-902b.1/raw_readcounts_failing'), ('Achilles_gene_effect', 'avana-consortium-20q3-902b.1/gene_effect'), ('CCLE_gene_cn', 'depmap-cn-data-9b9d.15/dmc_20Q3_gene_cn'), ('Achilles_dropped_guides', 'avana-consortium-20q3-902b.1/dropped_guides'), ('Achilles_gene_effect_unscaled', 'avana-consortium-20q3-902b.1/gene_effect_unscaled'), ('CCLE_RNAseq_reads', 'depmap-rnaseq-expression-data-80ef.17/dmc_20Q3_counts'), ('Achilles_logfold_change', 'avana-consortium-20q3-902b.1/logfold_change'), ('CCLE_RNAseq_transcripts', 'depmap-rnaseq-expression-data-80ef.17/dmc_20Q3_transcripts_tpm'), ('CCLE_mutations', 'depmap-mutation-calls-dfce.17/depmap_20Q3_mutation_calls'), ('Achilles_gene_dependency', 'avana-consortium-20q3-902b.1/gene_dependency'), ('Achilles_guide_efficacy', 'avana-consortium-20q3-902b.1/guide_efficacy'), ('Achilles_replicate_map', 'avana-consortium-20q3-902b.1/replicate_map'), ('Achilles_guide_map', 'avana-consortium-20q3-902b.1/guide_gene_map'), ('sample_info', 'dmc-20q3-033d.11/sample_info'), ('CCLE_segment_cn', 'depmap-cn-data-9b9d.15/dmc_20Q3_segs_cn'), ('Achilles_logfold_change_failures', 'avana-consortium-20q3-902b.1/logfold_change_failures'), ('common_essentials', 'avana-consortium-20q3-902b.1/essential_genes'), ('CCLE_expression_full', 'depmap-rnaseq-expression-data-80ef.17/dmc_20Q3_tpm'), ('Achilles_high_variance_genes', 'avana-consortium-20q3-902b.1/high_variance_genes'), ('CCLE_expression', 'depmap-rnaseq-expression-data-80ef.17/dmc_20Q3_proteincoding_tpm'), ('nonessentials', 'avana-consortium-20q3-902b.1/nonessential_genes')]\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datafile/4ef108138f8349669d402f645a5b2210\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 87c7897efa554e569450062b9a3e0d63 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/87c7897efa554e569450062b9a3e0d63\n"
     ]
    }
   ],
   "source": [
    "AddToVirtual(virtual_dmc, \"gene-fusions-375f\", files=[('CCLE_fusions', 'filtered_fusions_'+release),('CCLE_fusions_unfiltered', 'unfiltered_fusions_'+release)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACH-001577', 'ACH-001686', 'ACH-002055', 'ACH-002463'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(unfiltered.DepMap_ID) - set(counts_genes.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34874\n",
      "34115\n",
      "353446\n",
      "347954\n"
     ]
    }
   ],
   "source": [
    "## removing first blacklisted, then embargoed, to create two datasets\n",
    "print(len(filtered))\n",
    "filtered = filtered[filtered.DepMap_ID.isin(prevprevcells)]\n",
    "print(len(filtered))\n",
    "filtered[~filtered.DepMap_ID.isin(rna_dmc_embargo)].to_csv('temp/filtered_fusions_'+release, index=False)\n",
    "\n",
    "print(len(unfiltered))\n",
    "unfiltered = unfiltered[unfiltered.DepMap_ID.isin(prevprevcells)]\n",
    "print(len(unfiltered))\n",
    "unfiltered[~unfiltered.DepMap_ID.isin(rna_dmc_embargo)].to_csv('temp/unfiltered_fusions_'+release, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1374"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(unfiltered.DepMap_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_unfilt = tc.get(name='gene-fusions-6212', version=13, file='unfiltered_fusions_'+prevname)\n",
    "print('shoud be None')\n",
    "print(set(prev_unfilt.DepMap_ID) - set(unfiltered.DepMap_ID))\n",
    "print(\"new lines\")\n",
    "newlines = set(unfiltered.DepMap_ID) - set(prev_unfilt.DepMap_ID) \n",
    "newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading filtered_fusions_20Q3...\n",
      "hitting https://cds.team/taiga/api/datafile/e145c47fa89b4eb9b588dfc6b544ef73\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: filtered_fusions_20Q3 properly converted and uploaded\n",
      "Uploading unfiltered_fusions_20Q3...\n",
      "hitting https://cds.team/taiga/api/datafile/e145c47fa89b4eb9b588dfc6b544ef73\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: unfiltered_fusions_20Q3 properly converted and uploaded\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id eb232e2f0e974bbd8f815a560dadb8cc created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/eb232e2f0e974bbd8f815a560dadb8cc\n",
      "[('CCLE_fusions', 'gene-fusions-6212.13/filtered_fusions_20Q3'), ('CCLE_fusions_unfiltered', 'gene-fusions-6212.13/unfiltered_fusions_20Q3'), ('sample_info', 'public-20q3-3d35.10/sample_info'), ('Achilles_raw_readcounts', 'avana-public-tentative-20q3-3e73.4/raw_readcounts'), ('Achilles_replicate_map', 'avana-public-tentative-20q3-3e73.4/replicate_map'), ('Achilles_dropped_guides', 'avana-public-tentative-20q3-3e73.4/dropped_guides'), ('Achilles_gene_effect', 'avana-public-tentative-20q3-3e73.4/gene_effect'), ('Achilles_gene_effect_unscaled', 'avana-public-tentative-20q3-3e73.4/gene_effect_unscaled'), ('CCLE_RNAseq_transcripts', 'depmap-rnaseq-expression-data-ccd0.23/public_20Q3_transcripts_tpm'), ('common_essentials', 'avana-public-tentative-20q3-3e73.4/essential_genes'), ('Achilles_common_essentials', 'avana-public-tentative-20q3-3e73.4/pan_dependent_genes'), ('Achilles_raw_readcounts_failures', 'avana-public-tentative-20q3-3e73.4/raw_readcounts_failing'), ('Achilles_logfold_change', 'avana-public-tentative-20q3-3e73.4/logfold_change'), ('CCLE_RNAseq_reads', 'depmap-rnaseq-expression-data-ccd0.23/public_20Q3_counts'), ('CCLE_expression_full', 'depmap-rnaseq-expression-data-ccd0.23/public_20Q3_tpm'), ('CCLE_mutations', 'depmap-mutation-calls-9a1a.20/depmap_20Q3_mutation_calls'), ('Achilles_logfold_change_failures', 'avana-public-tentative-20q3-3e73.4/logfold_change_failures'), ('CCLE_gene_cn', 'depmap-wes-cn-data-97cc.33/public_20Q3_gene_cn'), ('Achilles_guide_efficacy', 'avana-public-tentative-20q3-3e73.4/guide_efficacy'), ('CCLE_segment_cn', 'depmap-wes-cn-data-97cc.33/public_20Q3_segs_cn'), ('nonessentials', 'avana-public-tentative-20q3-3e73.4/nonessential_genes'), ('Achilles_guide_map', 'avana-public-tentative-20q3-3e73.4/guide_gene_map'), ('Achilles_high_variance_genes', 'avana-public-tentative-20q3-3e73.4/high_variance_genes'), ('Achilles_gene_dependency', 'avana-public-tentative-20q3-3e73.4/gene_dependency'), ('CCLE_expression', 'depmap-rnaseq-expression-data-ccd0.23/public_20Q3_proteincoding_tpm')]\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datafile/069b12f8d37f4017b70be1cd565935de\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id dab80bbe67f14784ab4f3073514eadc6 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/dab80bbe67f14784ab4f3073514eadc6\n"
     ]
    }
   ],
   "source": [
    "tc.update_dataset(dataset_permaname=\"gene-fusions-6212\",\n",
    "                 upload_file_path_dict={'temp/filtered_fusions_'+release: 'TableCSV',\n",
    "                                       'temp/unfiltered_fusions_'+release: 'TableCSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# PUBLIC Fusions\n",
    "\n",
    "* Version 1 Public 2017 data*\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines, ID contained in the column Broad_ID\n",
    "\n",
    "Original Raw Data: Generated by Mahmoud Ghandi on April 25, 2017. Can be found at xchip_ccle_dist/broad_only/unpublished_Novartis_data/RNAseq/fusions.txt\n",
    "\n",
    "* Version 2-3 Public 19Q1*\n",
    "\n",
    "version 3 contains the correct data for 19Q1\n",
    "\n",
    "* Version 4-6 Public 19Q2*\n",
    "\n",
    "in version 5 formatting of the columns is improved\n",
    "\n",
    "* Version 7 Public 19Q3*\n",
    "\n",
    "* Version 8 Public 19Q4*\n",
    "\n",
    "* Version 9 Public 20Q1*\n",
    "\n",
    "* Version 10 Public 20Q2*\n",
    "adding 50 new lines\n",
    "\n",
    "* Version 11 Public 20Q3*\n",
    "nothing different from  20Q2. no new cell lines added\n",
    "\n",
    "* Version 12 Public 20Q3*\n",
    "updated blacklists\n",
    "\n",
    "* Version 13 Public 20Q3*\n",
    "issues with the blacklists\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines, IDs contained in the column DepMap_ID\n",
    "LeftGene and RightGene separated by an ampersand (\"&\").\n",
    "\n",
    "Unfiltered data contains all output fusions, while the filtered data uses the filters suggested by the star fusion docs. These filters are:\n",
    "- FFPM > 0.1 -  a cutoff of 0.1 means&nbsp;at least 1 fusion-supporting RNAseq fragment per 10M total reads\n",
    "- Remove known false positives, such as GTEx recurrent fusions and certain paralogs\n",
    "- Genes that are next to each other\n",
    "- Fusions with mitochondrial breakpoints\n",
    "- Removing fusion involving mitochondrial chromosomes or HLA genes\n",
    "- Removed common false positive fusions (red herring annotations as described in the STAR-Fusion docs)\n",
    "- Recurrent fusions observed in CCLE across cell lines (in 25 or more samples)\n",
    "- Removed fusions where SpliceType='INCL_NON_REF_SPLICE' and LargeAnchorSupport='NO_LDAS' and FFPM < 0.1\n",
    "- FFPM < 0.05\n",
    "\n",
    "\"\"\")\n",
    "AddToVirtual(virtual_public, \"gene-fusions-6212\", files=[('CCLE_fusions', 'filtered_fusions_'+release),('CCLE_fusions_unfiltered', 'unfiltered_fusions_'+release)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [OPTIONAL] IF want to merge here instead of on Terra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevsamplesets = ['CCLE_19Q3interim',samplesetname]\n",
    "samples = []\n",
    "for i in prevsamplesets:\n",
    "    samples.extend(refwm.get_sample_sets().loc[i].samples)\n",
    "res = []\n",
    "terrasamp = refwm.get_samples()\n",
    "for i, sample in enumerate(samples):\n",
    "    res.append(terrasamp.loc[sample])\n",
    "    genes_fusion = res[i]['fusion_predictions_abridged']\n",
    "    rsem_genes_transcripts = res[i]['rsem_isoforms']\n",
    "    rsem_genes_expected_count = res[i]['rsem_genes']\n",
    "    ! gsutil cp $rsem_genes_expected_count 'temp/' && gsutil cp $rsem_genes_transcripts 'temp/' && gsutil cp $genes_fusion 'temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainres = refwm.get_sample_sets().loc['DM19Q2_PATHS_CORRECTED_V2']\n",
    "maingenes_fusion = mainres['fusions_star']\n",
    "mainrsem_genes_tpm = mainres['rsem_genes_tpm']\n",
    "mainrsem_genes_transcripts = mainres['rsem_transcripts_tpm']\n",
    "mainrsem_genes_expected_count = mainres['rsem_genes_expected_count']\n",
    "! gsutil cp $mainrsem_genes_expected_count \"temp/\" && gsutil cp $mainrsem_genes_transcripts \"temp/\" && gsutil cp $maingenes_fusion \"temp/expression.fusion.tsv\" && gsutil cp $mainrsem_genes_tpm \"temp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainres['rsem_genes_expected_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addSamplesRSEMToMain(res,mainres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_fusion = ['temp/'+val['fusion_predictions_abridged'].split('/')[-1] for val in res]\n",
    "addToMainFusion(genes_fusion,'temp/expression.fusion.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
