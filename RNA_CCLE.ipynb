{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you need to have installed JKBio in the same folder as ccle_processing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n",
       "  var css_urls = [];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n  var css_urls = [];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import dalmatian as dm\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from src.CCLE_postp_function import *\n",
    "\n",
    "from JKBio import Datanalytics as da \n",
    "from JKBio import TerraFunction as terra\n",
    "from JKBio.Helper import * \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext rpy2.ipython\n",
    "from IPython.display import Image,display\n",
    "from taigapy import TaigaClient\n",
    "tc = TaigaClient()\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from bokeh.plotting import *\n",
    "from bokeh.models import HoverTool\n",
    "output_notebook()\n",
    "from collections import OrderedDict\n",
    "from gsheets import Sheets\n",
    "sheets = Sheets.from_files('~/.client_secret.json', '~/.storage.json')\n",
    "replace = {'T': 'Tumor', 'N': 'Normal', 'm': 'Unknown', 'L': 'Unknown'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesetname=\"20Q2\"\n",
    "prevname=\"20Q1\"\n",
    "prevprevname = \"19Q4\"\n",
    "prevprevversion='21'\n",
    "virtual_public='public-20q2-075d'\n",
    "virtual_dmc='dmc-20q2-2db6'\n",
    "virtual_internal='internal-20q2-7f46'\n",
    "\n",
    "workspace2=\"broad-firecloud-ccle/CCLE_DepMap_RNAseq\"\n",
    "workspace4=\"broad-genomics-delivery/Cancer_Cell_Line_Factory_CCLF_RNAseq\"\n",
    "workspace5=\"nci-mimoun-bi-org/CCLF_RNA_2_0\"\n",
    "\n",
    "workspace3=\"broad-genomics-delivery/CCLE_DepMap_RNAseq\"\n",
    "workspace1=\"broad-genomics-delivery/Getz_IBM_CellLines_RNASeqData\"\n",
    "\n",
    "workspace6=\"terra-broad-cancer-prod/CCLE_DepMap_RNAseq\"\n",
    "workspace7=\"terra-broad-cancer-prod/Getz_IBM_CellLines_RNASeqData\"\n",
    "\n",
    "refworkspace=\"broad-firecloud-ccle/DepMap_hg38_RNAseq\"\n",
    "source1=\"ibm\"\n",
    "source2=\"ccle\"\n",
    "source3=\"ccle\"\n",
    "source4=\"cclf\"\n",
    "source5=\"cclf\"\n",
    "source6=\"ccle\"\n",
    "source7=\"ibm\"\n",
    "release = samplesetname\n",
    "\n",
    "refsheet_url = \"https://docs.google.com/spreadsheets/d/1XkZypRuOEXzNLxVk9EOHeWRE98Z8_DBvL4PovyM01FE\"\n",
    "privacy_release_url = \"https://docs.google.com/spreadsheets/d/115TUgA1t_mD32SnWAGpW9OKmJ2W5WYAOs3SuSdedpX4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "genome_version <- 'hg38'\n",
    "release <- '20Q2'\n",
    "hg38_cyto_band_reference <- '../JKBio/data/hg38_cytoband.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate sample set from new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_to_change = {'from_arxspan_id': 'participant',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm1 = dm.WorkspaceManager(workspace1)\n",
    "#wm2 = dm.WorkspaceManager(workspace2)\n",
    "wm3 = dm.WorkspaceManager(workspace3)\n",
    "#wm4 = dm.WorkspaceManager(workspace4)\n",
    "#wm5 = dm.WorkspaceManager(workspace5)\n",
    "wm6 = dm.WorkspaceManager(workspace6)\n",
    "wm7 = dm.WorkspaceManager(workspace7)\n",
    "refwm = dm.WorkspaceManager(refworkspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccle_refsamples = sheets.get(refsheet_url).sheets[0].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be missing \"primary disease\",\"sm_id\", \"cellosaurus_id\", \"gender, \"age\", \"primary_site\", \"primary_disease\", \"subtype\", \"subsubtype\", \"origin\", \"comments\"\n",
    "#when SMid: match==\n",
    "samples, pairs, noarxspan = GetNewCellLinesFromWorkspaces(refworkspace, stype='rna', refurl=refsheet_url, wmfroms = [workspace1, workspace3, workspace6, workspace7], sources=[source1, source3,source6, source7], match=['ACH-','CDS-'], participantslicepos=10, accept_unknowntypes=True, extract=extract_to_change, recomputedate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am trying to remove duplicates from samples without arxspan ids to then look more into them and see if I have to get data for them or if I should just throw them out\n",
    "toremov=set()\n",
    "for k, val in noarxspan.iterrows():\n",
    "    withsamesize = noarxspan[noarxspan[\"sample_id\"] == val[\"sample_id\"]]\n",
    "    if len(withsamesize) > 1:\n",
    "        for l, v in withsamesize.iloc[1:].iterrows():\n",
    "            toremov.add(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in toremov:\n",
    "    noarxspan = noarxspan.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noarxspan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noarxspan.sample_id = [i.split('NA_O')[0] for i in noarxspan.sample_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noarxspan['ccle_name'] = [''.join(i.split('_')[1:-1]).split('v')[0] for i in noarxspan.sample_id]\n",
    "noarxspan['readgroup'] = [i.split('_')[0] for i in noarxspan.sample_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noarxspan['ccle_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in noarxspan.iterrows():\n",
    "    if not gcp.exists(v['cram_or_bam_path']):\n",
    "        print(v.ccle_name)\n",
    "        noarxspan = noarxspan.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toupdate = {\"gender\":[],\n",
    "\"primary_disease\":[],\n",
    "\"sm_id\":[],\n",
    "\"cellosaurus_id\":[],\n",
    "\"age\":[],\n",
    "\"primary_site\":[],\n",
    "\"subtype\":[],\n",
    "\"subsubtype\":[],\n",
    "\"origin\":[],\n",
    "\"comments\":[],\n",
    "\"patient_id\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refwm.get_bucket_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If I have a previous samples I can update unknown data directly\n",
    "index=[]\n",
    "notfound=[]\n",
    "for k, val in samples.iterrows():\n",
    "    dat = ccle_refsamples[ccle_refsamples['arxspan_id']==val['arxspan_id']]\n",
    "    if len(dat)>0:\n",
    "        index.append(k)\n",
    "        for k, v in toupdate.items():\n",
    "            toupdate[k].append(dat[k].tolist()[0])\n",
    "    else:\n",
    "        notfound.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(toupdate['patient_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing so..\n",
    "for k, v in toupdate.items():\n",
    "    samples.loc[index,k] =v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples.loc[notfound].patient_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for these samples I will need to check and manually add the data in the list \n",
    "samples.loc[notfound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found same patient\n",
    "a = [\"ACH-000635\",\"ACH-000717\", \"ACH-000864\", \"ACH-001042\", \"ACH-001547\"]\n",
    "b = [\"ACH-002291\",\"ACH-001672\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccle_refsamples[ccle_refsamples.arxspan_id.isin(a)].patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we found we had cell lines from different patients\n",
    "ccle_refsamples.loc[ccle_refsamples[ccle_refsamples.arxspan_id.isin(a)].index,\"patient_id\"]=\"PT-B15oIzKN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate ach-id\n",
    "dup = {\"ACH-001620\": \"ACH-001605\",\n",
    "\"ACH-001621\": \"ACH-001606\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccle_refsamples[ccle_refsamples.arxspan_id==\"ACH-001605\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.loc[\"CDS-R2h6fu\",\"arxspan_id\"]=\"ACH-001620\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = changeCellLineNameInNewSet(new = samples, ref=ccle_refsamples, datatype=\"rna\", dupdict=dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notfound.remove(\"CDS-R2h6fu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename ccle_name TODO: ask becky what to do\n",
    "rename = {\"PEDS117\": \"CCLFPEDS0009T\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(notfound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the addtional data and writing it here in the right order 'as shown above'\n",
    "- use the stripped_cell_line_name to find the samples on https://docs.google.com/spreadsheets/d/1uqCOos-T9EMQU7y2ZUw4Nm84opU5fIT1y7jet1vnScE/edit#gid=356471436. \n",
    "- Make sure that we don't have duplicate cell lines in there. Otherwise, use the duplicate renaming function\n",
    "- copy Primary Site, Primary Disease, Subtype, Comments, Disease Sub-subtype, if they exist. (sometimes subtype and subsubtype are the same.. don't use subsubtype then.\n",
    "- look for the cell line in cellosaurus, you might need to use one of the aliases given in master depmap pv..\n",
    "- copy  cellosaurus_id gender age info or write 'U' if they don't exist. 'can be a number or {Embryonic, Children, Adult, Fetus, U} \n",
    "- check that it does not say this cell line is not a duplicate from another cell line\n",
    "- check that if it says this cell line is derived/children/father/samepatient from other cell lines, and that if we have any of the other cell lines, that the patient id is changed to be the same one for all (be sure that you are updating everywhere these patient ids are used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sample info file has different age annotations\n",
    "toupdate = {\"sm_id\":[\"\"]*30,\n",
    "            \n",
    "\"cellosaurus_id\":['CVCL_H248',\"\",\"CVCL_S505\",\"CVCL_2277\",\"CVCL_1966\",\"CVCL_M147\",\"CVCL_2024\",\"CVCL_A344\",\"CVCL_8354\",\"CVCL_S506\",\"CVCL_M258\",\"CVCL_2581\",\"CVCL_2604\",\"CVCL_4449\",\"CVCL_2610\",\"CVCL_M266\",\"CVCL_4D12\",\"CVCL_3118\",\"\",\"CVCL_T041\",\"CVCL_1852\",\"CVCL_2777\",\"CVCL_B371\", \"CVCL_3220\",\"CVCL_1854\",\"CVCL_2223\",\"CVCL_2226\",\"CVCL_1903\",\"CVCL_3423\",\"CVCL_1898\"],\n",
    "\"gender\":['Female',\"Male\",\"Male\",\"Male\",\"Male\",\"Male\",\"Female\",\"Male\",\"Female\",\"Male\",\"Male\",\"Male\",\"Male\",\"Female\",\"Male\",\"U\",\"Female\",\"Male\",\"U\",'Female',\"Male\",\"Male\",\"Male\",\"Female\",\"Male\",\"Female\",\"Female\",\"Male\",\"Female\",\"Male\"],\n",
    "            \n",
    "\"age\":[2,'U',\"Embryo\",\"Embryo\",7,12,49,\"Embryo\",52,3,56,57,\"U\",\"U\",\"U\",18,49,71,\"U\",55,11,22,10,22,57,61,69,37,\"U\",42],\n",
    "            \n",
    "\"primary_site\":[\"haematopoietic_and_lymphoid_tissue\",\"skin\",\"Embryonal\",\"Embryonal\",\"haematopoietic_and_lymphoid_tissue\",\"central_nervous_system\",\"pleural_effusion\",\"Embryonal\",\"ascites\",\"central_nervous_system\",\"biliary_tract\",\"pleural_effusion\",\"lymph_node\",\"lymph_node\",\"lymph_node\",\"biliary_tract\",\"uvea\",\"biliary_tract\",\"bone\",\"pleural_effusion\",\"haematopoietic_and_lymphoid_tissue\",\"lung\",\"haematopoietic_and_lymphoid_tissue\",\"Thymus\",\"ascites\",\"upper_aerodigestive_tract\",\"upper_aerodigestive_tract\",\"haematopoietic_and_lymphoid_tissue\",\"breast\",\"haematopoietic_and_lymphoid_tissue\"],\n",
    "            \n",
    "\"primary_disease\":['Leukemia',\"Skin Cancer\",\"Embryonal Cancer\",\"Embryonal Cancer\",\"Lymphoma\",\"Brain Cancer\",\"Lymphoma\",\"Teratoma\",\"Ovarian Cancer\",\"Brain Cancer\",\"Bile Duct Cancer\",\"Lung Cancer\",\"Skin Cancer\",\"Skin Cancer\",\"Skin Cancer\",\"Bile Duct Cancer\",\"Eye Cancer\",\"Bile Duct Cancer\",\"Bone Cancer\",\"Breast Cancer\",\"Leukemia\",\"Embryonal Cancer\",\"Lymphoma\",\"Thymic Cancer\",\"Lymphoma\",\"Head and Neck Cancer\",\"Head and Neck Cancer\",\"Lymphoma\",\"Breast Cancer\",\"Lymphoma\"],\n",
    "            \n",
    "\"subtype\":['AML',\"Melanoma\",\"Carcinoma\",\"Carcinoma\",\"B-cell, Non-Hodgkins, Burkitts\",\"Glioblastoma\",\"B-cell, Non-Hodgkins, Burkitts\",\"Testicular\",\"Adenocarcinoma, clear cell\",\"Medulloblastoma\",\"Cholangiocarcinoma\",\"Mesothelioma\",\"Melanoma\",\"Melanoma\",\"Melanoma\",\"Engineered\",\"Uveal Melanoma\",\"Cholangiocarcinoma\",\"Ewings Sarcoma\",\"Breast Ductal Carcinoma\",\"Acute Lymphoblastic Leukemia (ALL), B-cell\",\"Carcinoma\",\"B-cell, Non-Hodgkins, Burkitts\",\"Carcinoma\",\"B-cell\",\"Squamous Cell Carcinoma\",\"Squamous Cell Carcinoma\",\"B-cell\",\"Carcinoma\",\"B-cell\"],\n",
    "            \n",
    "\"subsubtype\":[\"\",'acral',\"\",\"\",\"b_cell_burkitt\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"engineered_bile_duct\",\"\",\"intrahepatic\",\"\",\"\",\"b_cells\",\"\",\"b_cell_burkitt\",\"anaplastic\",\"\",\"\",\"\",\"\",\"ERneg_HER2pos\",\"\"],\n",
    "            \n",
    "\"origin\":['']*30,\n",
    "            \n",
    "\"comments\":[\"Gifted to Fred Hutchinson Cancer Center by Dr. Jeff Taub at Wayne State University; Received from Academic lab (FHCRC and Taub, Wayne State University)\",\"M1610113 (p9); Mutation: NRASG12R; biopsy site - right dorsal heel; Received from Academic lab (Hayward, QIMRB)\",\"cytoreductive thoracotomy specimen of a man with primary retroperitoneal embryonal carcinoma\",\"retro-peritoneal metastasis of a non seminomatous germ cell carcinoma\",\"\",\"Received from Academic lab (Hong, Broad)\",\"\",\"malignant testicular teratoma (intermediate grade)\",\"ovarian clear cell adenocarcinoma\",\"\",\"Human poorly differentiated cholangiocarcinoma cell line established from biliary tract.\",\"\",\"taken from metastatic site: lymph node\",\"taken from metastatic site: lymph node\",\"taken from metastatic site: lymph node\",\"Received from Academic lab (Bardeesy, MGH)/DSMZ\",\"display GNAQ or GNA11 activating mutations\",\"Received from Academic lab (Bardeesy, MGH)/DSMZ\",\"(PCR) EWS/FLI\",\"breast cancer, Infiltrating ductal carcinoma\",\"ST & GR-ST are subclones of Tanoue (JCRB). GR-ST are ST cells transformed with human G-CSF receptor cDNA. Responds to G-CSF.\",\"malignant embryonal carcinoma derived from lung\",\"Human lymphoid cell line derived from Burkitt's lymphoma. Tumorigenic in nude mouse (meta from ileocecal lymph node)\",\"Thymic carcinoma cell line.\",\"\",\"\",\"\",\"\",\"HER2Amp; basal\",\"\"],\n",
    "            \n",
    "\"patient_id\":[\"PT-IMn7eqLZ\",\"PT-pUBtKkYR\",\"PT-El9YBuz5\",\"PT-P6Iueia9\", \"PT-2gssvols\", \"PT-NczSWWPv\", \"PT-RY8p3mdI\", \"PT-olH0fsQe\", \"PT-3CksfnKs\", \"PT-OrIcgwcE\", \"PT-W3uUJEi1\", \"PT-B15oIzKN\", \"PT-fo7MYTEA\", \"PT-ley1jk9c\", \"PT-3f6vZIv9\", \"PT-MdR8meiJ\", \"PT-z5KcZzaT\", \"PT-3wNFFmJl\", \"PT-gpiijlSj\", \"PT-bdAvWDQz\", \"PT-uDGwdjJW\", \"PT-L63btI6L\", \"PT-uXJgRBuh\", \"PT-GkdLfVhc\", \"PT-UGLaOKH7\", \"PT-hw8KmxDu\", \"PT-G3kJesC0\", \"PT-zFOjyoHz\", \"PT-A8FG8EcM\", \"PT-98h5OTqP\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = pd.DataFrame(toupdate)\n",
    "a['name'] = samples.loc[notfound,\"stripped_cell_line_name\"].tolist()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating..\n",
    "for k, v in toupdate.items():\n",
    "    samples.loc[notfound,k] =v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading to our bucket (now a new function)\n",
    "a = changeToBucket(samples,'gs://cclebams/rna/', values=['internal_bam_filepath','internal_bai_filepath'], catchdup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and updating the spreadsheet with these\n",
    "print(\"YOU NOW NEED TO UPDATE THE GOOGLE SHEET!\")\n",
    "pd.concat([ccle_refsamples,samples],sort=False).to_csv('temp/updated_ccle_samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['arxspan_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.rename(columns={'patient_id':'participant_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairs.participant_id = samples.participant_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pairs = pairs.rename(columns={'patient_id':'participant_id'}).set_index('pair_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploading new samples\n",
    "refwm = refwm.disable_hound()\n",
    "#refwm.upload_samples(samples)\n",
    "#refwm.upload_entities('pairs', pairs)\n",
    "#creating a sample set\n",
    "refwm.update_sample_set(sample_set_id=samplesetname, sample_ids=samples.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that we have all the cell lines we expect for this release\n",
    "This involves comparing to the list in the Google sheet \"Cell Line Profiling Status.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function may not work - it hasn't been tested\n",
    "url = 'https://docs.google.com/spreadsheets/d/1qus-9TKzqzwUMNWp8S1QP4s4-3SsMo2vuQRZrNXf7ag'\n",
    "\n",
    "compareToCuratedGS(url, sample = samples, samplesetname = samplesetname, colname = 'RNA New to internal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the pipeline\n",
    "\n",
    "We are using Dalmatian to send request to Terra, we are running a set of 6 functions To generate the expression/fusion dataset:\n",
    "\n",
    "We use the GTEx pipeline ([https://github.com/broadinstitute/gtex-pipeline/blob/v9/TOPMed_RNAseq_pipeline.md](https://github.com/broadinstitute/gtex-pipeline/blob/v9/TOPMed_RNAseq_pipeline.md)).\n",
    "\n",
    "To generate the expression dataset, run the following tasks on all samples that you need, in this order:\n",
    "\n",
    "\n",
    "\n",
    "*   samtofastq_v1-0_BETA_cfg \n",
    "\n",
    "    (broadinstitute_gtex/samtofastq_v1-0_BETA Snapshot ID: 5)\n",
    "\n",
    "*   star_v1-0_BETA_cfg\n",
    "\n",
    "(broadinstitute_gtex/star_v1-0_BETA Snapshot ID: 7)\n",
    "\n",
    "\n",
    "\n",
    "*   rsem_v1-0_BETA_cfg \n",
    "\n",
    "    (broadinstitute_gtex/rsem_v1-0_BETA Snapshot ID: 4)\n",
    "\n",
    "*   rsem_aggregate_results_v1-0_BETA_cfg (broadinstitute_gtex/rsem_aggregate_results_v1-0_BETA Snapshot ID: 3)\n",
    "\n",
    "The outputs to be downloaded will be saved under the sample set that you ran. The outputs we use for the release are:\n",
    "\n",
    "\n",
    "\n",
    "*   rsem_genes_expected_count\n",
    "*   rsem_genes_tpm\n",
    "*   rsem_transcripts_tpm\n",
    "\n",
    "****Make sure that you delete the intermediate files. These files are quite large so cost a lot to store. To delete, you can either write a task that deletes them or use gsutil rm*****\n",
    "\n",
    "\n",
    "##### Fusions {#fusions}\n",
    "\n",
    "We use STAR-Fusion [https://github.com/STAR-Fusion/STAR-Fusion/wiki](https://github.com/STAR-Fusion/STAR-Fusion/wiki). The fusions are generated by running the following tasks\n",
    "\n",
    "\n",
    "\n",
    "*   hg38_STAR_fusion (gkugener/STAR_fusion Snapshot ID: 14)\n",
    "*   Aggregate_Fusion_Calls (gkugener/Aggregate_files_set Snapshot ID: 2)\n",
    "\n",
    "The outputs to be downloaded will be saved under the sample set you ran. The outputs we use for the release are: \n",
    "\n",
    "\n",
    "\n",
    "*   fusions_star\n",
    "\n",
    "This task uses the same samtofastq_v1-0_BETA_cfg task as in the expression pipeline, although in the current implementation, this task will be run twice. It might be worth combing the expression/fusion calling into a single workflow. This task also contains a flag that lets you specify if you want to delete the intermediates (fastqs). \n",
    "\n",
    "There are several other tasks in this workspace. In brief:\n",
    "\n",
    "\n",
    "\n",
    "*   Tasks prefixed with **EXPENSIVE** or **CHEAP** are identical to their non-prefixed version, except that they specify different memory, disk space, etc. parameters. These versions can be used when samples fail the normal version of the task due to memory errors.\n",
    "*   The following tasks are part of the GTEx pipeline but we do not use them (we use RSEM exclusively): markduplicates_v1-0_BETA_cfg (broadinstitute_gtex/markduplicates_v1-0_BETA Snapshot ID: 2), rnaseqc2_v1-0_BETA_cfg (broadinstitute_gtex/rnaseqc2_v1-0_BETA Snapshot ID: 2)\n",
    "*   **ExonUsage_hg38_fixed** (gkugener/ExonUsage_fixed Snapshot ID: 1): this task calculates exon usage ratios. The non-fixed version contains a bug in the script that is not able to handle chromosome values prefixed with ‘chr’. The ‘fixed’ version resolves this issue.\n",
    "*   **AggregateExonUsageRObj_hg38** (ccle_mg/AggregateExonUsageRObj Snapshot ID: 2): combines the exon usage ratios into a matrices that are saved in an R object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Terra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id = refwm.create_submission(\"samtofastq_v1-0_BETA_cfg\", samplesetname,'sample_set',expression='this.samples')\n",
    "terra.waitForSubmission(refworkspace, submission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id = refwm.create_submission(\"star_v1-0_BETA_cfg\", samplesetname,'sample_set',expression='this.samples')\n",
    "terra.waitForSubmission(refworkspace, submission_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id1 = refwm.create_submission(\"rsem_v1-0_BETA_cfg\", samplesetname,'sample_set',expression='this.samples')\n",
    "terra.waitForSubmission(refworkspace, [submission_id1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update a samples set with another sampleset\n",
    "terra.updateAllSampleSet(refworkspace, samplesetname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id1 = refwm.create_submission(\"rsem_aggregate_results\", 'All_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id3 = refwm.create_submission(\"hg38_STAR_fusion\", samplesetname,'sample_set',expression='this.samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id_snp = refwm.create_submission(\"rnaseq-germline-snps-indels\", samplesetname,'sample_set',expression='this.samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra.waitForSubmission(refworkspace, [submission_id1])#submission_id_snp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id_snp = refwm.create_submission(\"merge_vcfs\", 'All_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the workflow configurations used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra.saveConfigs(refworkspace,'data/'+samplesetname+'/RNAconfig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load QC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getRNAQC(workspace=refworkspace ,only=[], qcname=\"star_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ref = pd.read_csv('temp/newrefWES.csv',index_col=\"cds_sample_id\")\n",
    "for k,v in data.items():\n",
    "    if k =='nan':\n",
    "        continue\n",
    "    new_ref.loc[k,'bam_qc']=v\n",
    "new_ref.to_csv('temp/newrefAll.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove some datafile to save money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colstoremove = ['fastq1', 'fastq2', 'star_chimeric_bam_file','star_chimeric_bam_index', 'star_transcriptome_bam','recalibrated_bam','recalibrated_bam_index']\n",
    "for val in colstoremove:\n",
    "    refwm.disable_hound().delete_entity_attributes('sample', res[val], delete_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.iloc[1]['star_bam_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## copy star bam file to our cclebams/rnasq_hg38/ bucket\n",
    "renamed, _ = terra.changeGSlocation(workspacefrom=refworkspace, newgs=\"gs://cclebams/rnasq_hg38/\", onlycol=[\"star_bam_file\",'star_bam_index'], entity=\"sample\", keeppath=False,dry_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expression post processing\n",
    "\n",
    "Here we get all data and remove the duplicates directly with the function `removeDuplicates`\n",
    "\n",
    "we then run:\n",
    "\n",
    "- readTranscripts\n",
    "- readCounts\n",
    "- readTPM\n",
    "- renameFunction\n",
    "\n",
    "- Allie's gene renaming / filtering and log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = refwm.get_sample_sets().loc['All_samples']\n",
    "rsem_genes_expected_count = res['rsem_genes_expected_count']\n",
    "rsem_genes_tpm = res['rsem_genes_tpm']\n",
    "rsem_transcripts_tpm = res['rsem_transcripts_tpm']\n",
    "! gsutil cp $rsem_genes_expected_count \"temp/expression.expectedcount\" & gsutil cp $rsem_genes_tpm \"temp/expression.genes.tpm\" & gsutil cp $rsem_transcripts_tpm \"temp/expression.transcript.tpm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in [\"temp/expression.expectedcount\"]:#,\"temp/expression.transcript.tpm\",\"temp/expression.genes.tpm\"]:\n",
    "    file = pd.read_csv(val, compression='gzip', header=0, sep='\\t', quotechar='\"', error_bad_lines=False)\n",
    "    print(file.columns[:10])\n",
    "    renaming,_ = removeOlderVersions(names=file.columns[2:], refsamples=refwm.get_samples(), arxspan_id=\"arxspan_id\", version=\"version\")\n",
    "    file[file.columns[:2].tolist()+[i for i in file.columns if i in renaming.keys()]].rename(columns=renaming).to_csv(val, sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saving samples used for 20Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ref.loc[renaming.keys(),'20q2']=1\n",
    "new_ref.to_csv('temp/newrefAll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "library('taigr')\n",
    "source('../gkugener/RScripts/load_libraries_and_annotations.R')\n",
    "source('src/CCLE_postp_function.R')\n",
    "library('cdsomics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# These files are downloaded from FireCloud/Terra\n",
    "download_paths <- list(\n",
    "  tpm_genes='temp/expression.genes.tpm',\n",
    "  tpm_transcripts='temp/expression.transcript.tpm',\n",
    "  counts_genes='temp/expression.expectedcount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "tpm_transcripts = readTranscripts(download_paths$tpm_transcripts)\n",
    "counts_genes = readCounts(download_paths$counts_genes)\n",
    "tpm_genes = readTPM(download_paths$tpm_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "head(tpm_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Fix the colnames (for cases where there are mixed values (CCLE_name and DepMap_IDs))\n",
    "colnames(counts_genes) %<>% renameFunction(.)\n",
    "colnames(tpm_genes) %<>% renameFunction(.)\n",
    "colnames(tpm_transcripts) %<>% renameFunction(.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data exploration and QC\n",
    "\n",
    "- we QC on the amount of genes with 0 counts for each samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R \n",
    "# Quick QC\n",
    "# We are looking for samples with a worrying amount of zeros\n",
    "zero_threshold <- 39000\n",
    "number_zeros <- apply(tpm_genes[,2:ncol(tpm_genes)-1] ==0, 2, FUN = sum)\n",
    "nzdf <- data.frame(CL=names(number_zeros), nz=number_zeros, stringsAsFactors = F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "number_zeros <- number_zeros[order(-number_zeros)]\n",
    "number_zeros <- number_zeros[number_zeros < zero_threshold]\n",
    "pass <- number_zeros %>% names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "counts_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "colnames(tpm_transcripts)[which(colnames(tpm_transcripts)=='transcript')] <- 'transcript_id(s)'\n",
    "colnames(tpm_genes)[which(colnames(tpm_genes)=='transcript')] <- 'transcript_id(s)'\n",
    "colnames(counts_genes)[which(colnames(counts_genes)=='transcript')] <- 'transcript_id(s)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# These samples failed\n",
    "failed <- setdiff(colnames(tpm_genes), pass) %>% .[!(. %in% c('gene_id', 'transcript_id(s)'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "counts_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "counts_genes %<>% dplyr::select(c(\"gene_id\",\"transcript_id(s)\", pass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "head(tpm_transcripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "pass <- setdiff(pass,c(\"gene_id\",\"transcript_id(s)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "tpm_genes %<>% dplyr::select(c(\"gene_id\",\"transcript_id(s)\", pass))\n",
    "tpm_transcripts %<>% dplyr::select(c(\"transcript_id\", \"gene_id\", pass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Plot of the samples that fail\n",
    "plot <- ggplot(nzdf, aes(nz)) +\n",
    "  geom_histogram(bins = 100, color='black', fill='white') +\n",
    "  geom_vline(xintercept = zero_threshold, linetype=2) +\n",
    "  geom_label_repel(data = nzdf %>% filter(nz > zero_threshold), aes(x=nz, y=0, label=CL), size=5, fill=rgb(1,1,1,0.5))\n",
    "\n",
    "ggsave(plot, filename ='data/zero_transcripts_distribution.png', width=20, height = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='data/zero_transcripts_distribution.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allie post processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "colnames(tpm_transcripts)[which(colnames(tpm_transcripts)=='transcript')] <- 'transcript_id(s)'\n",
    "colnames(tpm_genes)[which(colnames(tpm_genes)=='transcript')] <- 'transcript_id(s)'\n",
    "colnames(counts_genes)[which(colnames(counts_genes)=='transcript')] <- 'transcript_id(s)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "print(colnames(tpm_genes)[1:10])\n",
    "tpm_gene_ids <- gsub(\"\\\\..*\", \"\", tpm_genes$gene_id)\n",
    "if(nrow(tpm_genes) != length(unique(tpm_gene_ids))){\n",
    "  print(\"Duplicated ensembl ids\")\n",
    "  print(nrow(tpm_genes) - length(unique(tpm_gene_ids)))\n",
    "  tpm_genes <- tpm_genes[-which(duplicated(tpm_gene_ids)==T),]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "annotations<-download.raw.from.taiga(data.name='gene-annotations-e737', data.version=1, data.file='gencode.v29.annotation')\n",
    "tpm_protein_coding <- prepare_depmap_TPM_for_taiga(tpm_genes, log_transform=T, just_protein_coding=T, gencode_annotations=annotations)\n",
    "tpm_genes <- prepare_depmap_TPM_for_taiga(tpm_genes, log_transform=T, just_protein_coding=F, gencode_annotations=annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "### transcripts\n",
    "library('cdsomics')\n",
    "print(colnames(tpm_transcripts)[1:10])\n",
    "tpm_transcript_id <- gsub(\"\\\\..*\", \"\", tpm_transcripts$`transcript_id`)\n",
    "if(nrow(tpm_transcripts) != length(unique(tpm_transcript_id))) {\n",
    "  print(\"Duplicated transcript ids\")\n",
    "  print(nrow(tpm_transcripts) - length(unique(tpm_transcript_id)))\n",
    "  tpm_transcript <- tpm_transcripts[-which(duplicated(tpm_transcript_id)==T),]\n",
    "}\n",
    "tpm_transcripts <- prepare_depmap_transcripts_for_taiga(tpm_transcript, gencode_annotations = annotations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "### counts\n",
    "print(colnames(counts_genes)[1:10])\n",
    "counts_gene_ids <- gsub(\"\\\\..*\", \"\", counts_genes$gene_id)\n",
    "if(nrow(counts_genes) != length(unique(counts_gene_ids))) {\n",
    "  print(\"Duplicate ensembl ids\")\n",
    "  print(length(which(duplicated(counts_gene_ids)==T)))\n",
    "  counts_genes <- counts_genes[-which(duplicated(counts_gene_ids)==T),]\n",
    "}\n",
    "counts_genes <- prepare_depmap_TPM_for_taiga(counts_genes, gencode_annotations =annotations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "write.table(\n",
    "  counts_genes, \n",
    "  file = paste0('temp/expression.', release,'.counts'), \n",
    "  sep = '\\t', quote = F)\n",
    "write.table(\n",
    "  tpm_genes, \n",
    "  file = paste0('temp/expression.', release,'.genes'), \n",
    "  sep = '\\t', quote = F)\n",
    "write.table(\n",
    "  tpm_protein_coding, \n",
    "  file = paste0('temp/expression.', release,'.protein_coding'), \n",
    "  sep = '\\t', quote = F)\n",
    "write.table(\n",
    "  tpm_transcripts, \n",
    "  file = paste0('temp/expression.', release,'.transcripts'),\n",
    "  sep = '\\t', quote = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_genes = pd.read_csv('temp/expression.'+ release + '.counts', sep='\\t',index_col=0)\n",
    "tpm_genes = pd.read_csv('temp/expression.'+ release + '.genes', sep='\\t' ,index_col=0)\n",
    "tpm_proteincoding = pd.read_csv('temp/expression.'+ release + '.protein_coding', sep='\\t',index_col=0)\n",
    "tpm_transcripts = pd.read_csv('temp/expression.'+ release + '.transcripts', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevcounts = tc.get(name='depmap-expression-87f8', version=13, file=\"expression.\"+prevname+'.counts')\n",
    "prevgenes = tc.get(name='depmap-expression-87f8', version=13, file=\"expression.\"+prevname+'.genes')\n",
    "prevtranscripts = tc.get(name='depmap-expression-87f8', version=13, file=\"expression.\"+prevname+'.transcripts')\n",
    "prevproteincoding = tc.get(name='depmap-expression-87f8', version=13, file=\"expression.\"+prevname+'.protein_coding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNonZeroCorrelations(df1,df2):\n",
    "    res=[]\n",
    "    for k,val in df1.iterrows():\n",
    "        if k in df2.index:\n",
    "            corr = np.corrcoef(df1.loc[k],df2.loc[k])\n",
    "            if corr[0,1]<0.99999999999999:\n",
    "                res.append((k,corr[0,1]))\n",
    "        else:\n",
    "            print(k+\" not in second df\")\n",
    "    print(\"found \"+str(len(res))+\" samples that did not match\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched = getNonZeroCorrelations(counts_genes ,prevcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is it because of  duplicate version?\n",
    "rnasamples = ccle_refsamples[ccle_refsamples.datatype=='rna']\n",
    "for i,val in unmatched:\n",
    "    print(len(rnasamples[rnasamples.arxspan_id==i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <20million (~10million)\n",
    "removedfromPiPelineNotEnoughReads = ['CDS-ABH0uZ','CDS-fk564T','CDS-kU30H5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notinnew = set(prevcounts.index) - set(counts_genes.index)\n",
    "notinnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removedFromListNotEnoughExpressedGenes = [\"ACH-001577\", \"ACH-001686\", \"ACH-002463\", \"ACH-002055\", \"ACH-001388\", \"ACH-000052\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replacing different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_genes = counts_genes.T\n",
    "tpm_genes = tpm_genes.T\n",
    "tpm_proteincoding = tpm_proteincoding.T\n",
    "tpm_transcripts = tpm_transcripts.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, _ in unmatched+[('ACH-000561','')]:\n",
    "    counts_genes[k] = prevcounts.loc[k].values\n",
    "    tpm_genes[k] = prevgenes.loc[k].values\n",
    "    tpm_proteincoding[k] = prevproteincoding.loc[k].values\n",
    "    tpm_transcripts[k] = prevtranscripts.loc[k].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_genes = counts_genes.T\n",
    "tpm_proteincoding = tpm_proteincoding.T\n",
    "tpm_genes = tpm_genes.T\n",
    "tpm_transcripts = tpm_transcripts.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = tc.get(name='depmap-a0ab', file='sample_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding train and test set\n",
    "trainame = [val for val in new1&prev if val[:3] == 'ACH']\n",
    "testname = [val for val in new1-prev if val[:3] == 'ACH']\n",
    "\n",
    "#looking at the 2000 most variable genes in the two sets\n",
    "genetolookfor = 2000\n",
    "gene_var = counts_genes[trainame].var(1).values\n",
    "print(len(gene_var))\n",
    "sorting = np.argsort(gene_var)[-genetolookfor:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unregistered = set(testname) - set(metadata[\"DepMap_ID\"].values.tolist())\n",
    "unregistered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(counts_genes['ACH-001767']) - np.count_nonzero(counts_genes['ACH-001767'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and reodering train and test sets\n",
    "traindata = counts_genes[set(trainame)-unregistered].values[sorting].T\n",
    "trainlabels = [metadata[metadata[\"DepMap_ID\"]==val][\"disease\"].values[0] for val in counts_genes[set(trainame)-unregistered].columns.tolist() if val not in unregistered]\n",
    "\n",
    "testdata = counts_genes[set(testname)-unregistered].values[sorting].T\n",
    "testlabels = [metadata[metadata[\"DepMap_ID\"]==val][\"disease\"].values[0] for val in counts_genes[set(testname)-unregistered].columns.tolist() if val not in unregistered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn KNN classifier to the metadata diseases\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(traindata, trainlabels) \n",
    "predicted = neigh.predict(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = trainlabels + testlabels\n",
    "colors=[0]*len(trainlabels)\n",
    "colors.extend([1,2,2,2,2,1,2,2,2,1,2])\n",
    "data = np.vstack([traindata,testdata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot them with TSNE, highlight the points that failed and show colors for diseases\n",
    "dimred = TSNE(2,10).fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(dimred, labels=labels,colors=colors, radi=1.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files for taiga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls temp/expression.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR 20Q3 KNOW THAT WE WILL RETRIEVE DATA FROM 20Q2 WITH THE VERY DFFERENT CELL LINES!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('temp/expression.20Q2.genes.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.loc['ACH-001632']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.update_dataset(dataset_permaname=\"depmap-expression-87f8\",\n",
    "                 upload_file_path_dict={'temp/expression.'+release+'.transcripts.tsv': 'NumericMatrixTSV',\n",
    "                                       'temp/expression.'+release+'.genes.tsv': 'NumericMatrixTSV',\n",
    "                                       'temp/expression.'+release+'.counts.tsv': 'NumericMatrixTSV',\n",
    "                                       'temp/expression.'+release+'.protein_coding.tsv': 'NumericMatrixTSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# RNAseq\n",
    "\n",
    "Combined segment and gene-level CN calls from Broad WES, Sanger WES, and Broad SNP. Relative CN, not log2 transformed.\n",
    "\n",
    "PORTAL TEAM SHOULD NOT USE THIS: There are lines here that should not make it even to internal. Must use subsetted dataset instead. These data will not make it on the portal starting 19Q1. With the DMC portal, there is new cell line release prioritization as to which lines can be included, so a new taiga dataset will be created containing CN for the portal.\n",
    "\n",
    "version 1-8: guillaume releases\n",
    "version 9: 19Q3 release\n",
    "version 10:  adding missing samples in Terra merge files\n",
    "version 11: 19Q4 new release.\n",
    "\n",
    "Adding 93 new cell lines. \n",
    "Some cells lines have been removed because they:\n",
    "\n",
    " - had too many 0 values = [\"ACH-001388\" \"ACH-001577\" \"ACH-001767\" \"ACH-002463\"] \n",
    "\n",
    "Some cells lines have been flagged as:\n",
    "\n",
    " - having too many..\n",
    " \n",
    "version 12:\n",
    "    uploading as matrices \n",
    "    \n",
    "version 13:\n",
    "    20Q1 new release. \n",
    "    \n",
    "version 14:\n",
    "    20Q2 new release, adding 50 new samples\n",
    " \n",
    "transcriptions (Transcripts rpkm):\n",
    "\n",
    "genes (gene rpkm):\n",
    "__Rows__:\n",
    "__Columns__:\n",
    "Counts (gene counts):\n",
    "__Rows__:\n",
    "__Columns__:\n",
    "Gene level CN data:\n",
    "__Rows__:\n",
    "__Columns__:\n",
    " DepMap cell line IDs\n",
    " gene names in the format HGNC\\_symbol (Entrez\\_ID)\n",
    "DepMap\\_ID, Chromosome, Start, End, Num\\_Probes, Segment\\_Mean\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevprevcells = set(tc.get(name='depmap-rnaseq-expression-data-363a', file='internal_'+prevprevname+'_proteincoding_tpm',version=prevprevversion).index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsheets = sheets.get(privacy_release_url).sheets[6].to_frame()\n",
    "rna_ibm_embargo = [i for i in gsheets['RNAseq_IBM_embargo'].values.tolist() if i is not np.nan]\n",
    "rna_dmc_embargo = [i for i in gsheets['RNAseq_DMC_embargo'].values.tolist() if i is not np.nan]\n",
    "blacklist = [i for i in gsheets['blacklist'].values.tolist() if i is not np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ACH-001006',\n",
       "  'ACH-001007',\n",
       "  'ACH-001008',\n",
       "  'ACH-001009',\n",
       "  'ACH-001010',\n",
       "  'ACH-001062',\n",
       "  'ACH-001082',\n",
       "  'ACH-001083',\n",
       "  'ACH-001153',\n",
       "  'ACH-001154',\n",
       "  'ACH-001155',\n",
       "  'ACH-001156',\n",
       "  'ACH-001157',\n",
       "  'ACH-001158',\n",
       "  'ACH-001159',\n",
       "  'ACH-001160',\n",
       "  'ACH-001161',\n",
       "  'ACH-001178',\n",
       "  'ACH-001179',\n",
       "  'ACH-001181',\n",
       "  'ACH-001204',\n",
       "  'ACH-001218',\n",
       "  'ACH-001219',\n",
       "  'ACH-001220',\n",
       "  'ACH-001221',\n",
       "  'ACH-001222',\n",
       "  'ACH-001223',\n",
       "  'ACH-001251',\n",
       "  'ACH-001252',\n",
       "  'ACH-001253',\n",
       "  'ACH-001254',\n",
       "  'ACH-001255',\n",
       "  'ACH-001256',\n",
       "  'ACH-001257',\n",
       "  'ACH-001258',\n",
       "  'ACH-001259',\n",
       "  'ACH-001269',\n",
       "  'ACH-001434',\n",
       "  'ACH-001752',\n",
       "  'ACH-001753',\n",
       "  'ACH-001754',\n",
       "  'ACH-001755',\n",
       "  'ACH-001756',\n",
       "  'ACH-001757',\n",
       "  'ACH-001758',\n",
       "  'ACH-001759',\n",
       "  'ACH-001760',\n",
       "  'ACH-001780',\n",
       "  'ACH-001781',\n",
       "  'ACH-001782',\n",
       "  'ACH-001783',\n",
       "  'ACH-001784',\n",
       "  'ACH-001785',\n",
       "  'ACH-002446-311cas9',\n",
       "  'ACH-002476',\n",
       "  'ACH-003000'],\n",
       " ['ACH-001024',\n",
       "  'ACH-001036',\n",
       "  'ACH-001293',\n",
       "  'ACH-001349',\n",
       "  'ACH-001385',\n",
       "  'ACH-001429',\n",
       "  'ACH-001437',\n",
       "  'ACH-001438',\n",
       "  'ACH-001449',\n",
       "  'ACH-001493',\n",
       "  'ACH-001502',\n",
       "  'ACH-001533',\n",
       "  'ACH-001537',\n",
       "  'ACH-001547',\n",
       "  'ACH-001574',\n",
       "  'ACH-001577',\n",
       "  'ACH-001578',\n",
       "  'ACH-001603',\n",
       "  'ACH-001620',\n",
       "  'ACH-001625',\n",
       "  'ACH-001626',\n",
       "  'ACH-001632',\n",
       "  'ACH-001662',\n",
       "  'ACH-001669',\n",
       "  'ACH-001672',\n",
       "  'ACH-001676',\n",
       "  'ACH-001678',\n",
       "  'ACH-001686',\n",
       "  'ACH-001693',\n",
       "  'ACH-001696',\n",
       "  'ACH-001703',\n",
       "  'ACH-001716',\n",
       "  'ACH-001767',\n",
       "  'ACH-001820',\n",
       "  'ACH-001847',\n",
       "  'ACH-001854',\n",
       "  'ACH-001855',\n",
       "  'ACH-001970',\n",
       "  'ACH-001971',\n",
       "  'ACH-001973',\n",
       "  'ACH-002010',\n",
       "  'ACH-002014',\n",
       "  'ACH-002021',\n",
       "  'ACH-002048',\n",
       "  'ACH-002055',\n",
       "  'ACH-002065',\n",
       "  'ACH-002475',\n",
       "  'ACH-002508',\n",
       "  'ACH-002510',\n",
       "  'ACH-002511',\n",
       "  'ACH-002512',\n",
       "  'ACH-002709',\n",
       "  'ACH-002462',\n",
       "  'ACH-002463',\n",
       "  'ACH-002464',\n",
       "  'ACH-002465',\n",
       "  'ACH-002466',\n",
       "  'ACH-002467'],\n",
       " ['ACH-001151',\n",
       "  'ACH-001194',\n",
       "  'ACH-001229',\n",
       "  'ACH-001339',\n",
       "  'ACH-001347',\n",
       "  'ACH-001370',\n",
       "  'ACH-001384',\n",
       "  'ACH-001388',\n",
       "  'ACH-001389',\n",
       "  'ACH-001393',\n",
       "  'ACH-001395',\n",
       "  'ACH-001398',\n",
       "  'ACH-001419',\n",
       "  'ACH-001422',\n",
       "  'ACH-001441',\n",
       "  'ACH-001453',\n",
       "  'ACH-001512',\n",
       "  'ACH-001516',\n",
       "  'ACH-001518',\n",
       "  'ACH-001519',\n",
       "  'ACH-001528',\n",
       "  'ACH-001539',\n",
       "  'ACH-001541',\n",
       "  'ACH-001569',\n",
       "  'ACH-001573',\n",
       "  'ACH-001616',\n",
       "  'ACH-001627',\n",
       "  'ACH-001630',\n",
       "  'ACH-001648',\n",
       "  'ACH-001649',\n",
       "  'ACH-001679',\n",
       "  'ACH-001687',\n",
       "  'ACH-001688',\n",
       "  'ACH-001690',\n",
       "  'ACH-001692',\n",
       "  'ACH-001694',\n",
       "  'ACH-001698',\n",
       "  'ACH-001708',\n",
       "  'ACH-001709',\n",
       "  'ACH-001719',\n",
       "  'ACH-001834',\n",
       "  'ACH-001991',\n",
       "  'ACH-002019',\n",
       "  'ACH-002022',\n",
       "  'ACH-002023',\n",
       "  'ACH-002024',\n",
       "  'ACH-002025',\n",
       "  'ACH-002026',\n",
       "  'ACH-002027',\n",
       "  'ACH-002038',\n",
       "  'ACH-002039',\n",
       "  'ACH-002041',\n",
       "  'ACH-002042',\n",
       "  'ACH-002046',\n",
       "  'ACH-002062',\n",
       "  'ACH-002067',\n",
       "  'ACH-002069',\n",
       "  'ACH-002458',\n",
       "  'ACH-002459',\n",
       "  'ACH-002460',\n",
       "  'ACH-002461'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blacklist, rna_dmc_embargo, rna_ibm_embargo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing first blacklisted, then embargoed, to create two datasets\n",
    "print(len(counts_genes))\n",
    "counts_genes = counts_genes[~counts_genes.index.isin(blacklist)]\n",
    "print(len(counts_genes))\n",
    "counts_genes.to_csv('temp/internal_'+release+'_counts', sep='\\t')\n",
    "print(len(tpm_genes))\n",
    "tpm_genes = tpm_genes[~tpm_genes.index.isin(blacklist)]\n",
    "print(len(tpm_genes))\n",
    "tpm_genes.to_csv('temp/internal_'+release+'_tpm', sep='\\t')\n",
    "print(len(tpm_proteincoding))\n",
    "tpm_proteincoding = tpm_proteincoding[~tpm_proteincoding.index.isin(blacklist)]\n",
    "print(len(tpm_proteincoding))\n",
    "tpm_proteincoding.to_csv('temp/internal_'+release+'_proteincoding_tpm', sep='\\t')\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts = tpm_transcripts[~tpm_transcripts.index.isin(blacklist)]\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts.to_csv('temp/internal_'+release+'_transcripts_tpm', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.update_dataset(dataset_permaname=\"depmap-rnaseq-expression-data-363a\",\n",
    "                 upload_file_path_dict={'temp/internal_'+release+'_counts': 'NumericMatrixTSV',\n",
    "                                       'temp/internal_'+release+'_tpm': 'NumericMatrixTSV',\n",
    "                                       'temp/internal_'+release+'_proteincoding_tpm': 'NumericMatrixTSV',\n",
    "                                       'temp/internal_'+release+'_transcripts_tpm': 'NumericMatrixTSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# INTERNAL RNA\n",
    "\n",
    "* Version 1-3 Internal 18Q1*\n",
    "\n",
    "All CCLE cell lines with RNAseq data.\n",
    "\n",
    "Original data source:\n",
    "\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_depMap_18Q1_RNAseq_reads_20180201.gct`\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_depMap_18Q1_RNAseq_RPKM_20180201.gct`\n",
    "\n",
    "Version 2 of RPKM data are log2-transformed with a noisy floor at -3 (-3 + N(0, 0.1))\n",
    "\n",
    "* Version 4-6 Internal 18Q2*\n",
    "\n",
    "Original data source:\n",
    "\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_depMap_18q2_RNAseq_reads_20180420.gct` `/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_depMap_18q2_RNAseq_RPKM_20180420.gct`\n",
    "\n",
    "RPKM data are log2-transformed with a noisy floor at -3 (-3 + N(0, 0.1)). Reads file unaltered aside from formatting row / column names.\n",
    "\n",
    "* Version 7 Internal 18Q2*\n",
    "\n",
    "Includes a matrix with genes filtered by HGNC protein-coding gene locus group.\n",
    "\n",
    "* Version 8-10 Internal 18Q3*\n",
    "\n",
    "use version 10\n",
    "\n",
    "Original data source:\n",
    "\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_depMap_18q3_RNAseq_reads_20180716.gct` `/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_depMap_18q3_RNAseq_RPKM_20180716.gct`\n",
    "\n",
    "RPKM data are log2-transformed with a noisy floor at -3 (-3 + N(0, 0.1)). Reads file unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are Broad (Arxspan) cell line IDs.\n",
    "\n",
    "Columns: In the complete RPKM and read datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding RPKM and read datasets column names are HGNC_symbol (Entrez_ID)\n",
    "\n",
    "version 9 updates names, and slightly different RPKM values due to randomly added noisy floor (using a seed of 4)\n",
    "\n",
    "version 10 removes duplicate gene names from the protein coding datasets\n",
    "\n",
    "* Version 11-12 Internal 18Q4*\n",
    "\n",
    "18Q4 transcript level data is found in version 14. (In versions 1-13 transcript data contains only gene level not transcript level data)\n",
    "\n",
    "changing to TPM expression\n",
    "\n",
    "Original data source:\n",
    "\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_DepMap_18Q4_rsem_genes_tpm_20181029.txt` `/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_DepMap_18Q4_rsem_transcripts_tpm_20181029.gct` `/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_DepMap_18Q4_RNAseq_reads_20181029.gct` `/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_DepMap_18Q4_RNAseq_RPKM_20181029.gct`\n",
    "\n",
    "TPM data is the primary expression data now. It is log2-transformed with a pseudo count of 1 added. The TPM data contains 4 cell lines not included in the RPKM data.\n",
    "\n",
    "RPKM data are log2-transformed with a pseudo count of 1 added. RPKM values are no longer thresholded.\n",
    "\n",
    "Reads/transcript files unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are DepMap (Arxspan) cell line IDs\n",
    "\n",
    "Columns: In the complete TPM, TPM transcripts, RPKM and read datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding TPM dataset column names are HGNC_symbol (Entrez_ID)\n",
    "\n",
    "* Version 13-15 Internal 19Q1*\n",
    "\n",
    "Version 15 contains the correct data sets for 19Q1 - 2 cell lines are removed\n",
    "\n",
    "Version 14 contains the correct transcript level data for 18Q4\n",
    "\n",
    "* Version 16 Internal 19Q2*\n",
    "\n",
    "* Version 17 Internal 19Q3*\n",
    "\n",
    "* Version 18 Internal 19Q4\n",
    "\n",
    "Adding 93 new cell lines - Blacklisted\n",
    "Some cells lines have been removed because they:\n",
    "\n",
    " - had too many 0 values = [\"ACH-001388\" \"ACH-001577\" \"ACH-001767\" \"ACH-002463\"] \n",
    "\n",
    "Some cells lines have been flagged as:\n",
    "\n",
    " - None\n",
    "* Version 19 Internal 19Q4\n",
    "removing blacklisted\n",
    "\n",
    "* Version 20 Internal 19Q4\n",
    "removing blacklisted in transcripts\n",
    "\n",
    "* Version 21 Internal 19Q4\n",
    "uploading as matrices \n",
    "\n",
    "* Version 21 Internal 20Q1\n",
    "adding 6 new cell lines\n",
    "\n",
    "* Version 22 Internal 20Q2\n",
    "adding  new cell lines\n",
    "\n",
    "data is aligned to hg38\n",
    "\n",
    "read count data is created using RSEM, which, when a read maps to multiple places, splits the counts between genes, with the weight based on the likelihood that it came from one gene or the other, so counts data may not be integers\n",
    "\n",
    "TPM data is log2-transformed with a pseudo count of 1 added. log2(X+1)\n",
    "\n",
    "Reads/transcript files unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are DepMap cell line IDs Mapping between Broad IDs and CCLE IDs can be done using a R or python package\n",
    "\n",
    "To install R implementation: options(repos = c(\"https://iwww.broadinstitute.org/~datasci/R-packages\", \"https://cran.cnr.berkeley.edu\")) install.packages('celllinemapr')\n",
    "\n",
    "To install python implementation: pip install https://intranet.broadinstitute.org/~datasci/python-packages/cell_line_mapper-0.1.9.tar.gz)\n",
    "\n",
    "Columns: In the complete TPM and read counts datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding TPM dataset column names are HGNC_symbol (Entrez_ID). In the TPM transcript dataset column names are HGNC_symbol (Transcript_ID) - the HGNC symbols are not unique, use the transcript IDs for unique identifiers.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AddToVirtual('depmap-a0ab', \"depmap-rnaseq-expression-data-363a\", files=[('CCLE_RNAseq_reads', 'internal_'+release+'_counts'),('CCLE_expression_full', 'internal_'+release+'_tpm'),('CCLE_expression', 'internal_'+release+'_proteincoding_tpm'),('CCLE_RNAseq_transcripts', 'internal_'+release+'_transcripts_tpm')])\n",
    "\n",
    "AddToVirtual(virtual_internal, \"depmap-rnaseq-expression-data-363a\", files=[('CCLE_RNAseq_reads', 'internal_'+release+'_counts'),('CCLE_expression_full', 'internal_'+release+'_tpm'),('CCLE_expression', 'internal_'+release+'_proteincoding_tpm'),('CCLE_RNAseq_transcripts', 'internal_'+release+'_transcripts_tpm')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM\n",
    "\n",
    "like internal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387\n",
      "1356\n",
      "1387\n",
      "1356\n",
      "1387\n",
      "1356\n",
      "1387\n",
      "1356\n"
     ]
    }
   ],
   "source": [
    "## removing first blacklisted, then embargoed, to create two datasets\n",
    "print(len(counts_genes))\n",
    "counts_genes = counts_genes[~counts_genes.index.isin(rna_ibm_embargo)]\n",
    "print(len(counts_genes))\n",
    "counts_genes.to_csv('temp/dmc_'+release+'_counts', sep='\\t')\n",
    "print(len(tpm_genes))\n",
    "tpm_genes = tpm_genes[~tpm_genes.index.isin(rna_ibm_embargo)]\n",
    "print(len(tpm_genes))\n",
    "tpm_genes.to_csv('temp/dmc_'+release+'_tpm', sep='\\t')\n",
    "print(len(tpm_proteincoding))\n",
    "tpm_proteincoding = tpm_proteincoding[~tpm_proteincoding.index.isin(rna_ibm_embargo)]\n",
    "print(len(tpm_proteincoding))\n",
    "tpm_proteincoding.to_csv('temp/dmc_'+release+'_proteincoding_tpm', sep='\\t')\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts = tpm_transcripts[~tpm_transcripts.index.isin(rna_ibm_embargo)]\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts.to_csv('temp/dmc_'+release+'_transcripts_tpm', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading dmc_20Q2_counts...\n",
      "hitting https://cds.team/taiga/api/datafile/2731c2300a264c92846d517cfa857220\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: dmc_20Q2_counts properly converted and uploaded\n",
      "Uploading dmc_20Q2_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/2731c2300a264c92846d517cfa857220\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: dmc_20Q2_tpm properly converted and uploaded\n",
      "Uploading dmc_20Q2_proteincoding_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/2731c2300a264c92846d517cfa857220\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: dmc_20Q2_proteincoding_tpm properly converted and uploaded\n",
      "Uploading dmc_20Q2_transcripts_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/2731c2300a264c92846d517cfa857220\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: dmc_20Q2_transcripts_tpm properly converted and uploaded\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 1a0ae385f48448c184054c86850635b3 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/1a0ae385f48448c184054c86850635b3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1a0ae385f48448c184054c86850635b3'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.update_dataset(dataset_permaname=\"depmap-rnaseq-expression-data-80ef\",\n",
    "                 upload_file_path_dict={'temp/dmc_'+release+'_counts': 'NumericMatrixTSV',\n",
    "                                       'temp/dmc_'+release+'_tpm': 'NumericMatrixTSV',\n",
    "                                       'temp/dmc_'+release+'_proteincoding_tpm': 'NumericMatrixTSV',\n",
    "                                       'temp/dmc_'+release+'_transcripts_tpm': 'NumericMatrixTSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# DMC RNA\n",
    "\n",
    "* Version 1-3 DMC 19Q1*\n",
    "\n",
    "version 3 contains the correct data for 19Q1\n",
    "\n",
    "version 2 contains correct TPM transcript data (in version 1 transcript data contains only gene level not transcript level data)\n",
    "\n",
    "* Version 4 DMC 19Q2*\n",
    "\n",
    "* Version 5 DMC 19Q3*\n",
    "\n",
    "* Version 6 DMC 19Q4*\n",
    "\n",
    "Adding 93 new cell lines - Blacklisted - IBM\n",
    "Some cells lines have been removed because they:\n",
    "\n",
    " - had too many 0 values = [\"ACH-001388\" \"ACH-001577\" \"ACH-001767\" \"ACH-002463\"] \n",
    "\n",
    "Some cells lines have been flagged as:\n",
    "\n",
    " - None\n",
    "\n",
    "* Version 7 DMC 19Q4\n",
    "\n",
    "removing blacklisted\n",
    "\n",
    "* Version 8 DMC 19Q4\n",
    "\n",
    "removing blacklisted in transcripts\n",
    "\n",
    "* Version 9 DMC 19Q4\n",
    "\n",
    "uploading as numeric matrix\n",
    "\n",
    "* Version 10 DMC 20Q1\n",
    "\n",
    "* Version 11 DMC 20Q2\n",
    "adding  new cell lines\n",
    "\n",
    "\n",
    "data is aligned to hg38\n",
    "\n",
    "read count data is created using RSEM, which, when a read maps to multiple places, splits the counts between genes, with the weight based on the likelihood that it came from one gene or the other, so counts data may not be integers\n",
    "\n",
    "TPM data is log2-transformed with a pseudo count of 1 added.\n",
    "\n",
    "Reads/transcript files unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are DepMap cell line IDs\n",
    "\n",
    "Columns: In the complete TPM and read counts datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding TPM dataset column names are HGNC_symbol (Entrez_ID). In the TPM transcript dataset column names are HGNC_symbol (Transcript_ID) - the HGNC symbols are not unique, use the transcript IDs for unique identifiers.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CCLE_RNAseq_reads', 'depmap-rnaseq-expression-data-80ef.13/dmc_20Q2_counts'), ('CCLE_expression_full', 'depmap-rnaseq-expression-data-80ef.13/dmc_20Q2_tpm'), ('CCLE_expression', 'depmap-rnaseq-expression-data-80ef.13/dmc_20Q2_proteincoding_tpm'), ('CCLE_RNAseq_transcripts', 'depmap-rnaseq-expression-data-80ef.13/dmc_20Q2_transcripts_tpm'), ('Achilles_gene_effect', 'avana-consortium-20q2-d98a.1/gene_effect'), ('Achilles_gene_dependency', 'avana-consortium-20q2-d98a.1/gene_dependency'), ('Achilles_logfold_change', 'avana-consortium-20q2-d98a.1/logfold_change'), ('Achilles_gene_effect_unscaled', 'avana-consortium-20q2-d98a.1/gene_effect_unscaled'), ('common_essentials', 'avana-consortium-20q2-d98a.1/essential_genes'), ('Achilles_dropped_guides', 'avana-consortium-20q2-d98a.1/dropped_guides'), ('CCLE_mutations', 'depmap-mutation-calls-dfce.14/depmap_20Q2_mutation_calls'), ('CCLE_segmented_cn', 'depmap-cn-data-9b9d.12/dmc_20Q2_segs_cn'), ('CCLE_gene_cn', 'depmap-cn-data-9b9d.12/dmc_20Q2_gene_cn'), ('CCLE_fusions_unfiltered', 'gene-fusions-375f.7/unfiltered_fusions_20Q2'), ('README', 'dmc-20q2-2db6.13/README.txt'), ('nonessentials', 'avana-consortium-20q2-d98a.1/nonessential_genes'), ('sample_info', 'dmc-20q2-2db6.11/sample_info'), ('Achilles_common_essentials', 'avana-consortium-20q2-d98a.1/pan_dependent_genes'), ('Achilles_guide_efficacy', 'avana-consortium-20q2-d98a.1/guide_efficacy'), ('Achilles_raw_readcounts', 'avana-consortium-20q2-d98a.1/raw_readcounts'), ('Achilles_logfold_change_failures', 'avana-consortium-20q2-d98a.1/logfold_change_failures'), ('Achilles_guide_map', 'avana-consortium-20q2-d98a.1/guide_gene_map'), ('Achilles_high_variance_genes', 'avana-consortium-20q2-d98a.1/high_variance_genes'), ('Achilles_replicate_map', 'avana-consortium-20q2-d98a.1/replicate_map'), ('CCLE_fusions', 'gene-fusions-375f.7/filtered_fusions_20Q2'), ('Achilles_raw_readcounts_failures', 'avana-consortium-20q2-d98a.1/raw_readcounts_failing')]\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datafile/0207c48d605e42448899028d397979c4\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 00f50e512ebc4193b343da80225e90bb created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/00f50e512ebc4193b343da80225e90bb\n"
     ]
    }
   ],
   "source": [
    "AddToVirtual(virtual_dmc, \"depmap-rnaseq-expression-data-80ef\", files=[('CCLE_RNAseq_reads', 'dmc_'+release+'_counts'),('CCLE_expression_full', 'dmc_'+release+'_tpm'),('CCLE_expression', 'dmc_'+release+'_proteincoding_tpm'),('CCLE_RNAseq_transcripts', 'dmc_'+release+'_transcripts_tpm')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1356\n",
      "1312\n",
      "1312\n",
      "1312\n",
      "1356\n",
      "1312\n",
      "1304\n"
     ]
    }
   ],
   "source": [
    "## removing first blacklisted, then embargoed, to create two datasets\n",
    "print(len(counts_genes))\n",
    "counts_genes = counts_genes[counts_genes.index.isin(prevprevcells)]\n",
    "print(len(counts_genes))\n",
    "counts_genes[~counts_genes.index.isin(rna_dmc_embargo)].to_csv('temp/public_'+release+'_counts', sep='\\t')\n",
    "tpm_genes = tpm_genes[tpm_genes.index.isin(prevprevcells)]\n",
    "print(len(tpm_genes))\n",
    "tpm_genes[~tpm_genes.index.isin(rna_dmc_embargo)].to_csv('temp/public_'+release+'_tpm', sep='\\t')\n",
    "tpm_proteincoding = tpm_proteincoding[tpm_proteincoding.index.isin(prevprevcells)]\n",
    "print(len(tpm_proteincoding))\n",
    "tpm_proteincoding[~tpm_proteincoding.index.isin(rna_dmc_embargo)].to_csv('temp/public_'+release+'_proteincoding_tpm', sep='\\t')\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts = tpm_transcripts[tpm_transcripts.index.isin(prevprevcells)]\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts = tpm_transcripts[~tpm_transcripts.index.isin(rna_dmc_embargo)]\n",
    "print(len(tpm_transcripts))\n",
    "tpm_transcripts.to_csv('temp/public_'+release+'_transcripts_tpm', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading public_20Q2_counts...\n",
      "hitting https://cds.team/taiga/api/datafile/69c8832e12964719a0ae671d4c837c8f\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: public_20Q2_counts properly converted and uploaded\n",
      "Uploading public_20Q2_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/69c8832e12964719a0ae671d4c837c8f\n",
      "Conversion and upload...:\n",
      "\t Waiting in the task queue\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: public_20Q2_tpm properly converted and uploaded\n",
      "Uploading public_20Q2_proteincoding_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/69c8832e12964719a0ae671d4c837c8f\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: public_20Q2_proteincoding_tpm properly converted and uploaded\n",
      "Uploading public_20Q2_transcripts_tpm...\n",
      "hitting https://cds.team/taiga/api/datafile/69c8832e12964719a0ae671d4c837c8f\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: public_20Q2_transcripts_tpm properly converted and uploaded\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 0c5a51c7308d4594a039c737609dd6ab created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/0c5a51c7308d4594a039c737609dd6ab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0c5a51c7308d4594a039c737609dd6ab'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.update_dataset(dataset_permaname=\"depmap-rnaseq-expression-data-ccd0\",\n",
    "                 upload_file_path_dict={'temp/public_'+release+'_counts': 'NumericMatrixTSV',\n",
    "                                       'temp/public_'+release+'_tpm': 'NumericMatrixTSV',\n",
    "                                       'temp/public_'+release+'_proteincoding_tpm': 'NumericMatrixTSV',\n",
    "                                       'temp/public_'+release+'_transcripts_tpm': 'NumericMatrixTSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# PUBLIC RNA\n",
    "\n",
    "* Version 1-2 Public 18Q1*\n",
    "\n",
    "Original source (`CCLE_DepMap_18Q1_RNAseq_reads_20180214.gct`, `CCLE_DepMap_18Q1_RNAseq_RPKM_20180214.gct`) downloaded from portals.broadinstitute.org/ccle\n",
    "RPKM file is log2(RPKM) with a \"noisy floor\" around -3 (-3 + N(0, 0.1))\n",
    "\n",
    "* Version 3-5 Public 18Q2*\n",
    "\n",
    "gene expression data (RNAseq for1,076 cell lines, including data for 28 newly released cell lines)\n",
    "\n",
    "original source: (`/xchip/ccle_dist/public/DepMap_18Q2/CCLE_DepMap_18Q2_RNAseq_RPKM_20180502.gct`, `/xchip/ccle_dist/public/DepMap_18Q2/CCLE_DepMap_18Q2_RNAseq_reads_20180502.gct`)\n",
    "* Version 6-7 Public 18Q3*\n",
    "\n",
    "gene expression data (80 newly released cell lines)\n",
    "\n",
    "Original data source:\n",
    "\n",
    "`/xchip/ccle_dist/public/DepMap_18Q3/CCLE_DepMap_18q3_RNAseq_reads_20180718.gct`\n",
    "`/xchip/ccle_dist/public/DepMap_18Q3/CCLE_DepMap_18q3_RNAseq_RPKM_20180718.gct`\n",
    "\n",
    "RPKM data are log2-transformed with a noisy floor at -3 (-3 + N(0, 0.1)). Reads file unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are Broad (Arxspan) cell line IDs. Mapping between Broad IDs and CCLE IDs can be done using a R or python package\n",
    "\n",
    "To install R implementation: options(repos = c(\"https://iwww.broadinstitute.org/~datasci/R-packages\", \"https://cran.cnr.berkeley.edu\")) install.packages('celllinemapr')\n",
    "\n",
    "To install python implementation: pip install https://intranet.broadinstitute.org/~datasci/python-packages/cell_line_mapper-0.1.9.tar.gz)\n",
    "\n",
    "Columns: In the complete RPKM and read datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding RPKM and read datasets column names are HGNC_symbol (Entrez_ID)\n",
    "\n",
    "version 7 removes duplicate gene names from the protein coding datasets\n",
    "\n",
    "* Version 8-9 Public 18Q4*\n",
    "\n",
    "_ 18Q4 transcript level data is found in version 11. (In versions 8-9 transcript data contains only gene level not transcript level data)\n",
    "\n",
    "changing to TPM expression\n",
    "\n",
    "Original data source:\n",
    "\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_DepMap_18Q4_rsem_genes_tpm_20181029.txt`\n",
    "`/xchip/ccle_dist/broad_only/CMAG/expression/CCLE_DepMap_18Q4_rsem_transcripts_tpm_20181029.gct`\n",
    "`/xchip/ccle_dist/public/DepMap_18Q4/CCLE_DepMap_18q4_RNAseq_reads_20181029.gct`\n",
    "\n",
    "`/xchip/ccle_dist/public/DepMap_18Q4/CCLE_DepMap_18q4_RNAseq_RPKM_20181029.gct`\n",
    "\n",
    "TPM data is subsetted to just public cell lines using the cell line found in the RPKM dataset.\n",
    "\n",
    "TPM data is the primary expression data now. It is log2-transformed with a pseudo count of 1 added\n",
    "\n",
    "RPKM data are log2-transformed with a pseudo count of 1 added. RPKM values are no longer thresholded.\n",
    "\n",
    "Reads/transcript files unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are DepMap (Arxspan) cell line IDs\n",
    "\n",
    "Columns: In the complete TPM, TPM transcripts, RPKM and read datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding TPM and read datasets column names are HGNC_symbol (Entrez_ID)\n",
    "\n",
    "* Version 10-12 Public 19Q1*\n",
    "\n",
    "version 12 contains the correct data for 19Q1\n",
    "\n",
    "version 11 contains the correct transcript level data for 19Q1 and 18Q4\n",
    "\n",
    "* Version 13 Public 19Q2*\n",
    "\n",
    "* Version 14 Public 19Q3*\n",
    "\n",
    "* Version 15 Public 19Q4*\n",
    "Adding 93 new cell lines - Blacklisted - IBM - DMC\n",
    "Some cells lines have been removed because they:\n",
    "\n",
    " - had too many 0 values = [\"ACH-001388\" \"ACH-001577\" \"ACH-001767\" \"ACH-002463\"] \n",
    "\n",
    "Some cells lines have been flagged as:\n",
    "\n",
    " - None\n",
    " \n",
    "* Version 16 Public 19Q4*\n",
    "removing unauthorized cell lines\n",
    "\n",
    "* Version 17 Public 20Q1*\n",
    "adding 6 new lines\n",
    "\n",
    "* Version 18 Public 20Q2\n",
    "adding  new cell lines\n",
    "\n",
    "data is hg38 aligned\n",
    "\n",
    "read count data is created using RSEM, which, when a read maps to multiple places, splits the counts between genes, with the weight based on the likelihood that it came from one gene or the other, so counts data may not be integers\n",
    "\n",
    "TPM data is log2-transformed with a pseudo count of 1 added. log2(X+1)\n",
    "\n",
    "Reads/transcript files unaltered aside from formatting row / column names.\n",
    "\n",
    "The ProteinCoding datasets contain just protein coding genes and are based on protein coding annotations from https://www.genenames.org/cgi-bin/download (locus_group == \"protein-coding gene\")\n",
    "\n",
    "Rows: are DepMap cell line IDs\n",
    "\n",
    "Columns: In the complete TPM and read counts datasets column names are HGNC_symbol (Ensembl_ID), while in the ProteinCoding TPM dataset column names are HGNC_symbol (Entrez_ID). In the TPM transcript dataset column names are HGNC_symbol (Transcript_ID) - the HGNC symbols are not unique, use the transcript IDs for unique identifiers.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CCLE_RNAseq_reads', 'depmap-rnaseq-expression-data-ccd0.20/public_20Q2_counts'), ('CCLE_expression_full', 'depmap-rnaseq-expression-data-ccd0.20/public_20Q2_tpm'), ('CCLE_expression', 'depmap-rnaseq-expression-data-ccd0.20/public_20Q2_proteincoding_tpm'), ('CCLE_RNAseq_transcripts', 'depmap-rnaseq-expression-data-ccd0.20/public_20Q2_transcripts_tpm'), ('CCLE_gene_cn', 'depmap-wes-cn-data-97cc.30/public_20Q2_gene_cn'), ('Achilles_gene_effect_unscaled', 'avana-public-tentative-20q2-0306.5/gene_effect_unscaled'), ('Achilles_replicate_map', 'avana-public-tentative-20q2-0306.4/replicate_map'), ('Achilles_high_variance_genes', 'avana-public-tentative-20q2-0306.4/high_variance_genes'), ('Achilles_guide_map', 'avana-public-tentative-20q2-0306.4/guide_gene_map'), ('CCLE_mutations', 'depmap-mutation-calls-9a1a.17/depmap_20Q2_mutation_calls'), ('Achilles_logfold_change', 'avana-public-tentative-20q2-0306.4/logfold_change'), ('Achilles_gene_effect', 'avana-public-tentative-20q2-0306.5/gene_effect'), ('CCLE_fusions_unfiltered', 'gene-fusions-6212.10/unfiltered_fusions_20Q2'), ('common_essentials', 'avana-public-tentative-20q2-0306.4/essential_genes'), ('Achilles_guide_efficacy', 'avana-public-tentative-20q2-0306.4/guide_efficacy'), ('Achilles_gene_dependency', 'avana-public-tentative-20q2-0306.5/gene_dependency'), ('CCLE_segment_cn', 'depmap-wes-cn-data-97cc.30/public_20Q2_segs_cn'), ('nonessentials', 'avana-public-tentative-20q2-0306.4/nonessential_genes'), ('Achilles_raw_readcounts', 'avana-public-tentative-20q2-0306.4/raw_readcounts'), ('CCLE_fusions', 'gene-fusions-6212.10/filtered_fusions_20Q2'), ('Achilles_dropped_guides', 'avana-public-tentative-20q2-0306.4/dropped_guides'), ('Achilles_common_essentials', 'avana-public-tentative-20q2-0306.4/pan_dependent_genes')]\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datafile/04464d205e5e4e7b8a6f1e2cc510691a\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 06c6b7f444b744adbd0e7092f7385eb8 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/06c6b7f444b744adbd0e7092f7385eb8\n"
     ]
    }
   ],
   "source": [
    "AddToVirtual(virtual_public, \"depmap-rnaseq-expression-data-ccd0\", files=[('CCLE_RNAseq_reads', 'public_'+release+'_counts'),('CCLE_expression_full', 'public_'+release+'_tpm'),('CCLE_expression', 'public_'+release+'_proteincoding_tpm'),('CCLE_RNAseq_transcripts', 'public_'+release+'_transcripts_tpm')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra.waitForSubmission(refworkspace, submission_id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_id2 = refwm.create_submission(\"Aggregate_Fusion_Calls\", 'All_samples')\n",
    "terra.waitForSubmission(refworkspace, submission_id2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = refwm.get_sample_sets().loc['All_samples']['fusions_star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cp $aggregated \"temp/expression.fusion.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('temp/expression.fusion.tsv', header=0, sep='\\t', quotechar='\"', error_bad_lines=False, index_col=None)\n",
    "file[\"DepMap_ID\"] = [i.split('.')[0] for i in file['DepMap_ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(file))\n",
    "renaming = removeOlderVersions(names=set(file['DepMap_ID']), refsamples=refwm.get_samples(), arxspan_id=\"arxspan_id\", version=\"version\")\n",
    "file = file[file['DepMap_ID'].isin(renaming.keys())].replace({'DepMap_ID':renaming}).reset_index(drop=True)\n",
    "print(len(file))\n",
    "print(file['DepMap_ID'][:10])\n",
    "file.to_csv('temp/expression.fusion.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This document contains the code used to generate the unfiltered and filtered versions of the fusion datasets for the release. The bottom of the document also contains some comparisons between the release fusion dataset, CCLE2 fusion calls, and the translocation data from CCLE2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "source('../gkugener/RScripts/load_libraries_and_annotations.R')\n",
    "source(\"src/CCLE_postp_function.R\")\n",
    "library('cdsomics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate filtered fusion table\n",
    "\n",
    "Release: `r release`\n",
    "\n",
    "We want to apply filters to the fusion table to reduce the number of artifacts in the dataset. Specifically, we filter the following:\n",
    "\n",
    "* Remove fusions involving mitochondrial chromosomes, or HLA genes, or immunoglobulin genes\n",
    "* Remove red herring fusions (from STAR-Fusion annotations column)\n",
    "* Remove recurrent in CCLE (>= 25 samples)\n",
    "* Remove fusion with (SpliceType=\" INCL_NON_REF_SPLICE\" and LargeAnchorSupport=\"No\" and FFPM < 0.1)\n",
    "* Remove fusions with FFPM < 0.05 (STAR-Fusion suggests using 0.1, but looking at the translocation data, this looks like it might be too aggressive)\n",
    "\n",
    "with the functions:\n",
    "- readFusions\n",
    "- filterFusions\n",
    "- prepare_depmap_fusion_data_for_taiga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "unfiltered_fusions <- readFusions('temp/expression.fusion.tsv')\n",
    "filtered_fusions <- filterFusions(unfiltered_fusions)\n",
    "\n",
    "filtered_fusions <- prepare_depmap_fusion_data_for_taiga(filtered_fusions)\n",
    "unfiltered_fusions <- prepare_depmap_fusion_data_for_taiga(unfiltered_fusions)\n",
    "\n",
    "filtered_fusions <- filtered_fusions[,2:ncol(filtered_fusions)]\n",
    "unfiltered_fusions <- unfiltered_fusions[,2:ncol(unfiltered_fusions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Save the files (to be uploaded to taiga)\n",
    "write.table(\n",
    "  unfiltered_fusions,\n",
    "  file = paste0('temp/fusions.',release, '.unfiltered'),\n",
    "  sep = '\\t', quote = F, row.names = F\n",
    ")\n",
    "write.table(\n",
    "  filtered_fusions,\n",
    "  file = paste0('temp/fusions.', release, '.filtered'),\n",
    "  sep = '\\t', quote = F, row.names = F\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Protocol:\n",
    "\n",
    "to validate fusions, one should be able to list all cells with known fusions (i.e. elwing sarcoma) and check for each new cell in this set of knownfusioncells, if the fusion is present or not. and validate the fusion quality this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered = pd.read_csv('temp/fusions.'+release+'.unfiltered', sep='\\t',index_col=None)\n",
    "filtered = pd.read_csv('temp/fusions.'+ release+'.filtered',sep='\\t',index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading to Taiga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.update_dataset(dataset_permaname=\"depmap-fusions-7990\",\n",
    "                     upload_file_path_dict={'temp/fusions.'+release+'.filtered': 'TableTSV',\n",
    "                                        'temp/fusions.'+release+'.unfiltered': 'TableTSV'},\n",
    "                 dataset_description=\"\"\"\n",
    "# Fusions\n",
    "\n",
    "filtered and unfiltered fusion files from Broad RNAseq data mapped to hg38\n",
    "\n",
    "PORTAL TEAM SHOULD NOT USE THIS: There are lines here that should not make it even to internal. Must use subsetted dataset instead. These data will not make it on the portal starting 19Q1. With the DMC portal, there is new cell line release prioritization as to which lines can be included, so a new taiga dataset will be created containing CN for the portal.\n",
    "\n",
    "version 1-4: guillaume releases\n",
    "version 5: 19Q3 release\n",
    "version 6:  adding missing samples in Terra merge files\n",
    "\n",
    "## ** version 7 Internal 19Q4****\n",
    "Adding 17 new cell lines, 3 reprioritized cell lines. log2(COPY RATIO+1). \n",
    "Some cells lines have been flagged as:\n",
    "\n",
    "## ** version 8 Internal 20Q1****\n",
    "Adding 6 new lines\n",
    "\n",
    "## ** version 9 Internal 20Q2****\n",
    "Adding 50 new lines\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines, IDs contained in the column DepMap_ID\n",
    "LeftGene and RightGene separated by an ampersand (\"&\").\n",
    "\n",
    "Unfiltered data contains all output fusions, while the filtered data uses the filters suggested by the star fusion docs. These filters are:\n",
    "- FFPM > 0.1 -  a cutoff of 0.1 means&nbsp;at least 1 fusion-supporting RNAseq fragment per 10M total reads\n",
    "- Remove known false positives, such as GTEx recurrent fusions and certain paralogs\n",
    "- Genes that are next to each other\n",
    "- Fusions with mitochondrial breakpoints\n",
    "- Removing fusion involving mitochondrial chromosomes or HLA genes\n",
    "- Removed common false positive fusions (red herring annotations as described in the STAR-Fusion docs)\n",
    "- Recurrent fusions observed in CCLE across cell lines (in 25 or more samples)\n",
    "- Removed fusions where SpliceType='INCL_NON_REF_SPLICE' and LargeAnchorSupport='NO_LDAS' and FFPM < 0.1\n",
    "- FFPM < 0.05\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepMap_ID</th>\n",
       "      <th>#FusionName</th>\n",
       "      <th>JunctionReadCount</th>\n",
       "      <th>SpanningFragCount</th>\n",
       "      <th>SpliceType</th>\n",
       "      <th>LeftGene</th>\n",
       "      <th>LeftBreakpoint</th>\n",
       "      <th>RightGene</th>\n",
       "      <th>RightBreakpoint</th>\n",
       "      <th>LargeAnchorSupport</th>\n",
       "      <th>FFPM</th>\n",
       "      <th>LeftBreakDinuc</th>\n",
       "      <th>LeftBreakEntropy</th>\n",
       "      <th>RightBreakDinuc</th>\n",
       "      <th>RightBreakEntropy</th>\n",
       "      <th>annots</th>\n",
       "      <th>CCLE_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ACH-001113</td>\n",
       "      <td>CACNA2D1--SEMA3E</td>\n",
       "      <td>48</td>\n",
       "      <td>53</td>\n",
       "      <td>ONLY_REF_SPLICE</td>\n",
       "      <td>CACNA2D1 (ENSG00000153956)</td>\n",
       "      <td>chr7:82335135:-</td>\n",
       "      <td>SEMA3E (ENSG00000170381)</td>\n",
       "      <td>chr7:83469302:-</td>\n",
       "      <td>YES_LDAS</td>\n",
       "      <td>1.5703</td>\n",
       "      <td>GT</td>\n",
       "      <td>1.9899</td>\n",
       "      <td>AG</td>\n",
       "      <td>1.9899</td>\n",
       "      <td>[\"CCLE_StarF2019\",\"INTRACHROMOSOMAL[chr7:0.92M...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ACH-001113</td>\n",
       "      <td>ADNP2--PLIN3</td>\n",
       "      <td>50</td>\n",
       "      <td>40</td>\n",
       "      <td>INCL_NON_REF_SPLICE</td>\n",
       "      <td>ADNP2 (ENSG00000101544)</td>\n",
       "      <td>chr18:80117650:+</td>\n",
       "      <td>PLIN3 (ENSG00000105355)</td>\n",
       "      <td>chr19:4867657:-</td>\n",
       "      <td>YES_LDAS</td>\n",
       "      <td>1.3993</td>\n",
       "      <td>GT</td>\n",
       "      <td>1.7819</td>\n",
       "      <td>AG</td>\n",
       "      <td>1.8323</td>\n",
       "      <td>[\"CCLE_StarF2019\",\"INTERCHROMOSOMAL[chr18--chr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ACH-001113</td>\n",
       "      <td>XRCC4--SUGCT</td>\n",
       "      <td>23</td>\n",
       "      <td>104</td>\n",
       "      <td>ONLY_REF_SPLICE</td>\n",
       "      <td>XRCC4 (ENSG00000152422)</td>\n",
       "      <td>chr5:83111203:+</td>\n",
       "      <td>SUGCT (ENSG00000175600)</td>\n",
       "      <td>chr7:40749434:+</td>\n",
       "      <td>YES_LDAS</td>\n",
       "      <td>1.9745</td>\n",
       "      <td>GT</td>\n",
       "      <td>1.9656</td>\n",
       "      <td>AG</td>\n",
       "      <td>1.9656</td>\n",
       "      <td>[\"CCLE_StarF2019\",\"INTERCHROMOSOMAL[chr5--chr7]\"]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ACH-001113</td>\n",
       "      <td>PCYT1A--RBFOX1</td>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>ONLY_REF_SPLICE</td>\n",
       "      <td>PCYT1A (ENSG00000161217)</td>\n",
       "      <td>chr3:196247367:-</td>\n",
       "      <td>RBFOX1 (ENSG00000078328)</td>\n",
       "      <td>chr16:6654603:+</td>\n",
       "      <td>YES_LDAS</td>\n",
       "      <td>1.1505</td>\n",
       "      <td>GT</td>\n",
       "      <td>1.7819</td>\n",
       "      <td>AG</td>\n",
       "      <td>1.8295</td>\n",
       "      <td>[\"CCLE_StarF2019\",\"INTERCHROMOSOMAL[chr3--chr1...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ACH-001113</td>\n",
       "      <td>AF117829.1--PIP4P2</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>ONLY_REF_SPLICE</td>\n",
       "      <td>AF117829.1 (ENSG00000251136)</td>\n",
       "      <td>chr8:89757045:-</td>\n",
       "      <td>PIP4P2 (ENSG00000155099)</td>\n",
       "      <td>chr8:90996744:-</td>\n",
       "      <td>YES_LDAS</td>\n",
       "      <td>0.8707</td>\n",
       "      <td>GT</td>\n",
       "      <td>1.8062</td>\n",
       "      <td>AG</td>\n",
       "      <td>1.9329</td>\n",
       "      <td>[\"INTRACHROMOSOMAL[chr8:1.24Mb]\"]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33904</td>\n",
       "      <td>ACH-000052</td>\n",
       "      <td>SMURF2P1--SP2</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>ONLY_REF_SPLICE</td>\n",
       "      <td>SMURF2P1 (ENSG00000248121)</td>\n",
       "      <td>chr17:30610942:+</td>\n",
       "      <td>SP2 (ENSG00000167182)</td>\n",
       "      <td>chr17:47914726:+</td>\n",
       "      <td>YES_LDAS</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>GT</td>\n",
       "      <td>1.7819</td>\n",
       "      <td>AG</td>\n",
       "      <td>1.7819</td>\n",
       "      <td>[\"CCLE_StarF2019\",\"INTRACHROMOSOMAL[chr17:17.2...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33905</td>\n",
       "      <td>ACH-000052</td>\n",
       "      <td>AP002990.1--UBXN1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>ONLY_REF_SPLICE</td>\n",
       "      <td>AP002990.1 (ENSG00000255508)</td>\n",
       "      <td>chr11:62572584:-</td>\n",
       "      <td>UBXN1 (ENSG00000162191)</td>\n",
       "      <td>chr11:62677005:-</td>\n",
       "      <td>YES_LDAS</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>GT</td>\n",
       "      <td>1.9329</td>\n",
       "      <td>AG</td>\n",
       "      <td>1.9656</td>\n",
       "      <td>[\"INTRACHROMOSOMAL[chr11:0.08Mb]\",\"LOCAL_REARR...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33906</td>\n",
       "      <td>ACH-000052</td>\n",
       "      <td>EEF1G--UBXN1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>ONLY_REF_SPLICE</td>\n",
       "      <td>EEF1G (ENSG00000254772)</td>\n",
       "      <td>chr11:62572584:-</td>\n",
       "      <td>UBXN1 (ENSG00000162191)</td>\n",
       "      <td>chr11:62677005:-</td>\n",
       "      <td>YES_LDAS</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>GT</td>\n",
       "      <td>1.9329</td>\n",
       "      <td>AG</td>\n",
       "      <td>1.9656</td>\n",
       "      <td>[\"Klijn_CellLines\",\"INTRACHROMOSOMAL[chr11:0.1...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33907</td>\n",
       "      <td>ACH-000052</td>\n",
       "      <td>AC093484.3--FN1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>INCL_NON_REF_SPLICE</td>\n",
       "      <td>AC093484.3 (ENSG00000265401)</td>\n",
       "      <td>chr17:16382422:-</td>\n",
       "      <td>FN1 (ENSG00000115414)</td>\n",
       "      <td>chr2:215391753:-</td>\n",
       "      <td>YES_LDAS</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>TG</td>\n",
       "      <td>1.8892</td>\n",
       "      <td>AC</td>\n",
       "      <td>1.5546</td>\n",
       "      <td>[\"INTERCHROMOSOMAL[chr17--chr2]\"]</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33908</td>\n",
       "      <td>ACH-000052</td>\n",
       "      <td>MIR6723--PKM</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>INCL_NON_REF_SPLICE</td>\n",
       "      <td>MIR6723 (ENSG00000278791)</td>\n",
       "      <td>chr1:632197:-</td>\n",
       "      <td>PKM (ENSG00000067225)</td>\n",
       "      <td>chr15:72209724:-</td>\n",
       "      <td>YES_LDAS</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>TA</td>\n",
       "      <td>1.7465</td>\n",
       "      <td>GC</td>\n",
       "      <td>1.9656</td>\n",
       "      <td>[\"CCLE_StarF2019\",\"INTERCHROMOSOMAL[chr1--chr1...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33909 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        DepMap_ID         #FusionName  JunctionReadCount  SpanningFragCount  \\\n",
       "0      ACH-001113    CACNA2D1--SEMA3E                 48                 53   \n",
       "1      ACH-001113        ADNP2--PLIN3                 50                 40   \n",
       "2      ACH-001113        XRCC4--SUGCT                 23                104   \n",
       "3      ACH-001113      PCYT1A--RBFOX1                 26                 48   \n",
       "4      ACH-001113  AF117829.1--PIP4P2                 25                 31   \n",
       "...           ...                 ...                ...                ...   \n",
       "33904  ACH-000052       SMURF2P1--SP2                  5                 21   \n",
       "33905  ACH-000052   AP002990.1--UBXN1                  4                  6   \n",
       "33906  ACH-000052        EEF1G--UBXN1                  4                  6   \n",
       "33907  ACH-000052     AC093484.3--FN1                  4                  3   \n",
       "33908  ACH-000052        MIR6723--PKM                  3                  5   \n",
       "\n",
       "                SpliceType                      LeftGene    LeftBreakpoint  \\\n",
       "0          ONLY_REF_SPLICE    CACNA2D1 (ENSG00000153956)   chr7:82335135:-   \n",
       "1      INCL_NON_REF_SPLICE       ADNP2 (ENSG00000101544)  chr18:80117650:+   \n",
       "2          ONLY_REF_SPLICE       XRCC4 (ENSG00000152422)   chr5:83111203:+   \n",
       "3          ONLY_REF_SPLICE      PCYT1A (ENSG00000161217)  chr3:196247367:-   \n",
       "4          ONLY_REF_SPLICE  AF117829.1 (ENSG00000251136)   chr8:89757045:-   \n",
       "...                    ...                           ...               ...   \n",
       "33904      ONLY_REF_SPLICE    SMURF2P1 (ENSG00000248121)  chr17:30610942:+   \n",
       "33905      ONLY_REF_SPLICE  AP002990.1 (ENSG00000255508)  chr11:62572584:-   \n",
       "33906      ONLY_REF_SPLICE       EEF1G (ENSG00000254772)  chr11:62572584:-   \n",
       "33907  INCL_NON_REF_SPLICE  AC093484.3 (ENSG00000265401)  chr17:16382422:-   \n",
       "33908  INCL_NON_REF_SPLICE     MIR6723 (ENSG00000278791)     chr1:632197:-   \n",
       "\n",
       "                      RightGene   RightBreakpoint LargeAnchorSupport    FFPM  \\\n",
       "0      SEMA3E (ENSG00000170381)   chr7:83469302:-           YES_LDAS  1.5703   \n",
       "1       PLIN3 (ENSG00000105355)   chr19:4867657:-           YES_LDAS  1.3993   \n",
       "2       SUGCT (ENSG00000175600)   chr7:40749434:+           YES_LDAS  1.9745   \n",
       "3      RBFOX1 (ENSG00000078328)   chr16:6654603:+           YES_LDAS  1.1505   \n",
       "4      PIP4P2 (ENSG00000155099)   chr8:90996744:-           YES_LDAS  0.8707   \n",
       "...                         ...               ...                ...     ...   \n",
       "33904     SP2 (ENSG00000167182)  chr17:47914726:+           YES_LDAS  0.2356   \n",
       "33905   UBXN1 (ENSG00000162191)  chr11:62677005:-           YES_LDAS  0.0906   \n",
       "33906   UBXN1 (ENSG00000162191)  chr11:62677005:-           YES_LDAS  0.0906   \n",
       "33907     FN1 (ENSG00000115414)  chr2:215391753:-           YES_LDAS  0.0634   \n",
       "33908     PKM (ENSG00000067225)  chr15:72209724:-           YES_LDAS  0.0725   \n",
       "\n",
       "      LeftBreakDinuc  LeftBreakEntropy RightBreakDinuc  RightBreakEntropy  \\\n",
       "0                 GT            1.9899              AG             1.9899   \n",
       "1                 GT            1.7819              AG             1.8323   \n",
       "2                 GT            1.9656              AG             1.9656   \n",
       "3                 GT            1.7819              AG             1.8295   \n",
       "4                 GT            1.8062              AG             1.9329   \n",
       "...              ...               ...             ...                ...   \n",
       "33904             GT            1.7819              AG             1.7819   \n",
       "33905             GT            1.9329              AG             1.9656   \n",
       "33906             GT            1.9329              AG             1.9656   \n",
       "33907             TG            1.8892              AC             1.5546   \n",
       "33908             TA            1.7465              GC             1.9656   \n",
       "\n",
       "                                                  annots  CCLE_count  \n",
       "0      [\"CCLE_StarF2019\",\"INTRACHROMOSOMAL[chr7:0.92M...           2  \n",
       "1      [\"CCLE_StarF2019\",\"INTERCHROMOSOMAL[chr18--chr...           2  \n",
       "2      [\"CCLE_StarF2019\",\"INTERCHROMOSOMAL[chr5--chr7]\"]           2  \n",
       "3      [\"CCLE_StarF2019\",\"INTERCHROMOSOMAL[chr3--chr1...           4  \n",
       "4                      [\"INTRACHROMOSOMAL[chr8:1.24Mb]\"]           2  \n",
       "...                                                  ...         ...  \n",
       "33904  [\"CCLE_StarF2019\",\"INTRACHROMOSOMAL[chr17:17.2...           2  \n",
       "33905  [\"INTRACHROMOSOMAL[chr11:0.08Mb]\",\"LOCAL_REARR...           2  \n",
       "33906  [\"Klijn_CellLines\",\"INTRACHROMOSOMAL[chr11:0.1...           2  \n",
       "33907                  [\"INTERCHROMOSOMAL[chr17--chr2]\"]          14  \n",
       "33908  [\"CCLE_StarF2019\",\"INTERCHROMOSOMAL[chr1--chr1...          12  \n",
       "\n",
       "[33909 rows x 17 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = pd.read_csv('temp/filtered_fusions_'+release, sep='\\t')\n",
    "unfiltered = pd.read_csv('temp/unfiltered_fusions_'+release, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## removing first blacklisted, then embargoed, to create two datasets\n",
    "print(len(filtered))\n",
    "filtered = filtered[~filtered.DepMap_ID.isin(blacklist)]\n",
    "print(len(filtered))\n",
    "filtered.to_csv('temp/filtered_fusions_'+release, sep='\\t', index=False)\n",
    "print(len(unfiltered))\n",
    "unfiltered= unfiltered[~unfiltered.DepMap_ID.isin(blacklist)]\n",
    "print(len(unfiltered))\n",
    "unfiltered.to_csv('temp/unfiltered_fusions_'+release, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.update_dataset(dataset_permaname=\"gene-fusions-8b7a\",\n",
    "                 upload_file_path_dict={'temp/filtered_fusions_'+release: 'TableTSV',\n",
    "                                       'temp/unfiltered_fusions_'+release: 'TableTSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# Internal Fusions\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines\n",
    "\n",
    "Original Raw Data: Generated by Mahmoud Ghandi on April 25, 2017. Can be found at xchip_ccle_dist/broad_only/unpublished_Novartis_data/RNAseq/fusions.txt\n",
    "\n",
    "Version 3: added a column containing the Broad_ID\n",
    "\n",
    "* Version 4-5 Internal 19Q1*\n",
    "\n",
    "version 5 contains the correct data for 19Q1\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines\n",
    "\n",
    "* Version 6 Internal 19Q2*\n",
    "\n",
    "* Version 7*\n",
    "\n",
    "Josh D added \"common_fusion_matrix\".\n",
    "\n",
    "Binary matrix of the most common gene fusions (those where the two involved genes are fused in at least 5 cell lines) with no additional filtering. Use at your own risk.\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines\n",
    "\n",
    "* Version 8 Internal 19Q3*\n",
    "\n",
    "* Version 8 Internal 19Q4*\n",
    "\n",
    "* Version 8 Internal 20Q1*\n",
    "\n",
    "* Version 8 Internal 20Q2*\n",
    "Adding 50 new lines\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines, IDs contained in the column DepMap_ID\n",
    "LeftGene and RightGene separated by an ampersand (\"&\").\n",
    "\n",
    "Unfiltered data contains all output fusions, while the filtered data uses the filters suggested by the star fusion docs. These filters are:\n",
    "- FFPM > 0.1 -  a cutoff of 0.1 means&nbsp;at least 1 fusion-supporting RNAseq fragment per 10M total reads\n",
    "- Remove known false positives, such as GTEx recurrent fusions and certain paralogs\n",
    "- Genes that are next to each other\n",
    "- Fusions with mitochondrial breakpoints\n",
    "- Removing fusion involving mitochondrial chromosomes or HLA genes\n",
    "- Removed common false positive fusions (red herring annotations as described in the STAR-Fusion docs)\n",
    "- Recurrent fusions observed in CCLE across cell lines (in 25 or more samples)\n",
    "- Removed fusions where SpliceType='INCL_NON_REF_SPLICE' and LargeAnchorSupport='NO_LDAS' and FFPM < 0.1\n",
    "- FFPM < 0.05\n",
    "\n",
    "\"\"\")\n",
    "AddToVirtual('depmap-a0ab', \"gene-fusions-8b7a\", files=[('CCLE_fusions_unfiltered', 'unfiltered_fusions_'+release),('CCLE_fusions', 'filtered_fusions_'+release)])\n",
    "\n",
    "AddToVirtual(virtual_internal, \"gene-fusions-8b7a\", files=[('CCLE_fusions_unfiltered', 'unfiltered_fusions_'+release),('CCLE_fusions', 'filtered_fusions_'+release)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM\n",
    "\n",
    "same as internal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33909\n",
      "344093\n"
     ]
    }
   ],
   "source": [
    "## removing first blacklisted, then embargoed, to create two datasets\n",
    "print(len(filtered))\n",
    "filtered = filtered[~filtered.DepMap_ID.isin(rna_ibm_embargo)]\n",
    "print(len(filtered))\n",
    "filtered.to_csv('temp/filtered_fusions_'+release, sep='\\t',index=False)\n",
    "\n",
    "print(len(unfiltered))\n",
    "unfiltered  = unfiltered[~unfiltered.DepMap_ID.isin(rna_ibm_embargo)]\n",
    "unfiltered.to_csv('temp/unfiltered_fusions_'+release, sep='\\t',index=False)\n",
    "print(len(unfiltered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading filtered_fusions_20Q2...\n",
      "hitting https://cds.team/taiga/api/datafile/a2e1e4cd39fa42898d5872a19d5f4a43\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: filtered_fusions_20Q2 properly converted and uploaded\n",
      "Uploading unfiltered_fusions_20Q2...\n",
      "hitting https://cds.team/taiga/api/datafile/a2e1e4cd39fa42898d5872a19d5f4a43\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: unfiltered_fusions_20Q2 properly converted and uploaded\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 1f832888f6af4b7795788524a4be6743 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/1f832888f6af4b7795788524a4be6743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1f832888f6af4b7795788524a4be6743'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.update_dataset(dataset_permaname=\"gene-fusions-375f\",\n",
    "                 upload_file_path_dict={'temp/filtered_fusions_'+release: 'TableTSV',\n",
    "                                       'temp/unfiltered_fusions_'+release: 'TableTSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# DMC Fusions\n",
    "\n",
    "* Version 1-2 DMC 19Q1*\n",
    "\n",
    "version 2 contains the correct data for 19Q1\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines\n",
    "\n",
    "Unfiltered data contains all output fusions, while the filtered data uses the filters suggested by the star fusion docs. These filters are:\n",
    "- FFPM > 0.1 -  a cutoff of 0.1 means&nbsp;at least 1 fusion-supporting RNAseq fragment per 10M total reads\n",
    "- Remove known false positives, such as GTEx recurrent fusions and certain paralogs\n",
    "- Genes that are next to each other\n",
    "- Fusions with mitochondrial breakpoints\n",
    "\n",
    "* Version 3 DMC 19Q2*\n",
    "\n",
    "* Version 4 DMC 19Q3*\n",
    "\n",
    "* Version 5 DMC 19Q4*\n",
    "\n",
    "* Version 6 DMC 20Q1*\n",
    "\n",
    "* Version 7 DMC 20Q2*\n",
    "adding 50 new lines\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines, IDs contained in the column DepMap_ID\n",
    "LeftGene and RightGene separated by an ampersand (\"&\").\n",
    "\n",
    "Unfiltered data contains all output fusions, while the filtered data uses the filters suggested by the star fusion docs. These filters are:\n",
    "- FFPM > 0.1 -  a cutoff of 0.1 means&nbsp;at least 1 fusion-supporting RNAseq fragment per 10M total reads\n",
    "- Remove known false positives, such as GTEx recurrent fusions and certain paralogs\n",
    "- Genes that are next to each other\n",
    "- Fusions with mitochondrial breakpoints\n",
    "- Removing fusion involving mitochondrial chromosomes or HLA genes\n",
    "- Removed common false positive fusions (red herring annotations as described in the STAR-Fusion docs)\n",
    "- Recurrent fusions observed in CCLE across cell lines (in 25 or more samples)\n",
    "- Removed fusions where SpliceType='INCL_NON_REF_SPLICE' and LargeAnchorSupport='NO_LDAS' and FFPM < 0.1\n",
    "- FFPM < 0.05\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CCLE_fusions', 'gene-fusions-375f.8/filtered_fusions_20Q2'), ('CCLE_fusions_unfiltered', 'gene-fusions-375f.8/unfiltered_fusions_20Q2'), ('README', 'dmc-20q2-2db6.13/README.txt'), ('Achilles_guide_efficacy', 'avana-consortium-20q2-d98a.1/guide_efficacy'), ('common_essentials', 'avana-consortium-20q2-d98a.1/essential_genes'), ('CCLE_expression_full', 'depmap-rnaseq-expression-data-80ef.13/dmc_20Q2_tpm'), ('CCLE_gene_cn', 'depmap-cn-data-9b9d.12/dmc_20Q2_gene_cn'), ('Achilles_logfold_change_failures', 'avana-consortium-20q2-d98a.1/logfold_change_failures'), ('Achilles_replicate_map', 'avana-consortium-20q2-d98a.1/replicate_map'), ('Achilles_raw_readcounts', 'avana-consortium-20q2-d98a.1/raw_readcounts'), ('CCLE_expression', 'depmap-rnaseq-expression-data-80ef.13/dmc_20Q2_proteincoding_tpm'), ('Achilles_guide_map', 'avana-consortium-20q2-d98a.1/guide_gene_map'), ('Achilles_logfold_change', 'avana-consortium-20q2-d98a.1/logfold_change'), ('Achilles_high_variance_genes', 'avana-consortium-20q2-d98a.1/high_variance_genes'), ('Achilles_gene_dependency', 'avana-consortium-20q2-d98a.1/gene_dependency'), ('CCLE_RNAseq_reads', 'depmap-rnaseq-expression-data-80ef.13/dmc_20Q2_counts'), ('Achilles_raw_readcounts_failures', 'avana-consortium-20q2-d98a.1/raw_readcounts_failing'), ('Achilles_common_essentials', 'avana-consortium-20q2-d98a.1/pan_dependent_genes'), ('CCLE_segmented_cn', 'depmap-cn-data-9b9d.12/dmc_20Q2_segs_cn'), ('CCLE_RNAseq_transcripts', 'depmap-rnaseq-expression-data-80ef.13/dmc_20Q2_transcripts_tpm'), ('CCLE_mutations', 'depmap-mutation-calls-dfce.14/depmap_20Q2_mutation_calls'), ('Achilles_gene_effect', 'avana-consortium-20q2-d98a.1/gene_effect'), ('Achilles_dropped_guides', 'avana-consortium-20q2-d98a.1/dropped_guides'), ('sample_info', 'dmc-20q2-2db6.11/sample_info'), ('Achilles_gene_effect_unscaled', 'avana-consortium-20q2-d98a.1/gene_effect_unscaled'), ('nonessentials', 'avana-consortium-20q2-d98a.1/nonessential_genes')]\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datafile/8eca8402ee3c464ba4de1edd023af04e\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 65636bb3e87143bf8a6bfc7ce48f4a94 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/65636bb3e87143bf8a6bfc7ce48f4a94\n"
     ]
    }
   ],
   "source": [
    "AddToVirtual(virtual_dmc, \"gene-fusions-375f\", files=[('CCLE_fusions', 'filtered_fusions_'+release),('CCLE_fusions_unfiltered', 'unfiltered_fusions_'+release)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33587\n",
      "33587\n",
      "344093\n",
      "344093\n"
     ]
    }
   ],
   "source": [
    "## removing first blacklisted, then embargoed, to create two datasets\n",
    "print(len(filtered))\n",
    "filtered = filtered[filtered.DepMap_ID.isin(prevprevcells)]\n",
    "print(len(filtered))\n",
    "filtered[~filtered.DepMap_ID.isin(rna_dmc_embargo)].to_csv('temp/filtered_fusions_'+release, sep='\\t', index=False)\n",
    "\n",
    "print(len(unfiltered))\n",
    "unfiltered = unfiltered[unfiltered.DepMap_ID.isin(prevprevcells)]\n",
    "print(len(unfiltered))\n",
    "unfiltered[~unfiltered.DepMap_ID.isin(rna_dmc_embargo)].to_csv('temp/unfiltered_fusions_'+release, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading filtered_fusions_20Q2...\n",
      "hitting https://cds.team/taiga/api/datafile/ce8f5a392adc4c3abb3ef43031216f0a\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: filtered_fusions_20Q2 properly converted and uploaded\n",
      "Uploading unfiltered_fusions_20Q2...\n",
      "hitting https://cds.team/taiga/api/datafile/ce8f5a392adc4c3abb3ef43031216f0a\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: unfiltered_fusions_20Q2 properly converted and uploaded\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 7ecf0928cf7c48ed9f9928c5e262395b created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/7ecf0928cf7c48ed9f9928c5e262395b\n",
      "[('CCLE_fusions', 'gene-fusions-6212.11/filtered_fusions_20Q2'), ('CCLE_fusions_unfiltered', 'gene-fusions-6212.11/unfiltered_fusions_20Q2'), ('CCLE_segment_cn', 'depmap-wes-cn-data-97cc.30/public_20Q2_segs_cn'), ('Achilles_logfold_change', 'avana-public-tentative-20q2-0306.4/logfold_change'), ('CCLE_mutations', 'depmap-mutation-calls-9a1a.17/depmap_20Q2_mutation_calls'), ('CCLE_expression', 'depmap-rnaseq-expression-data-ccd0.20/public_20Q2_proteincoding_tpm'), ('common_essentials', 'avana-public-tentative-20q2-0306.4/essential_genes'), ('Achilles_guide_efficacy', 'avana-public-tentative-20q2-0306.4/guide_efficacy'), ('Achilles_replicate_map', 'avana-public-tentative-20q2-0306.4/replicate_map'), ('Achilles_raw_readcounts', 'avana-public-tentative-20q2-0306.4/raw_readcounts'), ('Achilles_dropped_guides', 'avana-public-tentative-20q2-0306.4/dropped_guides'), ('Achilles_high_variance_genes', 'avana-public-tentative-20q2-0306.4/high_variance_genes'), ('Achilles_gene_effect_unscaled', 'avana-public-tentative-20q2-0306.5/gene_effect_unscaled'), ('CCLE_RNAseq_reads', 'depmap-rnaseq-expression-data-ccd0.20/public_20Q2_counts'), ('Achilles_gene_effect', 'avana-public-tentative-20q2-0306.5/gene_effect'), ('CCLE_RNAseq_transcripts', 'depmap-rnaseq-expression-data-ccd0.20/public_20Q2_transcripts_tpm'), ('Achilles_common_essentials', 'avana-public-tentative-20q2-0306.4/pan_dependent_genes'), ('Achilles_gene_dependency', 'avana-public-tentative-20q2-0306.5/gene_dependency'), ('CCLE_expression_full', 'depmap-rnaseq-expression-data-ccd0.20/public_20Q2_tpm'), ('nonessentials', 'avana-public-tentative-20q2-0306.4/nonessential_genes'), ('Achilles_guide_map', 'avana-public-tentative-20q2-0306.4/guide_gene_map'), ('CCLE_gene_cn', 'depmap-wes-cn-data-97cc.30/public_20Q2_gene_cn')]\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datafile/a080be5b5c9c491484731683d3943beb\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 398fbc5bf9c0413cacf2a80b42758e31 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/398fbc5bf9c0413cacf2a80b42758e31\n"
     ]
    }
   ],
   "source": [
    "tc.update_dataset(dataset_permaname=\"gene-fusions-6212\",\n",
    "                 upload_file_path_dict={'temp/filtered_fusions_'+release: 'TableTSV',\n",
    "                                       'temp/unfiltered_fusions_'+release: 'TableTSV'},\n",
    "                  dataset_description=\n",
    "\"\"\"\n",
    "# PUBLIC Fusions\n",
    "\n",
    "* Version 1 Public 2017 data*\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines, ID contained in the column Broad_ID\n",
    "\n",
    "Original Raw Data: Generated by Mahmoud Ghandi on April 25, 2017. Can be found at xchip_ccle_dist/broad_only/unpublished_Novartis_data/RNAseq/fusions.txt\n",
    "\n",
    "* Version 2-3 Public 19Q1*\n",
    "\n",
    "version 3 contains the correct data for 19Q1\n",
    "\n",
    "* Version 4-5 Public 19Q2*\n",
    "\n",
    "in version 5 formatting of the columns is improved\n",
    "\n",
    "* Version 6 Public 19Q3*\n",
    "\n",
    "* Version 7 Public 19Q4*\n",
    "\n",
    "* Version 8 Public 20Q1*\n",
    "\n",
    "* Version 9 Public 20Q2*\n",
    "adding 50 new lines\n",
    "\n",
    "Description: Gene fusions derived from RNAseq data.\n",
    "\n",
    "Rows: cell lines, IDs contained in the column DepMap_ID\n",
    "LeftGene and RightGene separated by an ampersand (\"&\").\n",
    "\n",
    "Unfiltered data contains all output fusions, while the filtered data uses the filters suggested by the star fusion docs. These filters are:\n",
    "- FFPM > 0.1 -  a cutoff of 0.1 means&nbsp;at least 1 fusion-supporting RNAseq fragment per 10M total reads\n",
    "- Remove known false positives, such as GTEx recurrent fusions and certain paralogs\n",
    "- Genes that are next to each other\n",
    "- Fusions with mitochondrial breakpoints\n",
    "- Removing fusion involving mitochondrial chromosomes or HLA genes\n",
    "- Removed common false positive fusions (red herring annotations as described in the STAR-Fusion docs)\n",
    "- Recurrent fusions observed in CCLE across cell lines (in 25 or more samples)\n",
    "- Removed fusions where SpliceType='INCL_NON_REF_SPLICE' and LargeAnchorSupport='NO_LDAS' and FFPM < 0.1\n",
    "- FFPM < 0.05\n",
    "\n",
    "\"\"\")\n",
    "AddToVirtual(virtual_public, \"gene-fusions-6212\", files=[('CCLE_fusions', 'filtered_fusions_'+release),('CCLE_fusions_unfiltered', 'unfiltered_fusions_'+release)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [OPTIONAL] IF want to merge here instead of on Terra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevsamplesets = ['CCLE_19Q3interim',samplesetname]\n",
    "samples = []\n",
    "for i in prevsamplesets:\n",
    "    samples.extend(refwm.get_sample_sets().loc[i].samples)\n",
    "res = []\n",
    "terrasamp = refwm.get_samples()\n",
    "for i, sample in enumerate(samples):\n",
    "    res.append(terrasamp.loc[sample])\n",
    "    genes_fusion = res[i]['fusion_predictions_abridged']\n",
    "    rsem_genes_transcripts = res[i]['rsem_isoforms']\n",
    "    rsem_genes_expected_count = res[i]['rsem_genes']\n",
    "    ! gsutil cp $rsem_genes_expected_count 'temp/' && gsutil cp $rsem_genes_transcripts 'temp/' && gsutil cp $genes_fusion 'temp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainres = refwm.get_sample_sets().loc['DM19Q2_PATHS_CORRECTED_V2']\n",
    "maingenes_fusion = mainres['fusions_star']\n",
    "mainrsem_genes_tpm = mainres['rsem_genes_tpm']\n",
    "mainrsem_genes_transcripts = mainres['rsem_transcripts_tpm']\n",
    "mainrsem_genes_expected_count = mainres['rsem_genes_expected_count']\n",
    "! gsutil cp $mainrsem_genes_expected_count \"temp/\" && gsutil cp $mainrsem_genes_transcripts \"temp/\" && gsutil cp $maingenes_fusion \"temp/expression.fusion.tsv\" && gsutil cp $mainrsem_genes_tpm \"temp/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainres['rsem_genes_expected_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addSamplesRSEMToMain(res,mainres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_fusion = ['temp/'+val['fusion_predictions_abridged'].split('/')[-1] for val in res]\n",
    "addToMainFusion(genes_fusion,'temp/expression.fusion.tsv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
