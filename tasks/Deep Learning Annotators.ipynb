{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow tensorflow_hub kipoi_interpret kipoiseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfaidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from kipoi_interpret.importance_scores.ism import Mutation\n",
    "from kipoiseq import Interval\n",
    "import pandas as pd\n",
    "import pyfaidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ~/Homo_sapiens_assembly38_ERCC92.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta = pyfaidx.Fasta(\"../../Homo_sapiens_assembly38_ERCC92.fasta\")\n",
    "chromosome_sizes = {k: len(v) for k, v in fasta.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the kipoi model\n",
    "enformer_model = hub.load(\"https://tfhub.dev/deepmind/enformer/1\").model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastaStringExtractor:\n",
    "    \n",
    "    def __init__(self, fasta_file):\n",
    "        self.fasta = pyfaidx.Fasta(fasta_file)\n",
    "        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n",
    "\n",
    "    def extract(self, interval: Interval, **kwargs) -> str:\n",
    "        # Truncate interval if it extends beyond the chromosome lengths.\n",
    "        chromosome_length = self._chromosome_sizes[interval.chrom]\n",
    "        trimmed_interval = Interval(interval.chrom,\n",
    "                                    max(interval.start, 0),\n",
    "                                    min(interval.end, chromosome_length),\n",
    "                                    )\n",
    "        # pyfaidx wants a 1-based interval\n",
    "        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n",
    "                                          trimmed_interval.start + 1,\n",
    "                                          trimmed_interval.stop).seq).upper()\n",
    "        # Fill truncated values with N's.\n",
    "        pad_upstream = 'N' * max(-interval.start, 0)\n",
    "        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n",
    "        return pad_upstream + sequence + pad_downstream\n",
    "\n",
    "    def close(self):\n",
    "        return self.fasta.close()\n",
    "\n",
    "\n",
    "def variant_generator(vcf_file, gzipped=False):\n",
    "    \"\"\"Yields a kipoiseq.dataclasses.Variant for each row in VCF file.\"\"\"\n",
    "    def _open(file):\n",
    "        return gzip.open(vcf_file, 'rt') if gzipped else open(vcf_file)\n",
    "    \n",
    "    with _open(vcf_file) as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            chrom, pos, id, ref, alt_list = line.split('\\t')[:5]\n",
    "            # Split ALT alleles and return individual variants as output.\n",
    "            for alt in alt_list.split(','):\n",
    "                yield kipoiseq.dataclasses.Variant(chrom=chrom, pos=pos,\n",
    "                                            ref=ref, alt=alt, id=id)\n",
    "\n",
    "\n",
    "def one_hot_encode(sequence):\n",
    "    return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n",
    "\n",
    "\n",
    "def variant_centered_sequences(vcf_file, sequence_length, gzipped=False,\n",
    "                               chr_prefix=''):\n",
    "    seq_extractor = kipoiseq.extractors.VariantSeqExtractor(\n",
    "      reference_sequence=FastaStringExtractor(fasta_file))\n",
    "\n",
    "    for variant in variant_generator(vcf_file, gzipped=gzipped):\n",
    "        interval = Interval(chr_prefix + variant.chrom,\n",
    "                            variant.pos, variant.pos)\n",
    "        interval = interval.resize(sequence_length)\n",
    "        center = interval.center() - interval.start\n",
    "\n",
    "        reference = seq_extractor.extract(interval, [], anchor=center)\n",
    "        alternate = seq_extractor.extract(interval, [variant], anchor=center)\n",
    "\n",
    "        yield {'inputs': {'ref': one_hot_encode(reference),\n",
    "                          'alt': one_hot_encode(alternate)},\n",
    "              'metadata': {'chrom': chr_prefix + variant.chrom,\n",
    "                            'pos': variant.pos,\n",
    "                            'id': variant.id,\n",
    "                            'ref': variant.ref,\n",
    "                            'alt': variant.alt}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enformer:\n",
    "    def __init__(self, tfhub_url):\n",
    "        self._model = hub.load(tfhub_url).model\n",
    "\n",
    "    def predict_on_batch(self, inputs):\n",
    "        predictions = self._model.predict_on_batch(inputs)\n",
    "        return {k: v.numpy() for k, v in predictions.items()}\n",
    "\n",
    "    @tf.function\n",
    "    def contribution_input_grad(self, input_sequence, target_mask, output_head=\"human\"):\n",
    "        input_sequence = input_sequence[tf.newaxis]\n",
    "\n",
    "        target_mask_mass = tf.reduce_sum(target_mask)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_sequence)\n",
    "            prediction = (\n",
    "                tf.reduce_sum(\n",
    "                    target_mask[tf.newaxis]\n",
    "                    * self._model.predict_on_batch(input_sequence)[output_head]\n",
    "                )\n",
    "                / target_mask_mass\n",
    "            )\n",
    "\n",
    "        input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n",
    "        input_grad = tf.squeeze(input_grad, axis=0)\n",
    "        return tf.reduce_sum(input_grad, axis=-1)\n",
    "\n",
    "\n",
    "class EnformerScoreVariantsRaw:\n",
    "    def __init__(self, tfhub_url, organism=\"human\"):\n",
    "        self._model = Enformer(tfhub_url)\n",
    "        self._organism = organism\n",
    "\n",
    "    def predict_on_batch(self, inputs):\n",
    "        ref_prediction = self._model.predict_on_batch(inputs[\"ref\"])[self._organism]\n",
    "        alt_prediction = self._model.predict_on_batch(inputs[\"alt\"])[self._organism]\n",
    "\n",
    "        return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)\n",
    "\n",
    "\n",
    "class EnformerScoreVariantsNormalized:\n",
    "    def __init__(self, tfhub_url, transform_pkl_path, organism=\"human\"):\n",
    "        assert organism == \"human\", \"Transforms only compatible with organism=human\"\n",
    "        self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n",
    "        with tf.io.gfile.GFile(transform_pkl_path, \"rb\") as f:\n",
    "            transform_pipeline = joblib.load(f)\n",
    "        self._transform = transform_pipeline.steps[0][1]  # StandardScaler.\n",
    "\n",
    "    def predict_on_batch(self, inputs):\n",
    "        scores = self._model.predict_on_batch(inputs)\n",
    "        return self._transform.transform(scores)\n",
    "\n",
    "\n",
    "class EnformerScoreVariantsPCANormalized:\n",
    "    def __init__(\n",
    "        self, tfhub_url, transform_pkl_path, organism=\"human\", num_top_features=500\n",
    "    ):\n",
    "        self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n",
    "        with tf.io.gfile.GFile(transform_pkl_path, \"rb\") as f:\n",
    "            self._transform = joblib.load(f)\n",
    "        self._num_top_features = num_top_features\n",
    "\n",
    "    def predict_on_batch(self, inputs):\n",
    "        scores = self._model.predict_on_batch(inputs)\n",
    "        return self._transform.transform(scores)[:, : self._num_top_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gae",
   "language": "python",
   "name": "gae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
