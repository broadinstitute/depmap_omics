{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brazilian-original",
   "metadata": {},
   "source": [
    "# CCLE SNP Fingerprinting Pipeline\n",
    "Author: William Colgan (wcolgan@broadinstitute.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dalmatian as dm\n",
    "import subprocess\n",
    "from taigapy import TaigaClient\n",
    "from functools import reduce\n",
    "from genepy.google import gcp\n",
    "import asyncio\n",
    "from genepy import terra\n",
    "\n",
    "tc = TaigaClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline parameters\n",
    "\n",
    "# Local directory to save intermediate files to\n",
    "working_dir = \"temp/\"\n",
    "\n",
    "# GC storage bucket containing fingerprints\n",
    "fingerprints_dir = \"gs://fc-secure-6b6a3e1a-6fb8-4d30-b0df-a359e6c5d6e6/fingerprints/\"\n",
    "\n",
    "# GC storage bucket containing lists for vcf files\n",
    "vcf_list_dir = \"gs://fc-secure-6b6a3e1a-6fb8-4d30-b0df-a359e6c5d6e6/vcf_lists/\"\n",
    "\n",
    "# Batch sise for crosscheck_vcf. If more than 200 bams are being run this should be decreased\n",
    "crosscheck_batch_size = 500\n",
    "recreate_batch = False\n",
    "\n",
    "WORKSPACE = \"broad-firecloud-ccle/CCLE_SNP_QC\"\n",
    "\n",
    "bamcolname = [\"legacy_bam_filepath\",\"legacy_bai_filepath\"]\n",
    "sid = 'id'\n",
    "\n",
    "allsampleset = \"all_samples\"\n",
    "\n",
    "sampleset = \"21Q3\"\n",
    "\n",
    "taiga_dataset = \"ccle-bam-fingerprints-6f30\"\n",
    "taiga_filename = 'fingerprint_lod_matrix'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-batman",
   "metadata": {},
   "source": [
    "## Generate Fingerprint VCFs\n",
    "\n",
    "Here we use Dalmatian to run the `fingerprint_bam_with_liftover` workflow on Terra. \n",
    "This workflow calls Picard ExtractFingerprint to generate a fingerprint VCF and then \n",
    "calls Picard LiftoverVcf to covert this vcf to hg38. To fingerprint hg38 bam files just run `fingerprint_bam` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-condition",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r rnasamples\n",
    "%store -r wgssamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbams = pd.concat([rnasamples, wgssamples])\n",
    "bams = fbams[bamcolname]\n",
    "bams[sid] = bams.index\n",
    "bams # a [bamcolname]+[id] dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch files listing all vcfs in fingerprints dir and upload to bucket\n",
    "# (NEW VERSION ONLY) will only needed if need to recreate batches\n",
    "\n",
    "if recreate_batch:\n",
    "    #vcf_list = gcp.lsFiles([vcf_list_dir])\n",
    "    #vcf_list = wm.get_samples()[\"fingerprint_vcf\"].tolist()\n",
    "    batches = []\n",
    "    for i, l in enumerate(range(0, len(vcf_list), crosscheck_batch_size)):\n",
    "        f = open(working_dir + \"vcf_batch_\"+str(i), 'w')\n",
    "        f.write(\"\\n\".join(vcf_list[l:l + crosscheck_batch_size]))\n",
    "        f.close()\n",
    "        batches.append(working_dir+\"vcf_batch_\"+str(i))\n",
    "    gcp.cpFiles(batches, vcf_list_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload sample sheet\n",
    "wm = dm.WorkspaceManager(WORKSPACE).disable_hound()\n",
    "samples_df = pd.DataFrame()\n",
    "samples_df[[\"bam_filepath\", \"bai_filepath\", \"sample_id\", \"participant_id\"]] = bams[bamcolname + [sid, sid]].values\n",
    "samples_df = samples_df.set_index('sample_id')\n",
    "wm.upload_samples(samples_df, add_participant_samples=True)\n",
    "wm.update_sample_set(sampleset, samples_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit jobs \n",
    "#submission_id = wm.create_submission(\"fingerprint_bam_with_liftover\", sampleset, 'sample_set', expression='this.samples')\n",
    "await terra.waitForSubmission(WORKSPACE, submission_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-processor",
   "metadata": {},
   "source": [
    "## Crosscheck Fingerprint VCFs\n",
    "\n",
    "Here we use Dalmation to run the `crosscheck_vcfs` workflow on Terra. This workflow calls Picard CrosscheckFingerprints to compare the new fingerprint vcfs to batches of existing fingerprint vcfs in `fingerprints_dir`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list with new vcfs and upload to bucket\n",
    "f = open(working_dir + sampleset, 'w')\n",
    "f.write(('\\n').join(wm.get_samples().loc[samples_df.index, 'fingerprints'].tolist()))\n",
    "f.close()\n",
    "gcp.cpFiles(working_dir + sampleset, vcf_list_dir)\n",
    "rm = working_dir + sampleset\n",
    "! rm $rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload sample sheet\n",
    "if recreate_batch:\n",
    "    sample_group_df = pd.DataFrame(data={\"entity:sample_group_id\" : batches, \"vcf_group\" : [vcf_list_dir + x for x in batches]}).set_index('entity:sample_group_id')\n",
    "else:\n",
    "    sample_group_df = pd.DataFrame(data={\"entity:sample_group_id\" : [sampleset], \"vcf_group\" : [vcf_list_dir+sampleset]}).set_index('entity:sample_group_id')\n",
    "#in case it does not work\n",
    "sample_group_df.to_csv(\"../temp.tsv\", sep='\\t')\n",
    "print(wm.get_entities('sample_group').index.tolist())\n",
    "wm.upload_entities(\"sample_group\", sample_group_df)\n",
    "#wm.update_entity_set(allsampleset, wm.get_entities('sample_group').index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit jobs\n",
    "#conf = wm.get_config(\"crosscheck_vcfs\")\n",
    "conf['inputs']['crosscheck.run_crosscheck.vcf_second_input_file'] = '\"'+vcf_list_dir+sampleset+'\"'\n",
    "wm.update_config(conf)\n",
    "submission_id = wm.create_submission(\"crosscheck_vcfs\", allsampleset, 'sample_set',expression='this.samples')\n",
    "await terra.waitForSubmission(WORKSPACE, submission_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-effectiveness",
   "metadata": {},
   "source": [
    "## Update LOD matrix\n",
    "\n",
    "Here we update the fingerprint LOD matrix on taiga with the new fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate matrix with LOD score for new fingerprint vcfs\n",
    "new_lod_list = []\n",
    "samples_df = wm.get_entities(\"sample_group\")['cross_checks_out'].tolist()\n",
    "for batch in samples_df:\n",
    "    # could be pd concat\n",
    "    df = pd.read_csv(batch, sep='\\t', comment='#')\n",
    "    lod_mat = df.pivot(index = \"LEFT_SAMPLE\",columns=\"RIGHT_SAMPLE\",values = \"LOD_SCORE\")\n",
    "    new_lod_list.append(lod_mat)\n",
    "new_lod_mat = pd.concat(new_lod_list)\n",
    "new_lod_mat.index.name = None\n",
    "new_lod_mat = new_lod_mat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update LOD matrix ( have to update (A+a)*(B+b) = (AB)+(aB)+(Ab)+(ab))\n",
    "prev_lod_mat =  tc.get(name=taiga_dataset,file=taiga_filename)\n",
    "new_ids = set(new_lod_mat.index)\n",
    "old_ids = set(prev_lod_mat.index) - set(new_ids)\n",
    "updated_lod_mat = pd.concat((prev_lod_mat.loc[old_ids,old_ids],new_lod_mat.loc[new_ids,old_ids]), axis=0)\n",
    "updated_lod_mat = pd.concat((updated_lod_mat.loc[new_ids.union(old_ids), old_ids], new_lod_mat.transpose().loc[new_ids.union(old_ids, new_ids)]), axis=1)\n",
    "updated_lod_mat.to_csv(working_dir+taiga_filename+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from depmapomics import tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = tracker.getCCLETracker()\n",
    "ref = ref.append(fbams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"issues with \")\n",
    "previ = ''\n",
    "l = {}\n",
    "for i,j in [(v.index[x], v.columns[y]) for x, y in np.argwhere(v.values>500)]:\n",
    "    if i == j:\n",
    "        continue\n",
    "    if ref.loc[i]['participant_id'] == ref.loc[j]['participant_id']:\n",
    "        continue\n",
    "    if i != previ:\n",
    "        if previ!='':\n",
    "            l.update({'_'.join(ref.loc[previ, ['arxspan_id','version','datatype','participant_id','stripped_cell_line_name']].astype(str).values.tolist()):n})\n",
    "        n = [tuple(ref.loc[j, ['arxspan_id','version','datatype','participant_id','stripped_cell_line_name']].values)]\n",
    "    else:\n",
    "        n.append(tuple(ref.loc[j, ['arxspan_id','version','datatype','participant_id','stripped_cell_line_name']].values))\n",
    "    previ = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnasamples[rnasamples.arxspan_id==\"ACH-002061\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previ = ''\n",
    "l = {}\n",
    "#ref = ref.append(fbams)\n",
    "for u in set(fbams.arxspan_id):\n",
    "    res = v.loc[fbams[fbams.arxspan_id==u].index , ref[ref.arxspan_id==u].index.tolist()]\n",
    "    for i,j in [(res.index[x], res.columns[y]) for x, y in np.argwhere(res.values<100)]:\n",
    "        print('__________________________')\n",
    "        print(res.loc[i,j])\n",
    "        print(i,':', tuple(ref.loc[i,['arxspan_id','version','datatype','participant_id']].values),j,':',tuple(ref.loc[j,['arxspan_id','version','datatype','participant_id', 'blacklist']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload updated LOD matrix to Tiaga\n",
    "tc.update_dataset(dataset_permaname = taiga_dataset,\n",
    "                  changes_description=\"New bam fingerprints added for \"+sampleset,\n",
    "                  upload_files=[\n",
    "                    {\n",
    "                        \"path\": working_dir+taiga_filename+'.csv',\n",
    "                        \"name\": taiga_filename,\n",
    "                        \"format\": \"NumericMatrixCSV\",\n",
    "                        \"encoding\": \"utf-8\"\n",
    "                    }\n",
    "                 ],\n",
    "                 add_all_existing_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "230.4px",
    "left": "782px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
