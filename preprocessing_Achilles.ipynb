{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing achilles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1002\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n",
       "  var css_urls = [];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1002\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n  var css_urls = [];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from JKBio import Helper as h\n",
    "from taigapy import TaigaClient\n",
    "from JKBio.helper.google_sheet import GSheet\n",
    "from gsheets import Sheets\n",
    "from taigapy import TaigaClient\n",
    "import dalmatian as dm\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.stats import pearsonr,spearmanr\n",
    "\n",
    "from bokeh.plotting import *\n",
    "from bokeh.models import HoverTool\n",
    "from collections import OrderedDict\n",
    "from IPython.display import Image,display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext rpy2.ipython\n",
    "tc = TaigaClient()\n",
    "output_notebook()\n",
    "\n",
    "my_id = '~/.client_secret.json'\n",
    "mystorage_id = \"~/.storage.json\"\n",
    "sheets = Sheets.from_files(my_id, mystorage_id)\n",
    "replace = {'T': 'Tumor', 'N': 'Normal', 'm': 'Unknown', 'L': 'Unknown'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boot up\n",
    "\n",
    "we are instanciating all the parameters needed for this pipeline to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesetname = \"20Q4\"\n",
    "virtual_public='public-20q3-3d35'\n",
    "virtual_dmc='dmc-20q3-033d'\n",
    "virtual_internal='internal-20q3-00d0'\n",
    "\n",
    "prevname=\"20Q2\"\n",
    "prevversion=46\n",
    "release=samplesetname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some checks and manual rescuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "legacy_segments = tc.get(name='depmap-wes-cn-data--08f3', file='legacy_segments')\n",
    "legacy_segments = legacy_segments.drop(columns='Unnamed: 0')\n",
    "legacy_segments['Status']='U'\n",
    "prevgenecn = tc.get(name='segmented-cn-wes-prioritzed-7fe1', file='wes.'+prevname+'.gene', version=prevversion) \n",
    "prev = prevgenecn.index.tolist()\n",
    "priosegments = pd.read_csv(\"temp/segments_allWES_latest_\"+samplesetname+\".csv\")\n",
    "priogenecn = pd.read_csv('temp/gene_cn_allWES_latest_'+samplesetname+\".csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ACH-000003',\n",
       " 'ACH-000014',\n",
       " 'ACH-000016',\n",
       " 'ACH-000034',\n",
       " 'ACH-000044',\n",
       " 'ACH-000049',\n",
       " 'ACH-000057',\n",
       " 'ACH-000064',\n",
       " 'ACH-000068',\n",
       " 'ACH-000071',\n",
       " 'ACH-000077',\n",
       " 'ACH-000088',\n",
       " 'ACH-000164',\n",
       " 'ACH-000170',\n",
       " 'ACH-000179',\n",
       " 'ACH-000185',\n",
       " 'ACH-000194',\n",
       " 'ACH-000195',\n",
       " 'ACH-000216',\n",
       " 'ACH-000229',\n",
       " 'ACH-000230',\n",
       " 'ACH-000299',\n",
       " 'ACH-000300',\n",
       " 'ACH-000306',\n",
       " 'ACH-000333',\n",
       " 'ACH-000398',\n",
       " 'ACH-000413',\n",
       " 'ACH-000454',\n",
       " 'ACH-000479',\n",
       " 'ACH-000494',\n",
       " 'ACH-000526',\n",
       " 'ACH-000539',\n",
       " 'ACH-000550',\n",
       " 'ACH-000561',\n",
       " 'ACH-000575',\n",
       " 'ACH-000578',\n",
       " 'ACH-000600',\n",
       " 'ACH-000612',\n",
       " 'ACH-000632',\n",
       " 'ACH-000635',\n",
       " 'ACH-000642',\n",
       " 'ACH-000659',\n",
       " 'ACH-000685',\n",
       " 'ACH-000690',\n",
       " 'ACH-000705',\n",
       " 'ACH-000710',\n",
       " 'ACH-000731',\n",
       " 'ACH-000737',\n",
       " 'ACH-000739',\n",
       " 'ACH-000742',\n",
       " 'ACH-000842',\n",
       " 'ACH-000850',\n",
       " 'ACH-000854',\n",
       " 'ACH-000870',\n",
       " 'ACH-000904',\n",
       " 'ACH-000923',\n",
       " 'ACH-000928',\n",
       " 'ACH-000931',\n",
       " 'ACH-000982',\n",
       " 'ACH-001000',\n",
       " 'ACH-001015',\n",
       " 'ACH-001017',\n",
       " 'ACH-001018',\n",
       " 'ACH-001036',\n",
       " 'ACH-001037',\n",
       " 'ACH-001042',\n",
       " 'ACH-001043',\n",
       " 'ACH-001044',\n",
       " 'ACH-001045',\n",
       " 'ACH-001047',\n",
       " 'ACH-001071',\n",
       " 'ACH-001072',\n",
       " 'ACH-001079',\n",
       " 'ACH-001087',\n",
       " 'ACH-001088',\n",
       " 'ACH-001091',\n",
       " 'ACH-001092',\n",
       " 'ACH-001094',\n",
       " 'ACH-001101',\n",
       " 'ACH-001121',\n",
       " 'ACH-001130',\n",
       " 'ACH-001142',\n",
       " 'ACH-001146',\n",
       " 'ACH-001150',\n",
       " 'ACH-001171',\n",
       " 'ACH-001175',\n",
       " 'ACH-001188',\n",
       " 'ACH-001198',\n",
       " 'ACH-001207',\n",
       " 'ACH-001214',\n",
       " 'ACH-001224',\n",
       " 'ACH-001225',\n",
       " 'ACH-001230',\n",
       " 'ACH-001234',\n",
       " 'ACH-001239',\n",
       " 'ACH-001249',\n",
       " 'ACH-002217'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyinleg = set(legacy_segments.DepMap_ID) - set(priosegments.DepMap_ID)\n",
    "samegenes = set(prevgenecn.columns) & set(priogenecn.columns)\n",
    "onlyinleg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1697, 27562)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priogenecn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only gettinng good correlation samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achilles repriorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "toreplace = []\n",
    "corr = {}\n",
    "ge = priogenecn[samegenes]\n",
    "pre = (2**prevgenecn[samegenes])-1\n",
    "for sample in set(prevgenecn.index) & set(priogenecn.index):\n",
    "        c = ge.loc[sample].values\n",
    "        b = pre.loc[sample].values\n",
    "        corr[sample] = pearsonr(c,b)[0]   \n",
    "for k, val in corr.items():\n",
    "    if val<0.85:\n",
    "        toreplace.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6015625"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toreplace)/len(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedsegments = pd.concat([mergedsegments, legacy_segments[legacy_segments.DepMap_ID=='ACH-002335']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedgenecn.loc['ACH-002335'] = prevgenecn.loc['ACH-002335']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedsegments = priosegments[~priosegments.DepMap_ID.isin(toreplace)].append(legacy_segments[legacy_segments.DepMap_ID.isin(list(onlyinleg)+toreplace)]).reset_index(drop=True)\n",
    "mergedgenecn = priogenecn[~priogenecn.index.isin(toreplace)].append(prevgenecn.loc[list(onlyinleg)+toreplace][samegenes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767 1761 1794 1794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'ACH-000010',\n",
       "  'ACH-001210',\n",
       "  'ACH-001227',\n",
       "  'ACH-001293',\n",
       "  'ACH-001349',\n",
       "  'ACH-001434',\n",
       "  'ACH-001437',\n",
       "  'ACH-001438',\n",
       "  'ACH-001449',\n",
       "  'ACH-001493',\n",
       "  'ACH-001502',\n",
       "  'ACH-001512',\n",
       "  'ACH-001537',\n",
       "  'ACH-001662',\n",
       "  'ACH-001669',\n",
       "  'ACH-001672',\n",
       "  'ACH-001676',\n",
       "  'ACH-001686',\n",
       "  'ACH-001693',\n",
       "  'ACH-001696',\n",
       "  'ACH-001707',\n",
       "  'ACH-001708',\n",
       "  'ACH-001756',\n",
       "  'ACH-001758',\n",
       "  'ACH-001759',\n",
       "  'ACH-001760',\n",
       "  'ACH-001854',\n",
       "  'ACH-001855',\n",
       "  'ACH-001971',\n",
       "  'ACH-002010',\n",
       "  'ACH-002055',\n",
       "  'ACH-002138',\n",
       "  'ACH-002392'},\n",
       " {'ACH-001189',\n",
       "  'ACH-002303',\n",
       "  'ACH-002315',\n",
       "  'ACH-002335',\n",
       "  'ACH-002341',\n",
       "  'ACH-002359'})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new1 = set(mergedgenecn.index.values.tolist())\n",
    "new2 = set(mergedsegments['DepMap_ID'].values.tolist())\n",
    "print(len(prev), len(set(prev) & new1), len(new1), len(new1 & new2))\n",
    "new = new1-set(prev)\n",
    "removed = set(prev)-new1\n",
    "new,removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding WGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgssegments = pd.read_csv(\"temp/segments_allWGS_latest_\"+samplesetname+\".csv\")\n",
    "wgsgenecn = pd.read_csv('temp/gene_cn_allWGS_latest_'+samplesetname+\".csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1481.0390861476"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgssegments.Segment_Mean.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300.4713399641696"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgsgenecn.max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedgenecn = wgsgenecn.append(mergedgenecn[~mergedgenecn.index.isin(set(wgsgenecn.index))])\n",
    "mergedsegments = wgssegments.append(mergedsegments[~mergedsegments.DepMap_ID.isin(set(wgssegments.DepMap_ID))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedgenecn.loc[\"ACH-002875\"] = mergedgenecn.loc[\"ACH-000614\"]\n",
    "mergedgenecn.loc[\"ACH-002874\"] = mergedgenecn.loc[\"ACH-000219\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremie/miniconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "additional = mergedsegments[mergedsegments.DepMap_ID.isin([\"ACH-000614\",\"ACH-000219\"])]\n",
    "additional.loc[additional[additional.DepMap_ID == \"ACH-000614\"].index,'DepMap_ID'] = \"ACH-002875\"\n",
    "additional.loc[additional[additional.DepMap_ID == \"ACH-000219\"].index, 'DepMap_ID'] = 'ACH-002874'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedsegments = pd.concat([mergedsegments,additional])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedgenecn = mergedgenecn.apply(lambda x: np.log2(1+x))\n",
    "mergedsegments = mergedsegments[['DepMap_ID', 'Chromosome', 'Start', 'End', 'Segment_Mean', 'Num_Probes', 'Status', 'Source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(mergedgenecn.values.max() > 100):\n",
    "    print(\"\\n\\n\\nTOO HIGH, not LOG2 transformed!\")\n",
    "if(len(mergedgenecn.index) > len(set(mergedgenecn.index))):\n",
    "    print(\"Duplicate CL, not reprioritized well!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1806"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mergedgenecn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedgenecn.to_csv('temp/all_'+release+'_gene_cn.csv')\n",
    "mergedsegments.to_csv('temp/all_'+release+'_segment.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uploading on taiga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading all_20Q4_segment...\n",
      "hitting https://cds.team/taiga/api/datafile/ff78dac9bec14a5fa1831147be4f2ae4\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: all_20Q4_segment properly converted and uploaded\n",
      "Uploading all_20Q4_gene_cn...\n",
      "hitting https://cds.team/taiga/api/datafile/ff78dac9bec14a5fa1831147be4f2ae4\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1750\n",
      "\t Conversion in progress, line 1750\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: all_20Q4_gene_cn properly converted and uploaded\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 09439ce24ed841ef84afd1cfea7ad936 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/09439ce24ed841ef84afd1cfea7ad936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'09439ce24ed841ef84afd1cfea7ad936'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc.update_dataset(dataset_permaname=\"cn-wes-achilles-4dcd\",\n",
    "                 upload_file_path_dict={\n",
    "            'temp/all_'+release+'_segment.csv': 'TableCSV',\n",
    "            'temp/all_'+release+'_gene_cn.csv': \"NumericMatrixCSV\"}, \n",
    "                 changes_description=\n",
    "\"\"\"\n",
    "adding new lines:\n",
    "\"\"\"+str(new)+\"\"\"\n",
    "\n",
    "removed a couple of lines that were wrong:\n",
    "\"\"\"+str(removed),\n",
    "                  dataset_description=\"\"\"\n",
    "# Copy Number\n",
    "\n",
    "Combined segment and gene-level CN calls from Broad WES, Sanger WES, and Broad SNP. Relative CN, log2(x+1) transformed.\n",
    "\n",
    "PORTAL TEAM SHOULD NOT USE THIS: There are lines here that should not make it even to internal. Must use subsetted dataset instead. These data will not make it on the portal starting 19Q1. With the DMC portal, there is new cell line release prioritization as to which lines can be included, so a new taiga dataset will be created containing CN for the portal.\n",
    "\n",
    "These data are generated for Achilles to pull from to run CERES.\n",
    "\n",
    "Versions to use:\n",
    "\n",
    "v45 for 20Q2 (For Achilles QC, use v44 of segmentcn)\n",
    "v40 for 20Q1\n",
    "v38 for 19Q4\n",
    "v33 for 19Q4\n",
    "v25 for 19Q2 (hg38 aligned, Broad WES and Sanger WES based calls were generated from bam realignment. SNP based calls are still from liftover). The gene mapping script was updated to improve the gene level matrix (to remove NAs). The segment level matrix is untransformed relative CN. Gene level matrix is log2(CN + 1).\n",
    "v20 for 19Q1 (version 21 is hg19)\n",
    "v18: for 18Q4\n",
    "v15: for 18Q3\n",
    "v11: for 18Q2\n",
    "Gene-level matrix in versions below 10 were using hg38 and not hg19. Version 11 is corrected and should be used instead\n",
    "\n",
    "Calls on X, Y chromosome for profiles should not be used.\n",
    "\n",
    "Prioritization is as follows:\n",
    "\n",
    "Broad WES kept over everything\n",
    "Sanger WES kept if:\n",
    "This cell line did not fail fingerprinting\n",
    "This cell line has no other CN data\n",
    "This cell lines does not have CRISPR LFC data from the Achilles screen\n",
    "This CN profile correlates better with Achilles CRISPR LFC data than Broad SNP CN OR the % gene-level difference between this cell lines CN profile from Sanger WES and Broad SNP < 2.5%\n",
    "Broad SNP used for remaining lines with no Broad WES or with Sanger WES that does not pass the criteria above\n",
    "The 'Source' column indicates which CN profile was used for that cell line.\n",
    "\n",
    "version 6: renamed Sample column to CCLE_name for consistency for the Achilles pipeline\n",
    "\n",
    "version 7: missing chordoma lines\n",
    "\n",
    "version 8: fixed to names of two chordoma lines (changed suffix from CHORDOMA -> BONE) and removed renamed \n",
    "cell lines that were duplicated (with different names). Reran comparison using 18q2 LFC results. Gene level matrix will be generated for version 9\n",
    "\n",
    "version 11: corrected error in gene-level matrix calculation (previously had been aligned to hg38 however alignment should be hg19). Segment level calls are unaffected.\n",
    "\n",
    "versions 12-14: Sanger WES were multiplied by 2 so should not be used\n",
    "\n",
    "version 15: internal segments and gene level matrices for 18q3 release including public version (removed black list lines and Broad WES < 6 months old). Gene level matrices are indexed using Broad IDs.\n",
    "\n",
    "version 16: internal segments and gene level matrices for 18q4 release including public version (uses all SNP and only WES if those lines are present in the 18Q4 public Achilles dataset)\n",
    "\n",
    "version 17: same as version 16 but with two additional line in the internal version\n",
    "\n",
    "version 18: same as version 17 but switched one line in public to use SNP instead of WES because not in public Avana\n",
    "\n",
    "version 20: two major changes occurred (1) we are using a FireCloud based pipeline for CN calling now for Broad WES data (2) we have moved to use hg38. This is accomplished by lifting over coordinates from hg19 to hg38 after processed by the CN pipeline.\n",
    "v21 same as version 20, but we are using the original hg19 coordinates, not hg38\n",
    "\n",
    "version 25: Broad WES and Sanger WES were realigned to hg38. SNP still uses liftover from hg19 to hg38\n",
    "\n",
    "version 35: Seeing what went wrong with the upload.\n",
    "\n",
    "version 36: problem with not log2 transforming the data\n",
    "\n",
    "version 37: resolving the problem with log2 transforming the segment data\n",
    "\n",
    "version 38: resolving the problem with log2 transforming the segment data\n",
    "\n",
    "version 39: 20Q1. Samples ACH-002511 (M140325) and ACH-001370 (OCIP5X) appear to have too many segments looking at the CN profile.\n",
    "\n",
    "version 40: unlog2 transforming segmentcn\n",
    "\n",
    "version 41: 20Q2 (segmentcn is just relative copy number, whereas the genecn is log2(x+1) transformed). Added 7 new samples.\n",
    "These CN plots subjectively appear to have too many segments in new 20Q2 samples: ACH-002399 (CDS-sukIAT, 21NT\\_1), ACH-002401 (CDS-tVy3GF, 21MT2\\_1), ACH-002400 (CDS-VUHMHG, 21MT1\\_1)\n",
    "\n",
    "version 42: **note: version 42 is missing some of the cell lines. Do not use** \n",
    "\n",
    "version 43: Resolving issue of no DepMap ID index in the genecn file. Duplicating the CN data in genecn and segmentcn for ACH-000219 so we have CN data for ACH-002874, the same cell line grown in different media. This step is required for Achilles / CERES.\n",
    "\n",
    "version 44: Removing duplication of ACH-000219 from genecn file. The Achilles QC only needs the duplication in the segmentcn file. This change results in 1767 unique DepMap IDs in the genecn file, and 1768 unique DepMap IDs in the segmentcn file.\n",
    "\n",
    "version 45: Removing duplication of ACH-000219 from segmentcn file to prevent issues in future releases, which use this Taiga dataset in the process of determining which lines should be released to Public. Now both the genecn file and the segmentcn file have 1767 unique DepMap IDs.\n",
    "\n",
    "version 46: removing two weird undefined lines \n",
    "\n",
    "version 47: 20Q4 data with WGS now!\n",
    "\n",
    "version 48: solving issues with log transform\n",
    "\n",
    "Gene level CN data:\n",
    "\n",
    "__Rows__: DepMap cell line IDs\n",
    "\n",
    "__Columns__: gene names in the format HGNC\\_symbol (Entrez\\_ID)\n",
    "\n",
    "Segment level data:\n",
    "\n",
    "__Columns__: DepMap\\_ID, Chromosome, Start, End, Num\\_Probes, Segment\\_Mean\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "277px",
    "width": "375px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "198.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "617.8px",
    "left": "1232.4px",
    "right": "20px",
    "top": "120px",
    "width": "262.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
