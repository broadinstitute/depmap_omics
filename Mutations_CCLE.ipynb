{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you need to have installed JKBio in the same folder as ccle_processing\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The rpy2.ipython extension is already loaded. To reload it, use:\n",
      "  %reload_ext rpy2.ipython\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1005\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1005\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1005\");\n",
       "  if (element == null) {\n",
       "    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1005' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n",
       "  var css_urls = [];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1005\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1005\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };var element = document.getElementById(\"1005\");\n  if (element == null) {\n    console.error(\"Bokeh: ERROR: autoload.js configured with elementid '1005' but no matching script tag was found. \")\n    return false;\n  }\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.3.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.3.4.min.js\"];\n  var css_urls = [];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1005\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.CCLE_postp_function import *\n",
    "from JKBio import Datanalytics as da \n",
    "from JKBio import TerraFunction as terra\n",
    "from JKBio import Helper as h\n",
    "from JKBio.helper.google_sheet import GSheet\n",
    "from gsheets import Sheets\n",
    "from taigapy import TaigaClient\n",
    "import dalmatian as dm\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from bokeh.plotting import *\n",
    "from bokeh.models import HoverTool\n",
    "from collections import OrderedDict\n",
    "from IPython.display import Image,display\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext rpy2.ipython\n",
    "tc = TaigaClient()\n",
    "output_notebook()\n",
    "\n",
    "my_id = '~/.client_secret.json'\n",
    "mystorage_id = \"~/.storage.json\"\n",
    "sheets = Sheets.from_files(my_id, mystorage_id)\n",
    "replace = {'T': 'Tumor', 'N': 'Normal', 'm': 'Unknown', 'L': 'Unknown'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## boot up\n",
    "\n",
    "we are instanciating all the parameters needed for this pipeline to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesetname = \"20Q4\"\n",
    "prevname=\"20Q3\"\n",
    "prevversion = 23\n",
    "prevprevname ='20Q2'\n",
    "prevprevversion= 22\n",
    "virtual_public='public-20q3-3d35'\n",
    "virtual_dmc='dmc-20q3-033d'\n",
    "virtual_internal='internal-20q3-00d0'\n",
    "\n",
    "workspace1=\"broad-genomics-delivery/Getz_IBM_CellLines_Exomes\"\n",
    "workspace2=\"broad-firecloud-ccle/CCLE_DepMap_WES\"\n",
    "workspace3=\"broad-genomics-delivery/CCLE_DepMap_WES\"\n",
    "\n",
    "workspace6=\"terra-broad-cancer-prod/CCLE_DepMap_WES\"\n",
    "\n",
    "refworkspace=\"broad-firecloud-ccle/DepMap_Mutation_Calling_CGA_pipeline\"\n",
    "\n",
    "rnaworkspace=\"broad-firecloud-ccle/DepMap_hg38_RNAseq\"\n",
    "\n",
    "source1=\"ibm\"\n",
    "source2=\"ccle\"\n",
    "source3=\"ccle\"\n",
    "source6=\"ccle\"\n",
    "source7=\"ibm\"\n",
    "\n",
    "refsheet_url = \"https://docs.google.com/spreadsheets/d/1XkZypRuOEXzNLxVk9EOHeWRE98Z8_DBvL4PovyM01FE\"\n",
    "refsheet_id = \"555466897\"\n",
    "sheeturl = \"https://docs.google.com/spreadsheets/d/115TUgA1t_mD32SnWAGpW9OKmJ2W5WYAOs3SuSdedpX4\"\n",
    "\n",
    "release = samplesetname\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm1 = dm.WorkspaceManager(workspace1)\n",
    "wm2 = dm.WorkspaceManager(workspace2)\n",
    "wm3 = dm.WorkspaceManager(workspace3)\n",
    "\n",
    "wm6 = dm.WorkspaceManager(workspace6)\n",
    "\n",
    "refwm = dm.WorkspaceManager(refworkspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_to_change = {'from_arxspan_id': 'participant'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/Sep/2020 20:13:03] INFO - URL being requested: GET https://sheets.googleapis.com/v4/spreadsheets/1XkZypRuOEXzNLxVk9EOHeWRE98Z8_DBvL4PovyM01FE?alt=json\n",
      "[24/Sep/2020 20:13:03] INFO - Refreshing due to a 401 (attempt 1/2)\n",
      "[24/Sep/2020 20:13:03] INFO - access_token is expired. Now: 2020-09-24 20:13:03.499020, token_expiry: 2020-09-24 19:18:09\n",
      "[24/Sep/2020 20:13:03] INFO - Refreshing access_token\n",
      "[24/Sep/2020 20:13:03] INFO - URL being requested: GET https://sheets.googleapis.com/v4/spreadsheets/1XkZypRuOEXzNLxVk9EOHeWRE98Z8_DBvL4PovyM01FE/values:batchGet?majorDimension=ROWS&valueRenderOption=UNFORMATTED_VALUE&dateTimeRenderOption=FORMATTED_STRING&ranges=newsamples&alt=json\n"
     ]
    }
   ],
   "source": [
    "ccle_refsamples = sheets.get(refsheet_url).sheets[0].to_frame().set_index(\"cds_sample_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Service account info was not in the expected format, missing fields client_email.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-50f4f7cca1ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrefsheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGSheet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/jeremie/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmystorage_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefsheet_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefsheet_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/JKBio/helper/google_sheet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, credentials, document_id, sheet_id)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGSheet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredientials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sheets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/JKBio/helper/google_sheet.py\u001b[0m in \u001b[0;36mget_credentials\u001b[0;34m(credentials)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     return service_account.Credentials.from_service_account_file(\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscopes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSCOPES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/google/oauth2/service_account.py\u001b[0m in \u001b[0;36mfrom_service_account_file\u001b[0;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \"\"\"\n\u001b[1;32m    208\u001b[0m         info, signer = _service_account_info.from_filename(\n\u001b[0;32m--> 209\u001b[0;31m             filename, require=['client_email', 'token_uri'])\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_signer_and_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/google/auth/_service_account_info.py\u001b[0m in \u001b[0;36mfrom_filename\u001b[0;34m(filename, require)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/google/auth/_service_account_info.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(data, require)\u001b[0m\n\u001b[1;32m     49\u001b[0m         raise ValueError(\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m'Service account info was not in the expected format, missing '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             'fields {}.'.format(', '.join(missing)))\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Create a signer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Service account info was not in the expected format, missing fields client_email."
     ]
    }
   ],
   "source": [
    "refsheet = GSheet('/home/jeremie/'+mystorage_id[2:], refsheet_url, refsheet_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new data\n",
    "\n",
    "We are looking for new samples in a range of workspaces.\n",
    "\n",
    "They are quite messy and might contains duplicates, contain broken file paths...\n",
    "\n",
    "- We are thus looking at the bam files one by one and comparing them with our own bams. \n",
    "- We remove broken files, duplicates and add new version of a cell line's bam if we find some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refsamples is overrided by a refurl\n",
      "[23/Sep/2020 23:00:56] WARNING - file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jeremie/miniconda3/lib/python3.7/site-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
      "    from google.appengine.api import memcache\n",
      "ModuleNotFoundError: No module named 'google.appengine'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jeremie/miniconda3/lib/python3.7/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
      "    from oauth2client.contrib.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jeremie/miniconda3/lib/python3.7/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
      "    from oauth2client.locked_file import LockedFile\n",
      "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jeremie/miniconda3/lib/python3.7/site-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
      "    from . import file_cache\n",
      "  File \"/home/jeremie/miniconda3/lib/python3.7/site-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
      "    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n",
      "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
      "[23/Sep/2020 23:00:56] INFO - URL being requested: GET https://www.googleapis.com/discovery/v1/apis/sheets/v4/rest\n",
      "[23/Sep/2020 23:00:57] INFO - URL being requested: GET https://sheets.googleapis.com/v4/spreadsheets/1XkZypRuOEXzNLxVk9EOHeWRE98Z8_DBvL4PovyM01FE?alt=json\n",
      "[23/Sep/2020 23:00:57] INFO - Refreshing due to a 401 (attempt 1/2)\n",
      "[23/Sep/2020 23:00:57] INFO - Refreshing access_token\n",
      "[23/Sep/2020 23:00:57] INFO - URL being requested: GET https://sheets.googleapis.com/v4/spreadsheets/1XkZypRuOEXzNLxVk9EOHeWRE98Z8_DBvL4PovyM01FE/values:batchGet?majorDimension=ROWS&valueRenderOption=UNFORMATTED_VALUE&dateTimeRenderOption=FORMATTED_STRING&ranges=newsamples&alt=json\n",
      "Getting sample infos...\n",
      "\n",
      "The shape of the sample tsv from <dalmatian.wmanager.WorkspaceManager broad-genomics-delivery/Getz_IBM_CellLines_Exomes>: (197, 232)\n",
      "Identifying any true duplicates by checking file hashes (this runs for each data source)...\n",
      "This step can take a while as we need to use gsutil to check the size of each potential duplicate...\n",
      "listing files in gs\n",
      "These 1 bam file path do not exist: {'gs://fc-11136d2a-c57a-4e95-ac0e-ff6850e8db54/Getz_IBM_CellLines_Exomes_6samples_04282018/C836/NA/KYO-1/v1/KYO-1.bam'}\n",
      "listing files in gs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremie/ccle_processing/src/CCLE_postp_function.py:238: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  [i for i in samples[extract[\"bam\"]] if type(i) is str and str(i) != 'NA'], \"-L\", 200)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing files in gs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremie/ccle_processing/src/CCLE_postp_function.py:240: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[extract['size']] = [gcp.extractSize(i)[1] for i in gcp.lsFiles(samples[extract['bam']].tolist(), '-al', 200)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989247311827957945\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremie/ccle_processing/src/CCLE_postp_function.py:249: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[extract[\"release_date\"]] = h.getBamDate(samples[extract[\"bam\"]])\n",
      "/home/jeremie/ccle_processing/src/CCLE_postp_function.py:250: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[extract['release_date']] = list(h.datetoint(samples[extract['release_date']].values))\n",
      "/home/jeremie/ccle_processing/src/CCLE_postp_function.py:253: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[extract['ref_id']] = ['CDS-' + h.randomString(stringLength=6, stype='all', withdigits=True) for _ in range(len(samples))]\n",
      "/home/jeremie/ccle_processing/src/CCLE_postp_function.py:254: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[extract['patient_id']] = ['PT-' + h.randomString(stringLength=8, stype='all', withdigits=True) for _ in range(len(samples))]\n",
      "/home/jeremie/ccle_processing/src/CCLE_postp_function.py:256: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  samples[extract['source']] = source\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listing files in gs\n",
      "listing files in gs\n",
      "we found and removed 0 samples which did not match our id names: ['ACH-', 'CDS-']\n",
      "found 103 likely replicate\n",
      "listing files in gs\n",
      "Len of samples before removal: 103\n",
      "Dups from this workspace has len 0:\n",
      " []\n",
      "Len of samples after removal: 103\n",
      "\n",
      "The shape of the sample tsv from <dalmatian.wmanager.WorkspaceManager broad-firecloud-ccle/CCLE_DepMap_WES>: (288, 224)\n",
      "Identifying any true duplicates by checking file hashes (this runs for each data source)...\n",
      "This step can take a while as we need to use gsutil to check the size of each potential duplicate...\n",
      "listing files in gs\n",
      "These 1 bam file path do not exist: {''}\n",
      "listing files in gs\n",
      "listing files in gs\n",
      "listing files in gs\n",
      "listing files in gs\n",
      "we found and removed 0 samples which did not match our id names: ['ACH-', 'CDS-']\n",
      "found 17 likely replicate\n",
      "listing files in gs\n",
      "Len of samples before removal: 17\n",
      "Dups from this workspace has len 0:\n",
      " []\n",
      "Len of samples after removal: 17\n",
      "\n",
      "The shape of the sample tsv from <dalmatian.wmanager.WorkspaceManager terra-broad-cancer-prod/CCLE_DepMap_WES>: (231, 243)\n",
      "Identifying any true duplicates by checking file hashes (this runs for each data source)...\n",
      "This step can take a while as we need to use gsutil to check the size of each potential duplicate...\n",
      "listing files in gs\n",
      "These 14 bam file path do not exist: {'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/CCLE_DepMap_WES_July_Dec_2018/RP-1561/Exome/HSC5/v3/HSC5.bam', 'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/CCLE_DepMap_WES_July_Dec_2018/RP-1561/Exome/UMUC7/v4/UMUC7.bam', 'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/CCLE_DepMap_WES_July_Dec_2018/RP-1561/Exome/UMUC13/v3/UMUC13.bam', 'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/CCLE_DepMap_WES_July_Dec_2018/RP-1561/Exome/UMUC9/v3/UMUC9.bam', 'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/DepMap_CellLine_WES_Batch3_June2019/RP-1561/Exome/R256/v1/R256.bam', 'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/CCLE_DepMap_WES_July_Dec_2018/RP-1561/Exome/H103/v4/H103.bam', 'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/CCLE_DepMap_WES_July_Dec_2018/RP-1561/Exome/COV413A/v3/COV413A.bam', 'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/CCLE_DepMap_WES_July_Dec_2018/RP-1561/Exome/UMUC16/v3/UMUC16.bam', 'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/CCLE_DepMap_WES_July_Dec_2018/RP-1561/Exome/UMUC4/v3/UMUC4.bam', 'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/CCLE_DepMap_WES_July_Dec_2018/RP-1561/Exome/UMUC6/v3/UMUC6.bam', 'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/DepMap_CellLine_WES_July2019/RP-1561/Exome/HAP1MLH1KO_1/v1/HAP1MLH1KO_1.bam', 'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/CCLE_DepMap_WES_July_Dec_2018/RP-1561/Exome/UMUC11/v3/UMUC11.bam', 'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/CCLE_DepMap_WES_July_Dec_2018/RP-1561/Exome/OCIC5X/v3/OCIC5X.bam', 'gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/CCLE_DepMap_WES_July_Dec_2018/RP-1561/Exome/UMUC10/v3/UMUC10.bam'}\n",
      "listing files in gs\n",
      "listing files in gs\n",
      "listing files in gs6\n",
      "listing files in gs\n",
      "we found and removed 0 samples which did not match our id names: ['ACH-', 'CDS-']\n",
      "found 149 likely replicate\n",
      "listing files in gs\n",
      "Len of samples before removal: 151\n",
      "Dups from this workspace has len 0:\n",
      " []\n",
      "Len of samples after removal: 151\n",
      "we had 16 duplicates in the release buckets\n",
      "found 0 matched normals\n"
     ]
    }
   ],
   "source": [
    "# we will be missing \"primary disease\",\"sm_id\", \"cellosaurus_id\", \"gender, \"age\", \"primary_site\", \"primary_disease\", \"subtype\", \"subsubtype\", \"origin\", \"comments\"\n",
    "#when SMid: match== \n",
    "samples, pairs, noarxspan = GetNewCellLinesFromWorkspaces(refworkspace, stype='wes', refurl=refsheet_url, wmfroms = [workspace1, workspace2, workspace6], sources=[source1, source2, source6], match=['ACH-','CDS-'], participantslicepos=10, accept_unknowntypes=True, extract=extract_to_change, recomputedate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "noarxspan = noarxspan.sort_values(by='stripped_cell_line_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "noarxspan.to_csv('temp/noarxspan_wes_'+release+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finding back arxspan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "noarxspan['arxspan_id'] = [\"ACH-001001\", \"ACH-000520\", \"ACH-000740\", \"ACH-000283\", \"ACH-000757\", \"ACH-001328\", \"ACH-000157\", \"ACH-000557\", \"ACH-000593\", \"ACH-001454\", \"ACH-001456\", \"ACH-001458\", \"ACH-001459\", \"ACH-001460\", \"ACH-001461\", \"ACH-000511\", \"ACH-000025\", \"ACH-000458\", \"ACH-001339\", \"ACH-000662\", \"ACH-000695\", \"ACH-000278\", \"ACH-000123\", \"ACH-000608\", \"ACH-001061\", \"ACH-000698\", \"ACH-000877\", \"ACH-001496\", \"ACH-001497\", \"ACH-000487\", \"ACH-001067\", \"ACH-001500\", \"ACH-000047\", \"ACH-001345\", \"ACH-000840\", \"ACH-000868\", \"ACH-000901\", \"ACH-000143\", \"ACH-000150\", \"ACH-000339\", \"ACH-000872\", \"ACH-000029\", \"ACH-000941\", \"ACH-000946\", \"ACH-000954\", \"ACH-000004\", \"ACH-000005\", \"ACH-000393\", \"ACH-001522\", \"ACH-000274\", \"ACH-001523\", \"ACH-000509\", \"ACH-000672\", \"ACH-000310\", \"ACH-000577\", \"ACH-000237\", \"ACH-000993\", \"ACH-001098\", \"ACH-000167\", \"ACH-000419\", \"ACH-000028\", \"ACH-000823\", \"ACH-001113\", \"ACH-000596\", \"ACH-000591\", \"ACH-000634\", \"ACH-000673\", \"ACH-000676\", \"ACH-000128\", \"ACH-000215\", \"ACH-000760\", \"ACH-000007\", \"ACH-000152\", \"ACH-001550\", \"ACH-001551\", \"ACH-001552\", \"ACH-000019\", \"ACH-000884\", \"ACH-001554\", \"ACH-001555\", \"ACH-001556\", \"ACH-001557\", \"ACH-001558\", \"ACH-001559\", \"ACH-001560\", \"ACH-001561\", \"ACH-001562\", \"ACH-000758\", \"ACH-001563\", \"ACH-001566\", \"ACH-001567\", \"ACH-001568\", \"ACH-001569\", \"ACH-001570\", \"ACH-001129\", \"ACH-000866\", \"ACH-000514\", \"ACH-000921\", \"ACH-000434\", \"ACH-000010\", \"ACH-000912\", \"ACH-000700\", \"ACH-000251\", \"ACH-001075\", \"ACH-000337\", \"ACH-000837\", \"ACH-000800\", \"ACH-000767\", \"ACH-000378\", \"ACH-000200\", \"ACH-001368\", \"ACH-000436\", \"ACH-000247\", \"ACH-000544\", \"ACH-000296\", \"ACH-001373\", \"ACH-001151\", \"ACH-000022\", \"ACH-000606\", \"ACH-000960\", \"ACH-000791\", \"ACH-000774\", \"ACH-000261\", \"ACH-000398\", \"ACH-000473\", \"ACH-001386\", \"ACH-001645\", \"ACH-000887\", \"ACH-000655\", \"ACH-000490\", \"ACH-001190\", \"ACH-000312\", \"ACH-001194\", \"ACH-000017\", \"ACH-001654\", \"ACH-000127\", \"ACH-000302\", \"ACH-000461\", \"ACH-000466\", \"ACH-000736\", \"ACH-000898\", \"ACH-000537\", \"ACH-000460\", \"ACH-000280\", \"ACH-000316\", \"ACH-001390\", \"ACH-001391\", \"ACH-001394\", \"ACH-000122\", \"ACH-000677\", \"ACH-000820\", \"ACH-001402\", \"ACH-000452\", \"ACH-001210\", \"ACH-000036\", \"ACH-000262\", \"ACH-000304\", \"ACH-001709\", \"ACH-000836\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "noarxspan.loc[noarxspan[noarxspan['stripped_cell_line_name']==\"SUM299PE1\"].index,\"stripped_cell_line_name\"] = \"SUM299PE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we found and removed 0 samples which did not match our id names: ['ACH', 'CDS']\n",
      "found 159 likely replicate\n",
      "listing files in gs\n",
      "Len of samples before removal: 159\n",
      "Dups from this workspace has len 0:\n",
      " []\n",
      "Len of samples after removal: 159\n"
     ]
    }
   ],
   "source": [
    "noarxspan = resolveFromWorkspace(noarxspan, refsamples = ccle_refsamples[ccle_refsamples['datatype'] == 'wes'], match = ['ACH','CDS'], participantslicepos = 10, accept_unknowntypes = True, extract = extract_to_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#assess any potential issues\n",
    "set(noarxspan.arxspan_id) & set(samples.arxspan_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.concat([samples, noarxspan], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we had 0 duplicates in the release buckets\n"
     ]
    }
   ],
   "source": [
    "samples = assessAllSamples(samples, ccle_refsamples, stype='wes', rename={}, extract={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nan'}"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(pairs.control_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: manage the match normals in noarxspan samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the addtional data and writing it here in the right order 'as shown above'\n",
    "- use the stripped_cell_line_name to find the samples on https://docs.google.com/spreadsheets/d/1uqCOos-T9EMQU7y2ZUw4Nm84opU5fIT1y7jet1vnScE/edit#gid=356471436. \n",
    "- Make sure that we don't have duplicate cell lines in there. Otherwise, use the duplicate renaming function\n",
    "- copy Primary Site, Primary Disease, Subtype, Comments, Disease Sub-subtype, if they exist. (sometimes subtype and subsubtype are the same.. don't use subsubtype then.\n",
    "- look for the cell line in cellosaurus, you might need to use one of the aliases given in master depmap pv..\n",
    "- copy  cellosaurus_id gender age info or write 'U' if they don't exist. 'can be a number or {Embryonic, Children, Adult, Fetus, U} \n",
    "- check that it does not say this cell line is not a duplicate from another cell line\n",
    "- check that if it says this cell line is derived/children/father/samepatient from other cell lines, and that if we have any of the other cell lines, that the patient id is changed to be the same one for all (be sure that you are updating everywhere these patient ids are used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6610"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ccle_refsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, ['PT-6v9LD4Za', 'PT-fPnNIehK'])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If I have a previous samples I can update unknown data directly\n",
    "# TODO: to functionalize\n",
    "index=[]\n",
    "notfound=[]\n",
    "toupdate = {\"sex\":[],\n",
    "\"primary_disease\":[],\n",
    "\"sm_id\":[],\n",
    "\"cellosaurus_id\":[],\n",
    "\"age\":[],\n",
    "\"primary_site\":[],\n",
    "\"subtype\":[],\n",
    "\"subsubtype\":[],\n",
    "\"origin\":[],\n",
    "\"parent_cell_line\":[''],\n",
    "\"matched_normal\":[''],\n",
    "\"comments\":[],\n",
    "\"participant_id\":[]}\n",
    "for k, val in samples.iterrows():\n",
    "    dat = ccle_refsamples[ccle_refsamples['arxspan_id']==val['arxspan_id']]\n",
    "    if len(dat)>0:\n",
    "        index.append(k)\n",
    "        for k, v in toupdate.items():\n",
    "            toupdate[k].append(dat[k].tolist()[0])\n",
    "    else:\n",
    "        notfound.append(k)\n",
    "# doing so..\n",
    "for k, v in toupdate.items():\n",
    "    samples.loc[index,k] =v\n",
    "len(samples.loc[notfound].patient_id), samples.loc[notfound].patient_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found same patient\n",
    "a = [\"ACH-000635\",\"ACH-000717\", \"ACH-000864\", \"ACH-001042\", \"ACH-001547\"]\n",
    "b = [\"ACH-002291\",\"ACH-001672\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate ach-id\n",
    "dup = {\"ACH-001620\": \"ACH-001605\",\n",
    "\"ACH-001621\": \"ACH-001606\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any([i in samples.arxspan_id.tolist() for i in dup.keys()]):\n",
    "    samples = changeCellLineNameInNewSet(new = samples, ref=ccle_refsamples, datatype=\"wes\", dupdict=dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If I have a previous samples I can update unknown data directly\n",
    "index=[]\n",
    "notfound=[]\n",
    "toupdate = {\"sex\":[],\n",
    "\"primary_disease\":[],\n",
    "\"sm_id\":[],\n",
    "\"cellosaurus_id\":[],\n",
    "\"age\":[],\n",
    "\"primary_site\":[],\n",
    "\"subtype\":[],\n",
    "\"subsubtype\":[],\n",
    "\"parent_cell_line\":[''],\n",
    "\"matched_normal\":[''],\n",
    "\"origin\":[],\n",
    "\"comments\":[],\n",
    "\"participant_id\":[]}\n",
    "for k, val in samples.loc[notfound].iterrows():\n",
    "    dat = ccle_refsamples[ccle_refsamples['arxspan_id']==val['arxspan_id']]\n",
    "    if len(dat)>0:\n",
    "        index.append(k)\n",
    "        for k, v in toupdate.items():\n",
    "            toupdate[k].append(dat[k].tolist()[0])\n",
    "    else:\n",
    "        notfound.append(k)\n",
    "# doing so..\n",
    "for k, v in toupdate.items():\n",
    "    samples.loc[index,k] =v\n",
    "len(samples.loc[notfound].patient_id), samples.loc[notfound].patient_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>internal_bam_filepath</th>\n",
       "      <th>internal_bai_filepath</th>\n",
       "      <th>stripped_cell_line_name</th>\n",
       "      <th>arxspan_id</th>\n",
       "      <th>sequencing_date</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>crc32c_hash</th>\n",
       "      <th>size</th>\n",
       "      <th>version</th>\n",
       "      <th>datatype</th>\n",
       "      <th>sex</th>\n",
       "      <th>primary_disease</th>\n",
       "      <th>sm_id</th>\n",
       "      <th>cellosaurus_id</th>\n",
       "      <th>age</th>\n",
       "      <th>primary_site</th>\n",
       "      <th>subtype</th>\n",
       "      <th>subsubtype</th>\n",
       "      <th>origin</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>CDS-Qan1vz</td>\n",
       "      <td>gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/D...</td>\n",
       "      <td>gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/D...</td>\n",
       "      <td>WAOSEL</td>\n",
       "      <td>ACH-001707</td>\n",
       "      <td>737497</td>\n",
       "      <td>PT-fPnNIehK</td>\n",
       "      <td>y7Ygww==</td>\n",
       "      <td>20355332706</td>\n",
       "      <td>1</td>\n",
       "      <td>wes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        internal_bam_filepath  \\\n",
       "sample_id                                                       \n",
       "CDS-Qan1vz  gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/D...   \n",
       "\n",
       "                                        internal_bai_filepath  \\\n",
       "sample_id                                                       \n",
       "CDS-Qan1vz  gs://fc-9d2e10ea-1be3-4a23-a772-57854dbd1659/D...   \n",
       "\n",
       "           stripped_cell_line_name  arxspan_id  sequencing_date   patient_id  \\\n",
       "sample_id                                                                      \n",
       "CDS-Qan1vz                  WAOSEL  ACH-001707           737497  PT-fPnNIehK   \n",
       "\n",
       "           crc32c_hash         size  version datatype  sex primary_disease  \\\n",
       "sample_id                                                                    \n",
       "CDS-Qan1vz    y7Ygww==  20355332706        1      wes  NaN             NaN   \n",
       "\n",
       "            sm_id cellosaurus_id  age primary_site subtype subsubtype origin  \\\n",
       "sample_id                                                                      \n",
       "CDS-Qan1vz    NaN            NaN  NaN          NaN     NaN        NaN    NaN   \n",
       "\n",
       "           comments  \n",
       "sample_id            \n",
       "CDS-Qan1vz      NaN  "
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.loc[notfound]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "toupdate = {\"sex\":[\"Male\"],\n",
    "\"primary_disease\":[\"Leukemia\"],\n",
    "\"cellosaurus_id\":[\"CVCL_Y549\"],\n",
    "\"age\":['Adult'],\n",
    "\"primary_site\":[\"haematopoietic_and_lymphoid_tissue\"],\n",
    "\"subtype\":[\"CLL\"],\n",
    "\"subsubtype\":[\"b_cell\"],\n",
    "\"comments\":[\"B-type chronic lymphocytic leukemia (CLL, Rai stage I at diagnosis)\"],\n",
    "\"stripped_cell_line_name\":[\"21MT2\"],\n",
    "\"parent_cell_line\":[''],\n",
    "\"matched_normal\":[''],\n",
    "\"participant_id\":['PT-y3RbI7uD']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>primary_disease</th>\n",
       "      <th>cellosaurus_id</th>\n",
       "      <th>age</th>\n",
       "      <th>primary_site</th>\n",
       "      <th>subtype</th>\n",
       "      <th>subsubtype</th>\n",
       "      <th>comments</th>\n",
       "      <th>stripped_cell_line_name</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Leukemia</td>\n",
       "      <td>CVCL_Y549</td>\n",
       "      <td>Adult</td>\n",
       "      <td>haematopoietic_and_lymphoid_tissue</td>\n",
       "      <td>CLL</td>\n",
       "      <td>b_cell</td>\n",
       "      <td>B-type chronic lymphocytic leukemia (CLL, Rai ...</td>\n",
       "      <td>21MT2</td>\n",
       "      <td>PT-y3RbI7uD</td>\n",
       "      <td>21MT2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sex primary_disease cellosaurus_id    age  \\\n",
       "0  Male        Leukemia      CVCL_Y549  Adult   \n",
       "\n",
       "                         primary_site subtype subsubtype  \\\n",
       "0  haematopoietic_and_lymphoid_tissue     CLL     b_cell   \n",
       "\n",
       "                                            comments stripped_cell_line_name  \\\n",
       "0  B-type chronic lymphocytic leukemia (CLL, Rai ...                   21MT2   \n",
       "\n",
       "    patient_id   name  \n",
       "0  PT-y3RbI7uD  21MT2  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a  = pd.DataFrame(toupdate)\n",
    "a['name'] = samples.loc[notfound,\"stripped_cell_line_name\"].tolist()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updating..\n",
    "for k, v in toupdate.items():\n",
    "    samples.loc[notfound,k] =v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremie/miniconda3/lib/python3.7/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# uploading to our bucket (now a new function)\n",
    "h.changeToBucket(samples,'gs://cclebams/wes/', name_col= \"index\" , values=['internal_bam_filepath','internal_bai_filepath'], filetypes=['bam', 'bai'], catchdup=True, test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampes['baits'] = 'ice'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that we have all the cell lines we expect for this release\n",
    "\n",
    "This involves comparing to the list in the Google sheet \"Cell Line Profiling Status.\"\n",
    "\n",
    "_As the list cannot be parsed, we are not comparing it for now_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[]\n",
    "subccle_refsamples = ccle_refsamples[ccle_refsamples['datatype'] == \"wes\"]\n",
    "for k, val in samples.iterrows():\n",
    "    val = val[\"arxspan_id\"]\n",
    "    names.append(val)\n",
    "    samples.loc[k, 'version'] = len(subccle_refsamples[subccle_refsamples['arxspan_id'] == val]) + names.count(val)\n",
    "samples['version'] = samples['version'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccle_refsamples = pd.read_csv('temp/updated_ref_samples.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "subccle_refsamples.sequencing_date = h.datetoint(subccle_refsamples.sequencing_date.values, split='/', order = \"asc\")\n",
    "for k, val in samples.iterrows():\n",
    "    loc = subccle_refsamples[subccle_refsamples.arxspan_id==val.arxspan_id]\n",
    "    if len(loc)>0:\n",
    "        if val.sequencing_date > 0:\n",
    "            for i, v in loc.iterrows():\n",
    "                if v.sequencing_date > val.sequencing_date:\n",
    "                    ccle_refsamples.loc[i,'version']+=1\n",
    "                    samples.loc[k, 'version']-=1\n",
    "        else:\n",
    "            if max(loc['size']) > val['size']:\n",
    "                samples.loc[k, 'version'] = 1\n",
    "                ccle_refsamples.loc[loc.index,'version'] = ccle_refsamples.loc[loc.index,'version'].values+1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccle_refsamples = ccle_refsamples.append(samples, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in samepatient:\n",
    "    sub = ccle_refsamples[ccle_refsamples.arxspan_id.isin(val)]\n",
    "    if len(set(sub.participant_id))>2:\n",
    "        print('we found a missig participant relationship')\n",
    "        # ccle_refsamples.loc[ccle_refsamples.index, \"participant_id\"]=sub.participant_id[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccle_refsamples.to_csv('temp/updated_ref_samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 0 matched normals\n"
     ]
    }
   ],
   "source": [
    "pairs = setupPairsFromSamples(samples, subccle_refsamples, extract={'patient_id':'participant_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported 385 participants.\n",
      "Successfully imported 414 samples.\n",
      "Successfully imported 414 pairss.\n",
      "Pair set \"20Q4\" (414 pairs) successfully updated.\n",
      "Pair set \"all\" (2333 pairs) successfully updated.\n",
      "Pair set \"all_agilent\" (1403 pairs) successfully updated.\n",
      "Pair set \"all_ice\" (930 pairs) successfully updated.\n",
      "Sample set \"20Q4\" (414 samples) successfully updated.\n",
      "Sample set \"all\" (2333 samples) successfully updated.\n",
      "Sample set \"all_agilent\" (1403 samples) successfully updated.\n",
      "Sample set \"all_ice\" (930 samples) successfully updated.\n"
     ]
    }
   ],
   "source": [
    "#uploading new samples to mut\n",
    "refwm = refwm.disable_hound()\n",
    "refwm.upload_samples(samples)\n",
    "refwm.upload_entities('pairs', pairs)\n",
    "refwm.update_pair_set(pair_set_id=samplesetname,pair_ids=pairs.index)\n",
    "sam = refwm.get_samples()\n",
    "\n",
    "pair = refwm.get_pairs()\n",
    "refwm.update_pair_set(pair_set_id='all',pair_ids=pair.index)\n",
    "refwm.update_pair_set(pair_set_id='all_agilent',pair_ids=pair[pair[\"case_sample\"].isin(sam[sam['baits']==\"AGILENT\"].index.tolist())].index)\n",
    "refwm.update_pair_set(pair_set_id='all_ice',pair_ids=pair[pair[\"case_sample\"].isin([i for i in sam[(sam['baits'] == \"ICE\") |(sam['baits'].isna())].index.tolist() if i != 'nan'])].index)\n",
    "#creating a sample set\n",
    "refwm.update_sample_set(sample_set_id=samplesetname, sample_ids=samples.index)\n",
    "refwm.update_sample_set(sample_set_id='all', sample_ids=[i for i in sam.index.tolist() if i!='nan'])\n",
    "refwm.update_sample_set(sample_set_id='all_agilent', sample_ids = sam[sam['baits'] == \"AGILENT\"].index.tolist())\n",
    "refwm.update_sample_set(sample_set_id='all_ice', sample_ids=[i for i in sam[(sam['baits'] == \"ICE\") |(sam['baits'].isna())].index.tolist() if i != 'nan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported 385 participants.\n",
      "Successfully imported 414 samples.\n",
      "Successfully imported 414 pairss.\n",
      "Successfully imported 1 pair sets:\n",
      "  * 20Q4 (414 pairs)\n",
      "Pair set \"all\" (2333 pairs) successfully updated.\n",
      "Pair set \"all_agilent\" (1403 pairs) successfully updated.\n",
      "Pair set \"all_ice\" (930 pairs) successfully updated.\n",
      "Successfully imported 1 sample sets:\n",
      "  * 20Q4 (414 samples)\n",
      "Sample set \"all\" (2333 samples) successfully updated.\n",
      "Sample set \"all_agilent\" (1403 samples) successfully updated.\n",
      "Sample set \"all_ice\" (930 samples) successfully updated.\n"
     ]
    }
   ],
   "source": [
    "#and CN\n",
    "cnwm = dm.WorkspaceManager('broad-firecloud-ccle/DepMap_WES_CN_hg38')\n",
    "cnwm = cnwm.disable_hound()\n",
    "cnwm.upload_samples(samples)\n",
    "cnwm.upload_entities('pairs', pairs)\n",
    "cnwm.update_pair_set(pair_set_id=samplesetname,pair_ids=pairs.index)\n",
    "sam = cnwm.get_samples()\n",
    "\n",
    "pair = cnwm.get_pairs()\n",
    "cnwm.update_pair_set(pair_set_id='all',pair_ids=pair.index)\n",
    "cnwm.update_pair_set(pair_set_id='all_agilent',pair_ids=pair[pair[\"case_sample\"].isin(sam[sam['baits']==\"AGILENT\"].index.tolist())].index)\n",
    "cnwm.update_pair_set(pair_set_id='all_ice',pair_ids=pair[pair[\"case_sample\"].isin([i for i in sam[(sam['baits'] == \"ICE\") |(sam['baits'].isna())].index.tolist() if i != 'nan'])].index)\n",
    "#creating a sample set\n",
    "cnwm.update_sample_set(sample_set_id=samplesetname, sample_ids=samples.index)\n",
    "cnwm.update_sample_set(sample_set_id='all', sample_ids=[i for i in sam.index.tolist() if i!='nan'])\n",
    "cnwm.update_sample_set(sample_set_id='all_agilent', sample_ids = sam[sam['baits'] == \"AGILENT\"].index.tolist())\n",
    "cnwm.update_sample_set(sample_set_id='all_ice', sample_ids=[i for i in sam[(sam['baits'] == \"ICE\") |(sam['baits'].isna())].index.tolist() if i != 'nan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run the pipeline\n",
    "\n",
    "We are using Dalmatian to send request to Terra, we are running a set of 5 functions To generate the mutation dataset:\n",
    "\n",
    "*   For new samples in DepMap, run the ICE version of this task. CCLE2 samples used Agilent targets, so this pipeline should be used instead. The pipelines are identical in terms of their outputs, but the proper targets, baits, and pseudo normal should be used based on how the samples were sequenced.\n",
    "\n",
    "    **ICE_CGA_Production_Analysis_Pipeline_Cell_Lines_copy** (cclf/CGA_Production_Analysis_Pipeline_Cell_Lines_debuggingSnapshot ID: 22) OR\n",
    "\n",
    "\n",
    "    **AGILENT_CGA_Production_Analysis_Pipeline_Cell_Lines** (cclf/CGA_Production_Anablysis_Pipeline_Cell_Lines_debuggingSnapshot ID: 22)\n",
    "\n",
    "*   **common_variant_filter** (breardon/common_variant_filterSnapshot ID: 3)\n",
    "*   **filterMAF_on_CGA_pipeline** (gkugener/filterMAF_on_CGA_pipelineSnapshot ID: 8)\n",
    "*   **aggregateMAFs_selectFields** (ccle_mg/aggregateMAFs_selectFieldsSnapshot ID: 1)\n",
    "\n",
    "This outputs to be downloaded will be saved in the sample set that was run. The output we use for the release is:\n",
    "\n",
    "\n",
    "*   **passedCGA_filteredMAF_aggregated** \n",
    "\n",
    "There are several other tasks in this workspace. In brief:\n",
    "\n",
    "\n",
    "\n",
    "*   **CGA_Production_Analysis_Pipeline_Cell_Lines** (lelagina/CGA_Production_Analysis_Pipeline_Cell_LinesSnapshot ID: 12). This task is the same as the ICE and AGILENT prefixed version above, except that it relied on pulling the baits and targets to use from the metadata stored for the samples. Having AGILENT and ICE versions specified made the uploading and running process easier.\n",
    "*   **SANGER_CGA_Production_Analysis_Pipeline_Cell_Lines** (cclf/CGA_Production_Analysis_Pipeline_Cell_Lines_debuggingSnapshot ID: 22). This task was trying to run the CGA pipeline on the Sanger WES data, using a Sanger pseudo normal. In its current implementation, this task fails to complete for the samples.\n",
    "*   **UNFILTERED_aggregateMAFs_selectFields** (ccle_mg/aggregateMAFs_selectFieldsSnapshot ID: 1). Aggregates the MAF outputted by the CGA cell line pipeline prior to the common variant filter and germline filtering tasks. This can give us insight to which mutations are getting filtered out when. We may want to potentially include this MAF in the release so people can see why certain mutations of interest may be getting filtered out.\n",
    "*   WES_DM_Mutation_Calling_Pipeline_(standard |expensive) (gkugener/WES_DM_Mutation_Calling_PipelineSnapshot ID: 2). This was a previous mutation calling pipeline implemented for CCLE. We do not use this pipeline any more as the CGA pipeline looks better.\n",
    "*   aggregate_filterMAF_CGA (CCLE/aggregate_filterMAF_CGASnapshot ID: 1). An aggregation MAF task that we used in the past. We do not use this task anymore.\n",
    "*   calculate_mutational_burden (breardon/calculate_mutational_burdenSnapshot ID: 21). This task can be used to calculate the mutational rate of the samples. We do not make use of this data in the release although it could be of interest.\n",
    "*   summarizeWigFile (breardon/summarizeWigFileSnapshot ID: 5). CCLF ran this task (might be necessary for the mutational burden task). For our workflow, we do not run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Terra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20Q4'"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samplesetname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created submission c4de93d4-c807-473b-966c-5f7436ce8ab2.\n"
     ]
    }
   ],
   "source": [
    "submission_id1 = refwm.create_submission(\"CGA_WES_CCLE_ICE\", samplesetname, 'sample_set', expression='this.samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Germline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created submission 97f48fee-8011-4112-b3ca-9dd5f036ba52.\n"
     ]
    }
   ],
   "source": [
    "submission_id2 = refwm.create_submission(\"cnn-variant-filter\", samplesetname, 'sample_set', expression='this.samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy pairs data to sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = refwm.get_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs[pairs.index.isin(tokeep)]\n",
    "pairs = pairs[~pairs['mutation_validator_validated_maf'].isna()]\n",
    "pairs = pairs.drop(columns=['case_sample','control_sample','participant_id'])\n",
    "pairs.index = [i.split('_')[0] for i in pairs.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refwm.update_sample_attributes(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created submission 2b7316ae-8d11-4cb9-9143-efd9c7354cb5.\n"
     ]
    }
   ],
   "source": [
    "terra.waitForSubmission(refworkspace, submission_id1)\n",
    "submission_id1 = refwm.create_submission(\"common_variant_filter\", samplesetname, 'sample_set', expression='this.samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created submission a7c71e70-dc82-4b95-92c5-3d98b7438837.\n"
     ]
    }
   ],
   "source": [
    "terra.waitForSubmission(refworkspace, submission_id2)\n",
    "submission_id2 = refwm.create_submission(\"aggregate_vcfs\", \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-414-a85e9f9ff183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mterra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitForSubmission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmission_id1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msubmission_id1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrefwm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filterMAF_on_CGA_pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplesetname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sample_set'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexpression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'this.samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/JKBio/TerraFunction.py\u001b[0m in \u001b[0;36mwaitForSubmission\u001b[0;34m(workspace, submissions, raise_errors)\u001b[0m\n\u001b[1;32m     84\u001b[0m           \u001b[0mdone\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status is: Done for \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" jobs in submission \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscount\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\". \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiming\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\",5 mn elapsed.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mtiming\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "terra.waitForSubmission(refworkspace, submission_id1)\n",
    "submission_id1 = refwm.create_submission(\"filterMAF_on_CGA_pipeline\", samplesetname,'sample_set',expression='this.samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 of jobs Succeeded in submission 0.\n",
      "Successfully created submission 8f3249b7-99a7-40f9-a4ed-010fbed48389.\n"
     ]
    }
   ],
   "source": [
    "terra.waitForSubmission(refworkspace, submission_id1)\n",
    "submission_id1 = refwm.create_submission(\"aggregateMAFs_selectFields_filtered\", \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created submission 381337f5-dbd0-4d0d-8d4b-eda938904ce7.\n"
     ]
    }
   ],
   "source": [
    "submission_id3 = refwm.create_submission(\"aggregateMAFs_selectFields_unfiltered\", \"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra.waitForSubmission(refworkspace, [submission_id1,submission_id2, submission_id3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the workflow configurations used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terra.saveConfigs(refworkspace,'./data/'+samplesetname+'/Mutconfig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On local\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove some datafile to save money¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = refwm.get_samples()\n",
    "toremove = [\"fixedmate_bam\"]\n",
    "for val in toremove:\n",
    "    refwm.disable_hound().delete_entity_attributes('sample', res[val], delete_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil -m rm \"gs://fc-secure-012d088c-f039-4d36-bde5-ee9b1b76b912/9e3cc501-3f08-47fb-87a5-0359febb833c/**/call-tumorMM_Task/*.cleaned.bam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes it does not work so better check again\n",
    "a = res.fixedmate_bam\n",
    "a = [i for i in a if i is not np.nan]\n",
    "gcp.rmFiles(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### downloading from terra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam = refwm.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nowes = set(mutations.DepMap_ID)-set(sam.arxspan_id)\n",
    "nowes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nothing = nows -set(ccle_refsamples.arxspan_id)\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(mutations[mutations.DepMap_ID.isin(nothing) & ~mutations.SangerWES_AC.isna()].DepMap_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = refwm.get_sample_sets().loc[\"all\"]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = res['filtered_CGA_MAF_aggregated']\n",
    "! gsutil cp $filtered \"temp/mutation_filtered_terra_merged.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get QC files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataMut = getWESQC(workspace=refworkspace ,only=[], qcname=[\"gatk_cnv_all_plots\", \"lego_plotter_pngs\", \"copy_number_qc_report\", \"ffpe_OBF_figures\", \"mut_legos_html\", \"oxoG_OBF_figures\", \"tumor_bam_base_distribution_by_cycle_metrics\", \"tumor_bam_converted_oxog_metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBam = getWESQC(workspace=refworkspace ,only=[], qcname=[ \"tumor_bam_alignment_summary_metrics\", \"tumor_bam_bait_bias_summary_metrics\", \"tumor_bam_gc_bias_summary_metrics\", \"tumor_bam_hybrid_selection_metrics\", \"tumor_bam_insert_size_histogram\", \"tumor_bam_insert_size_metrics\", \"tumor_bam_pre_adapter_summary_metrics\", \"tumor_bam_quality_by_cycle_metrics\", \"tumor_bam_quality_distribution_metrics\", \"tumor_bam_quality_yield_metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_refsamples = pd.read_csv('temp/newrefCN.csv',index_col=\"cds_sample_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in dataMut.items():\n",
    "    if k =='nan':\n",
    "        continue\n",
    "    new_refsamples.loc[k,'processing_qc'] = str(v) + ',' + new_refsamples.loc[k,'processing_qc']\n",
    "for k,v in dataBam.items():\n",
    "    if k =='nan':\n",
    "        continue\n",
    "    new_refsamples.loc[k,'bam_qc'] = str(v) + ',' + new_refsamples.loc[k,'bam_qc']\n",
    "new_refsamples.to_csv('temp/newrefWES.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieving unfiltered mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered = res['unfiltered_CGA_MAF_aggregated']\n",
    "! gsutil cp $unfiltered \"temp/mutation_unfiltered_terra_merged.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered = pd.read_csv('temp/mutation_unfiltered_terra_merged.txt', sep='\\t', encoding='L6',na_values=[\"__UNKNOWN__\",'.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toremove = []\n",
    "for val in unfiltered.columns:\n",
    "    if len(unfiltered[unfiltered[val]=='nan'])>len(unfiltered)*0.99:\n",
    "        toremove.append(val)\n",
    "    elif len(set(unfiltered[val])-set(['nan']))==1:\n",
    "        toremove.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered = unfiltered.drop(columns=[\"UniProt_Site\",\"alt_allele_seen\",\"CCLE_ONCOMAP_overlapping_mutations\",\"failure_reasons\",\"ESP_CA\",\"SVTYPE\",\"id\",\"gnomADg_GT\",\"ESP_GWAS_PUBMED\", 'dbSNP_Val_Status', 'qual', 'iHpol', 'QSI_ref', 'BCNoise', 'score', 'Familial_Cancer_Genes_Reference', 'NT']+toremove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered['somatic'] = unfiltered['somatic'].replace('nan','False')\n",
    "unfiltered['HGNC_Status'] = unfiltered['HGNC_Status'].replace('nan','Unapproved')\n",
    "unfiltered['judgement'] = unfiltered['judgement'].replace('nan','REMOVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toint =  [\"Start_position\", \"End_position\"]\n",
    "for val in toint:\n",
    "    unfiltered[val]  = unfiltered[val].astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieving RNAseq vcfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnamutations = dm.WorkspaceManager(rnaworkspace).get_sample_sets().loc['All_samples']['merged_vcf']\n",
    "! gsutil cp $rnamutations \"temp/rna_mutation_unfiltered_terra_merged.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieving germline mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### postprocessing\n",
    "\n",
    "\n",
    "Here, rather than rerunning the entire analysis, because we know we are adding only WES samples, we can download the previous release's MAF, add the samples, update any annotations, and perform any global filters at the end.\n",
    "\n",
    "First we need to do an additional step of filtering on coverage and number \n",
    "\n",
    "- readMutations\n",
    "- createSNPs\n",
    "- addToMainMutation\n",
    "- filterAllelicFraction\n",
    "- filterMinCoverage\n",
    "- mergeAnnotations\n",
    "- addAnnotation\n",
    "- maf_add_variant_annotations\n",
    "- mutation_maf_to_binary_matrix (x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('temp/mutation_filtered_terra_merged.txt',sep='\\t') \n",
    "print(file.columns[:10])\n",
    "renaming = removeOlderVersions(names = set(file['Tumor_Sample_Barcode']), refsamples = refwm.get_samples(), arxspan_id = \"arxspan_id\", version=\"version\")\n",
    "print(file[file['Chromosome']=='0'])\n",
    "file[file['Tumor_Sample_Barcode'].isin(renaming.keys())].replace({'Tumor_Sample_Barcode':renaming}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfiltered[unfiltered.DepMap_ID.isin(renaming.keys())].replace(renaming).rename(columns={'Tumor_Sample_Barcode':'DepMap_ID'}).to_csv('temp/mutation_unfiltered_terra_merged.csv.gz', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = ''\n",
    "for val in renaming.keys():\n",
    "    keep+= val+','\n",
    "pd.DataFrame(data=renaming).to_csv('temp/renaming.txt', sep=' ')\n",
    "keep = keep[:-1]\n",
    "! bcftools view -s $keep $rna_mutations > $ rna_mutations\n",
    "! bcftools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saving samples used for 20Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccle_refsamples.loc[renaming.keys(),version]=1\n",
    "new_refsamples.to_csv('temp/newrefWES.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "newly_merged_maf <- readMutations('temp/mutation_filtered_terra_merged.txt')\n",
    "new_release <- createSNPs(newly_merged_maf)\n",
    "names(new_release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "previous.release.maf <- load.from.taiga(data.name='depmap-mutations-maf-35fe', data.file=paste0('mutations.',prevname),data.version=prevversion)\n",
    "if (colnames(previous.release.maf)[1] == 'X1' || colnames(previous.release.maf)[1] == \"\") {\n",
    " previous.release.maf[,1] <- NULL \n",
    "}\n",
    "prevnames <- names(previous.release.maf)\n",
    "prevnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "merged <- addToMainMutation(previous.release.maf, new_release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "## Adding more\n",
    "newly_merged_maf <- readMutations('temp/mutation_filtered_terra_merged.txt')\n",
    "new_release <- createSNPs(newly_merged_maf)\n",
    "print(names(new_release))\n",
    "merged <- addToMainMutation(merged, new_release)\n",
    "nrow(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "filtered <- filterAllelicFraction(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "filtered <- filterMinCoverage(filtered$merged, filtered$removed_from_maf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "clean_annotations <- mergeAnnotations(merged,previous.release.maf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "# Guillaume's version\n",
    "new_release <- addAnnotation(filtered$merged, clean_annotations, colnames(previous.release.maf))\n",
    "# Allie's version\n",
    "new_release <- maf_add_variant_annotations(new_release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterCoverage(maf, loc=['CGA_WES_AC'], sep=':',cov=4):\n",
    "    muts=np.zeroes((len(maf),2))\n",
    "    for val in loc:\n",
    "        muts+= np.array([[v[0],0] if 'NA' in v else v for v in mutations_20Q2_all[val].fillna('0'+sep+'0').astype(str).str.split(sep).tolist()]).astype(int)\n",
    "    return maf[muts[:,1]>=cov]\n",
    "\n",
    "def filterAllelicFraction(maf, loc=['CGA_WES_AC'], sep=':',frac=0.3):\n",
    "    muts=np.zeroes((len(maf),2))\n",
    "    for val in loc:\n",
    "        muts+= np.array([[v[0],0] if 'NA' in v else v for v in mutations_20Q2_all[val].fillna('0'+sep+'0').astype(str).str.split(sep).tolist()]).astype(int)\n",
    "    muts = muts[:,0]/muts[:,1]\n",
    "    return maf[muts>=frac]\n",
    "\n",
    "def mergeAnnotations(newmaf, additionalmaf, additionalonmerge=[]):\n",
    "    on = ['Chromosome', 'Start_position', 'End_position', 'Reference_Allele', 'Tumor_Seq_Allele1']\n",
    "    on.extend(additionalonmerge)\n",
    "    \n",
    "    newmaf = newmaf.join(additionalmaf, on = on)\n",
    "    if \n",
    "    solve issues with Hugo_Symbol, Entrez_Gene_Id\n",
    "    \n",
    "    \n",
    "    \n",
    "    return newmad\n",
    "    \n",
    "def mergeXY():\n",
    "    dbSNP_RS.x, dbSNP_RS.y\n",
    "\n",
    "\n",
    "def addAnnotation(maf, NCBI_Build='37', Strand=\"+\"):\n",
    "    maf['NCBI_Build'] = NCBI_Build\n",
    "    maf['Strand'] = Strand\n",
    "    maf = maf[['current', 'SangerWES_AC', 'SangerRecalibWES_AC', 'RNAseq_AC', 'HC_AC', 'RD_AC', 'WGS_AC']\n",
    "\n",
    "def mafToMat(maf, col, boolify = False, samplesCol = \"DepMap_ID\", mutNameCol=\"Hugo_Symbol\"):\n",
    "    maf = maf.sort_values(by = mutNameCol)\n",
    "    samples = set(maf[samplesCol])\n",
    "    mut = pd.DataFrame(data = np.zeros((len(set(maf[mutNameCol])), 1)), columns=['fake'], index=set(maf[mutNameCol])).astype(float)\n",
    "    for i,val in enumerate(samples):\n",
    "        h.showcount(i,len(samples))\n",
    "        mut = mut.join(maf[maf[samplesCol]==val].drop_duplicates(mutNameCol).set_index(mutNameCol)[col].rename(val))\n",
    "    return mut.nan_to_num(0).astype(bool if boolify else float).drop(columns=['fake'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_mutations = filterCoverage(mutations)\n",
    "filtered_mutations = filterAllelicFraction(filtered_mutations)\n",
    "\n",
    "merged_mutations = addAnnotation(mutations)\n",
    "\n",
    "mafToMat(filtered_mutations[filtered_mutations.damaging]).to_csv('.csv')\n",
    "mafToMat(filtered_mutations[filtered_mutations.other]).to_csv('.csv')\n",
    "mafToMat(filtered_mutations[filtered_mutations.hotspot]).to_csv('.csv')\n",
    "\n",
    "\n",
    "CCLE2othermutations = \n",
    "\n",
    "mutations = mergeAnnotations(filtered_mutations, CCLE2othermutations)\n",
    "\n",
    "#making \n",
    "for muttype in ['']:\n",
    "    mafToMat(CCLE2othermutations[CCLE2othermutations.damaging & CCLE2othermutations[muttype]]).to_csv(''+muttype+\".csv\")\n",
    "    mafToMat(CCLE2othermutations[CCLE2othermutations.other & CCLE2othermutations[muttype]]).to_csv(''+muttype+\".csv\")\n",
    "    mafToMat(CCLE2othermutations[CCLE2othermutations.hotspot & CCLE2othermutations[muttype]]).to_csv(''+muttype+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to previous release\n",
    "\n",
    "I would run some checks here comparing the results to the previous releases MAF. Namely:\n",
    "\n",
    "- Count the total number of mutations per cell line, split by type (SNP, INS, DEL)\n",
    "- Count the total number of mutations observed by position (group by chromosome, start position, end position and count the number of mutations)\n",
    "- Look at specific differences between the two MAFs (join on DepMap_ID, Chromosome, Start position, End position, Variant_Type). I would do this for WES only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeremie/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "mutations = pd.read_csv('temp/mutations.'+release+'.all.csv')\n",
    "damaging_mutation = pd.read_csv('temp/damaging_mutation.'+release+'.all.csv')\n",
    "print(len(damaging_mutation))\n",
    "other_mutation = pd.read_csv('temp/other_mutation.'+release+'.all.csv')\n",
    "print(len(other_mutation))\n",
    "hotspot_mutation = pd.read_csv('temp/hotspot_mutation.'+release+'.all.csv')\n",
    "print(len(hotspot_mutation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(mutations.DepMap_ID) - set(mutations[~(mutations['CGA_WES_AC'].isna() & mutations['SangerWES_AC'].isna() & mutations['WGS_AC'].isna() & mutations['SangerRecalibWES_AC'].isna())].DepMap_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations[mutations.DepMap_ID==\"ACH-000458\"].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations[mutations[\"Hugo_Symbol\"]==\"ACOT4\"][mutations['Start_position']==74058831]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_data = mutations[[val for val in mutations.columns.values if '_AC' in val]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_names = ac_data.columns.values\n",
    "ac_data = ac_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some checks and manual rescuing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations[mutations.DepMap_ID==\"ACH-003000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check important mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check MOLM13, MV411 cell lines- The well known mutation status of FLT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check TP53 mutation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toofew = 0\n",
    "allnan = 0\n",
    "for pos, val in enumerate(ac_data):\n",
    "    i = 0\n",
    "    print(str(100*pos/ac_data.shape[0]),end='\\r')\n",
    "    for p, v in enumerate(val):\n",
    "        if v is np.nan:\n",
    "            i+=1\n",
    "    if i==7:\n",
    "        mutations = mutations.drop[pos]\n",
    "        allnan+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the total number of mutations per cell line, split by type (SNP, INS, DEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of mutations observed by position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are mutation consistent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  to check this, if you group all the mutations in the mutations table by Chromosome, Start_position, End_position, Reference_Allele, Tumor_Seq_Allele1 columns, they should all have the same annotation for the other columns (protein change, exac_af, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QC mutations, for a known dependency, check if it matches mutation of this gene. (if P53 is mutated, cannot have dependency on P53 or MDM2 MDM4/ inverse fir BRAF and KRAF to themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevprevname,prevprevversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutations[mutations.DepMap_ID==\"ACH-001546\"][mutations.columns[-17:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevprev= set(tc.get(name='depmap-mutation-calls-9be3', file= \"depmap_\"+prevprevname+\"_mutation_calls\", version = prevprevversion).DepMap_ID.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uploading on taiga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsheets = sheets.get(sheeturl).sheets[6].to_frame()\n",
    "wes_dmc_embargo = [i for i in gsheets['WES_DMC_embargo'].values.tolist() if str(i) != \"nan\"]\n",
    "wes_embargo = [i for i in gsheets['WES_embargo'].values.tolist() if str(i) != \"nan\"]\n",
    "blacklist = [i for i in gsheets['blacklist'].values.tolist() if str(i) != \"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wes_embargo, wes_dmc_embargo, blacklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "local_metadata": {},
    "remote_metadata": {
     "scrolled": true
    }
   },
   "outputs": [],
   "source": [
    "! cd .. && git clone https://github.com/broadinstitute/depmap-release-readmes.git && cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd ../depmap-release-readmes && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../depmap-release-readmes/ && python3 make_new_release.py $release && git add . && git commit -m $release && git push "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('cd ../depmap-release-readmes && git pull && mv release-'+release+'/internal-'+release+'.txt ../ccle_processing/temp/README && cd -')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.update_dataset(dataset_permaname=\"depmap-mutations-maf-35fe\",\n",
    "                 upload_file_path_dict={'temp/mutations.'+release+'.all.csv': 'TableCSV'}, \n",
    "                 dataset_description=\"\"\"\n",
    "# Mutations\n",
    "\n",
    "filtered and unfiltered mutation files from Broad WES and Sanger WES data mapped to hg19\n",
    "The MAF file for DepMap that includes all of the latest WES samples. This MAF is generated by merging CCLE (WGS, RNAseq, RD, HC) and Sanger (WES) data.\n",
    "\n",
    "PORTAL TEAM SHOULD NOT USE THIS: There are lines here that should not make it even to internal. Must use subsetted dataset instead. These data will not make it on the portal starting 19Q1. With the DMC portal, there is new cell line release prioritization as to which lines can be included, so a new taiga dataset will be created containing CN for the portal.\n",
    "\n",
    "version 1:  In 19Q1 the WES_AC column has been replaced by two columns, VA_WES_AC and CGA_WES_AC. We are currently using the Van Allen and CGA based pipeline to generate mutation calls. The CGA pipeline includes more filtering on the MAFs than VA and has a better INDEL caller. However, some of these filters may be removing some variants of interest that are still capture by the VA pipeline, which is why both a retained for now. DEPRECATED:  Missing the VA_WES_AC, CGA_WES_AC columns\n",
    "version 2: 19Q1 data\n",
    "version 3: 19Q2 data. We are no longer using the CCLE_WES_AC column. We are only using the CGA pipeline for mutation calls.\n",
    "version 4: Updating to 19Q3interim DEPRECATED\n",
    "version 5: Updating to 19Q3interim DEPRECATED\n",
    "version 6: Updating to 19Q3interim\n",
    "version 7: Updating to 19Q3 DEPRECATED\n",
    "version 8: reparing the missing mutation problem DEPRECATED\n",
    "version 9: reparing the missing column problem\n",
    "\n",
    "\n",
    "version10:\n",
    "Adding 52 new cell lines. \n",
    "Some cells lines have been flagged as:\n",
    "\n",
    "version11:\n",
    "adding missing cell lines\n",
    "\n",
    "Adding 52 new cell lines. \n",
    "Some cells lines have been flagged as:\n",
    "\n",
    " - having bad looking copy ration plots = \n",
    " - Genes having a similar CN value accross all []\n",
    "\n",
    "version 12:\n",
    "\n",
    "adding 8 new cell lines\n",
    "\n",
    "version 13:\n",
    "\n",
    "removing a wrong column\n",
    "\n",
    "version 14:\n",
    "\n",
    "adding 8 new cell lines. Adding .all. since we are soon going to release a restricted set of mutations. this one contains everything which is not necessarily what we want\n",
    "\n",
    "\n",
    "genes (gene rpkm):\n",
    "__Rows__:\n",
    "__Columns__:\n",
    "Counts (gene counts):\n",
    "__Rows__:\n",
    "__Columns__:\n",
    "Gene level CN data:\n",
    "__Rows__:\n",
    "__Columns__:\n",
    " DepMap cell line IDs\n",
    " gene names in the format HGNC\\_symbol (Entrez\\_ID)\n",
    "DepMap\\_ID, Chromosome, Start, End, Num\\_Probes, Segment\\_Mean\n",
    " \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspot_mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevmut = tc.get(name='depmap-mutation-calls-9be3', version=24, file='depmap_'+prevname+'_mutation_calls.all')\n",
    "print('shoud be None')\n",
    "print(set(prevmut.DepMap_ID) - set(mutations.DepMap_ID))\n",
    "print(\"new lines\")\n",
    "newlines = set(mutations.DepMap_ID) - set(prevmut.DepMap_ID) \n",
    "newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.update_dataset(dataset_permaname=\"depmap-mutation-calls-9be3\",\n",
    "                 upload_file_path_dict={'temp/depmap_'+release+'_mutation_calls.all': 'TableCSV',\n",
    "                                        'temp/damaging_mutation.all': 'NumericMatrixCSV',\n",
    "                                        'temp/other_mutation.all': 'NumericMatrixCSV',\n",
    "                                        'temp/hotspot_mutation.all': 'NumericMatrixCSV',\n",
    "                                       },#'temp/README': 'Raw'},\n",
    "                 dataset_description=\"\"\"\n",
    "# Internal Mutations\n",
    "\n",
    "Mutation calls for Internal DepMap data\n",
    "\n",
    "* Version 1 Internal 18Q1*\n",
    "\n",
    "original source: `/xchip/ccle_dist/broad_only/CMAG/mutations/CCLE_depMap_18Q1_maf_20180202.txt`\n",
    "* Version 2-4 Internal 18Q2*\n",
    "\n",
    "merged mutations and indels file (1,606 cell lines, including CCLE and Sanger WES reanalysis)\n",
    "original source: \n",
    "`/xchip/ccle_dist/broad_only/CMAG/mutations/CCLE_depMap_18q2_maf_20180502.txt`\n",
    "Binary matrices:\n",
    "- damaging: if isDeleterious is true\n",
    "- missense: if isDeleterious is false\n",
    "- hotspot: if missense and either TCGA or COSMIC hotspot\n",
    "Version 2 contains the MAF file\n",
    "* Version 5-6 Internal 18Q3*\n",
    "\n",
    "version 5 deprecated\n",
    "\n",
    "original source: `/xchip/ccle_dist/broad_only/CMAG/mutations/CCLE_depMap_18q3_maf_20180716.txt`\n",
    "\n",
    "Binary matrices:\n",
    "- damaging: if isDeleterious is true\n",
    "- missense: if isDeleterious is false\n",
    "- hotspot: if missense and either TCGA or COSMIC hotspot\n",
    "- Rows: cell line, Broad (arxspan) IDs\n",
    "\n",
    "Columns: Gene, HGNC symbol (Entrez ID)\n",
    "\n",
    "MAF file\n",
    "\n",
    "* Version 7-8 Internal 18Q4*\n",
    "\n",
    "version 8 just changes a column name in the MAF file from Broad_ID to DepMap_ID\n",
    "\n",
    "original source: `/xchip/ccle_dist/broad_only/CMAG/mutations/CCLE_DepMap_18Q4_maf_20181028.txt`\n",
    "\n",
    "* Version 9-12 Internal 19Q1*\n",
    "\n",
    "version 12 updates the column name from VA_WES_AC to CCLE_WES_AC\n",
    "\n",
    "version 11+ uses an updated definition for hotspot mutations\n",
    "\n",
    "version 12 contains the correct data for 19Q1\n",
    "\n",
    "* Version 13 Internal 19Q2*\n",
    "\n",
    "* Version 14-15 Internal 19Q3*\n",
    "\n",
    "version 15 fixed entrez ids\n",
    "\n",
    "* Version 16 Internal 19Q4*\n",
    "\n",
    "adding 35 new cell lines.\n",
    "\n",
    "* Version 16 Internal 19Q4*\n",
    "uploading as matrices\n",
    "\n",
    "* Version 17 Internal 19Q4*\n",
    "removing unauthorized lines and setting as matrices\n",
    "\n",
    "* Version 18 Internal 19Q4*\n",
    "removing unauthorized lines and setting as matrices\n",
    "\n",
    "* Version 19 Internal 20Q1*\n",
    "uploading 8 new lines\n",
    "\n",
    "* Version 20 Internal 20Q1*\n",
    "removing unauthorized cl\n",
    "\n",
    "* Version 21 Internal 20Q2*\n",
    "uploading 8 new lines and adding .all to express the fact that this data is the aggregate of all different sequencing methods.\n",
    "\n",
    "* Version 22 Internal 20Q2*\n",
    "removing 2 cell lines\n",
    "\n",
    "* Version 23 Internal 20Q3*\n",
    "nothing different from 20Q2. no new cell lines\n",
    "\n",
    "* Version 24 Internal 20Q2*\n",
    "updating the blacklists\n",
    "\n",
    "*** Variant annotation column ***\n",
    "\n",
    "MAF file, added column (Variant_annotation) classifying each variant as either silent, damaging, other conserving, or other non-conserving, based on this mapping (old annotation from Variant_Classification column - new annotation):\n",
    "\n",
    "Silent - silent\n",
    "Splice_Site - damaging\n",
    "Missense_Mutation - other non-conserving\n",
    "Nonsense_Mutation - damaging\n",
    "De_novo_Start_OutOfFrame - damaging\n",
    "Nonstop_Mutation - other non-conserving\n",
    "Frame_Shift_Del - damaging\n",
    "Frame_Shift_Ins - damaging\n",
    "In_Frame_Del - other non-conserving\n",
    "In_Frame_Ins - other non-conserving\n",
    "Stop_Codon_Del - other non-conserving\n",
    "Stop_Codon_Ins - other non-conserving\n",
    "Start_Codon_SNP - damaging\n",
    "Start_Codon_Del - damaging\n",
    "Start_Codon_Ins - damaging\n",
    "5'Flank - other conserving\n",
    "Intron - other conserving\n",
    "IGR - other conserving\n",
    "3'UTR - other conserving\n",
    "5'UTR - other conserving\n",
    "Binary matrices:\n",
    "\n",
    "- damaging: if damaging\n",
    "- other: if other conserving or other non-conserving\n",
    "- hotspot: if it is not a silent mutation and is either TCGA or COSMIC hotspot\n",
    "- Rows: cell line, DepMap (arxspan) IDs\n",
    "\n",
    "Columns: Gene, HGNC symbol (Entrez ID)\n",
    "\n",
    "NEW LINES:\n",
    "\"\"\"+newlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add to a virtual dataset\n",
    "AddToVirtual(virtual_internal, 'depmap-mutation-calls-9be3', [('CCLE_mutations', 'depmap_'+release+'_mutation_calls'),])#('README','README')])\n",
    "# To add to a eternal dataset\n",
    "AddToVirtual('depmap-a0ab', 'depmap-mutation-calls-9be3', [('CCLE_mutations', 'depmap_'+release+'_mutation_calls')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('cd ../depmap-release-readmes && git pull && mv release-'+releAse+'/dmc-'+releAse+'.txt ../ccle_processing/temp/README && cd -')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1301656\n",
      "1297418\n",
      "1757\n",
      "1744\n",
      "1758\n",
      "1745\n",
      "1725\n",
      "1712\n"
     ]
    }
   ],
   "source": [
    "print(len(mutations))\n",
    "mutations = mutations[~mutations.DepMap_ID.isin(wes_embargo)]\n",
    "print(len(mutations))\n",
    "mutations.to_csv('temp/depmap_'+release+'_mutation_calls.all', index=False)\n",
    "print(len(damaging_mutation))\n",
    "damaging_mutation = damaging_mutation[~damaging_mutation.index.isin(wes_embargo)]\n",
    "print(len(damaging_mutation))\n",
    "damaging_mutation.to_csv('temp/damaging_mutation.all')\n",
    "print(len(other_mutation))\n",
    "other_mutation = other_mutation[~other_mutation.index.isin(wes_embargo)]\n",
    "print(len(other_mutation))\n",
    "other_mutation.to_csv('temp/other_mutation.all',)\n",
    "print(len(hotspot_mutation))\n",
    "hotspot_mutation = hotspot_mutation[~hotspot_mutation.index.isin(wes_embargo)]\n",
    "print(len(hotspot_mutation))\n",
    "hotspot_mutation.to_csv('temp/hotspot_mutation.all',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[##################]100% |  40.4 MiB/s | 277.3 MiB / 277.3 MiB | Time:  0:00:06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shoud be None\n",
      "set()\n",
      "new lines\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ACH-001533', 'ACH-001574', 'ACH-002021', 'ACH-002065'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevmut = tc.get(name='depmap-mutation-calls-dfce', version=15, file='depmap_'+prevname+'_mutation_calls')\n",
    "print('shoud be None')\n",
    "print(set(prevmut.DepMap_ID) - set(mutations.DepMap_ID))\n",
    "print(\"new lines\")\n",
    "newlines = set(mutations.DepMap_ID) - set(prevmut.DepMap_ID) \n",
    "newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.update_dataset(dataset_permaname=\"depmap-mutation-calls-dfce\",\n",
    "                 upload_file_path_dict={'temp/depmap_'+release+'_mutation_calls.all': 'TableCSV',\n",
    "                                        'temp/damaging_mutation.all': 'NumericMatrixCSV',\n",
    "                                        'temp/other_mutation.all': 'NumericMatrixCSV',\n",
    "                                        'temp/hotspot_mutation.all': 'NumericMatrixCSV',\n",
    "                                       },#'temp/README': 'Raw'},\n",
    "                 dataset_description=\"\"\"\n",
    "# DMC Mutations\n",
    "\n",
    "* Version 1-5 DMC 19Q1*\n",
    "\n",
    "version 5 is a one-off portal thing because dmc wanted to be able to plot if a gene has any mutation as one-hot encoded value in the x/y axes of the data explorer It adds the any_mutation matrix, but does not change the others. Code used to generate:\n",
    "\n",
    "```\n",
    "from taigapy import TaigaClient\n",
    "\n",
    "c = TaigaClient()\n",
    "\n",
    "dmc_19q1_mutation_taiga_root = \"depmap-mutation-calls-dfce.3/\"\n",
    "other_matrix = c.get(dmc_19q1_mutation_taiga_root + \"other_mutation\")\n",
    "damaging_matrix = c.get(dmc_19q1_mutation_taiga_root + \"damaging_mutation\")\n",
    "hotspot_matrix = c.get(dmc_19q1_mutation_taiga_root + \"hotspot_mutation\")\n",
    "\n",
    "df = other_matrix.append(damaging_matrix)\n",
    "df = df.groupby(level=0).sum()\n",
    "\n",
    "df = df.append(hotspot_matrix)\n",
    "df = df.groupby(level=0).sum()\n",
    "\n",
    "df[df > 1] = 1\n",
    "\n",
    "df.to_csv('any_mutation.csv')\n",
    "```\n",
    "The code uses version 3 because the dmc portal was using version 3\n",
    "\n",
    "version 4 updates the column name from VA_WES_AC to CCLE_WES_AC\n",
    "\n",
    "version 3 has an updated definition for hotspot mutations\n",
    "\n",
    "version 2+ contains the correct data for 19Q1\n",
    "\n",
    "* Version 6 DMC 19Q2*\n",
    "\n",
    "* Version 7-8 DMC 19Q3*\n",
    "version 8 fixed entrez ids\n",
    "\n",
    "* Version 9 DMC 19Q4*\n",
    "adding 52 new cell lines.\n",
    "\n",
    "* Version 10 DMC 19Q4*\n",
    "removing unauthorized lines and setting as matrices\n",
    "\n",
    "* Version 11 DMC 19Q4*\n",
    "removing unauthorized lines and setting as matrices\n",
    "\n",
    "* Version 12 Internal 20Q1*\n",
    "uploading 8 new lines\n",
    "\n",
    "* Version 13 Internal 20Q1*\n",
    "removing unauthorized cl\n",
    "\n",
    "* Version 14 Internal 20Q2*\n",
    "uploading 8 new lines and adding .all to express the fact that this data is the aggregate of all different sequencing methods.\n",
    "\n",
    "* Version 15 Internal 20Q2*\n",
    "removing 2 lines\n",
    "\n",
    "* Version 15 Internal 20Q3*\n",
    "nothing different from 20Q2. no new cell lines\n",
    "\n",
    "* Version 15 Internal 20Q3*\n",
    "updating the blacklists\n",
    "\n",
    "\n",
    "MAF file, added column (Variant_annotation) classifying each variant as either silent, damaging, other conserving, or other non-conserving, based on this mapping (old annotation from Variant_Classification column - new annotation):\n",
    "\n",
    "Silent - silent\n",
    "Splice_Site - damaging\n",
    "Missense_Mutation - other non-conserving\n",
    "Nonsense_Mutation - damaging\n",
    "De_novo_Start_OutOfFrame - damaging\n",
    "Nonstop_Mutation - other non-conserving\n",
    "Frame_Shift_Del - damaging\n",
    "Frame_Shift_Ins - damaging\n",
    "In_Frame_Del - other non-conserving\n",
    "In_Frame_Ins - other non-conserving\n",
    "Stop_Codon_Del - other non-conserving\n",
    "Stop_Codon_Ins - other non-conserving\n",
    "Start_Codon_SNP - damaging\n",
    "Start_Codon_Del - damaging\n",
    "Start_Codon_Ins - damaging\n",
    "5'Flank - other conserving\n",
    "Intron - other conserving\n",
    "IGR - other conserving\n",
    "3'UTR - other conserving\n",
    "5'UTR - other conserving\n",
    "Binary matrices:\n",
    "- damaging: if damaging\n",
    "- other: if other conserving or other non-conserving\n",
    "- hotspot: if it is not a silent mutation and is either TCGA or COSMIC hotspot\n",
    "- Rows: cell line, DepMap (arxspan) IDs\n",
    "\n",
    "Columns: Gene, HGNC symbol (Entrez ID)\n",
    "\n",
    "NEW LINES:\n",
    "\"\"\"+newlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add to a virtual dataset\n",
    "AddToVirtual(virtual_dmc, 'depmap-mutation-calls-dfce', [('CCLE_mutations', 'depmap_'+release+'_mutation_calls'),])#('README','README')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('cd ../depmap-release-readmes && git pull && mv release-'+releAse+'/public-'+releAse+'.txt README && cd -')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#damaging_mutation\n",
    "mutations=depmap_20Q3_mutation_calls\n",
    "#hotspot_mutation\n",
    "#other_mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1297418\n",
      "1297418\n",
      "1744\n",
      "1744\n",
      "1745\n",
      "1745\n",
      "1712\n",
      "1712\n"
     ]
    }
   ],
   "source": [
    "print(len(mutations))\n",
    "mutations = mutations[mutations.DepMap_ID.isin(prevprev)]\n",
    "mutations = mutations[~mutations.DepMap_ID.isin(wes_dmc_embargo)]\n",
    "print(len(mutations))\n",
    "mutations.to_csv('temp/depmap_'+release+'_mutation_calls.all', index=False)\n",
    "print(len(damaging_mutation))\n",
    "damaging_mutation = damaging_mutation[damaging_mutation.index.isin(prevprev)]\n",
    "damaging_mutation = damaging_mutation[~damaging_mutation.index.isin(wes_dmc_embargo)]\n",
    "print(len(damaging_mutation))\n",
    "damaging_mutation.to_csv('temp/damaging_mutation.all')\n",
    "print(len(other_mutation))\n",
    "other_mutation = other_mutation[other_mutation.index.isin(prevprev)]\n",
    "other_mutation = other_mutation[~other_mutation.index.isin(wes_dmc_embargo)]\n",
    "print(len(other_mutation))\n",
    "other_mutation.to_csv('temp/other_mutation.all')\n",
    "print(len(hotspot_mutation))\n",
    "hotspot_mutation = hotspot_mutation[hotspot_mutation.index.isin(prevprev)]\n",
    "hotspot_mutation = hotspot_mutation[~hotspot_mutation.index.isin(wes_dmc_embargo)]\n",
    "print(len(hotspot_mutation))\n",
    "hotspot_mutation.to_csv('temp/hotspot_mutation.all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shoud be None\n",
      "set()\n",
      "new lines\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ACH-001533', 'ACH-001574', 'ACH-002021', 'ACH-002065'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prevmut = tc.get(name='depmap-mutation-calls-9a1a', version=18, file='depmap_'+prevname+'_mutation_calls')\n",
    "print('shoud be None')\n",
    "ermgency_removed = set(prevmut.DepMap_ID) - set(mutations.DepMap_ID)\n",
    "print(ermgency_removed) \n",
    "print(\"new lines\")\n",
    "newlines = set(mutations.DepMap_ID) - set(prevmut.DepMap_ID) \n",
    "newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading depmap_20Q3_mutation_calls...\n",
      "hitting https://cds.team/taiga/api/datafile/00b151ca35a14f6d9f1be95ef24ea368\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: depmap_20Q3_mutation_calls properly converted and uploaded\n",
      "Uploading damaging_mutation...\n",
      "hitting https://cds.team/taiga/api/datafile/00b151ca35a14f6d9f1be95ef24ea368\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: damaging_mutation properly converted and uploaded\n",
      "Uploading other_mutation...\n",
      "hitting https://cds.team/taiga/api/datafile/00b151ca35a14f6d9f1be95ef24ea368\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 500\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1000\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Conversion in progress, line 1500\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: other_mutation properly converted and uploaded\n",
      "Uploading hotspot_mutation...\n",
      "hitting https://cds.team/taiga/api/datafile/00b151ca35a14f6d9f1be95ef24ea368\n",
      "Conversion and upload...:\n",
      "\t Downloading the file from S3\n",
      "\t Downloading the file from S3\n",
      "\t Scanning through file to determine size (line 1001)\n",
      "\t Conversion in progress, line 250\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 750\n",
      "\t Conversion in progress, line 1250\n",
      "\t Conversion in progress, line 1500\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\t Uploading to S3\n",
      "\n",
      "\t Done: hotspot_mutation properly converted and uploaded\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id 8cc4900a50874d8593b0bfc591001360 created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/8cc4900a50874d8593b0bfc591001360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'8cc4900a50874d8593b0bfc591001360'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description=\"\"\"\n",
    "# Public Mutations\n",
    "\n",
    "Mutation calls for Public DepMap data\n",
    "\n",
    "* Version 1 Public 18Q1*\n",
    "\n",
    "original source: CCLE data portal\n",
    "* Version 2 Public 18Q2*\n",
    "\n",
    "merged mutations and indels file (1,549 cell lines total, including data for 63 newly released cell lines)\n",
    "original source: `/xchip/ccle_dist/public/DepMap_18Q2/CCLE_DepMap_18Q2_maf_20180502.txt`\n",
    "* Version 3-4 Public 18Q3*\n",
    "\n",
    "version 3 deprecated\n",
    "\n",
    "original source: `/xchip/ccle_dist/public/DepMap_18Q3/CCLE_DepMap_18q3_maf_20180718.txt`\n",
    "\n",
    "Binary matrices:\n",
    "damaging: if isDeleterious is true\n",
    "missense: if isDeleterious is false\n",
    "hotspot: if missense and either TCGA or COSMIC hotspot\n",
    "Rows: cell line, Broad (arxspan) IDs\n",
    "\n",
    "Columns: Gene, HGNC symbol (Entrez ID)\n",
    "\n",
    "MAF file\n",
    "\n",
    "* Version 5 Public 18Q4*\n",
    "\n",
    "original source: `/xchip/ccle_dist/public/DepMap_18Q4/CCLE_DepMap_18q4_maf_20181029.txt`\n",
    "\n",
    "* Version 6-9 Public 19Q1*\n",
    "\n",
    "version 9 updates the column name from VA_WES_AC to CCLE_WES_AC\n",
    "\n",
    "version 8 uses an updated definition for hotspot mutations\n",
    "\n",
    "version 9 contains the correct data for 19Q1\n",
    "\n",
    "* Version 10 Public 19Q2*\n",
    "\n",
    "* Version 11-12 Public 19Q3*\n",
    "\n",
    "version 12 fixed entrez ids\n",
    "\n",
    "* Version 13 Public 19Q4*\n",
    "\n",
    "adding 52 new cell lines\n",
    "\n",
    "* Version 14 Public 19Q4*\n",
    "removing unauthorized lines and setting matrices\n",
    "\n",
    "* Version 15 Public 20Q1*\n",
    "adding 8 new lines \n",
    "\n",
    "* Version 16 Public 20Q1*\n",
    "removing an unauthorized line\n",
    "\n",
    "* Version 17 Internal 20Q2*\n",
    "uploading 8 new lines and adding .all to express the fact that this data is the aggregate of all different sequencing methods.\n",
    "\n",
    "* Version 18 Internal 20Q2*\n",
    "removing 2 lines\n",
    "\n",
    "* Version 19 Internal 20Q3*\n",
    "nothing different from 20Q2. no new cell lines\n",
    "\n",
    "* Version 20 Internal 20Q3*\n",
    "updating the blacklists\n",
    "\n",
    "* Version 21 Internal 20Q3*\n",
    "updating the dmc\n",
    "\n",
    "* Version 22 Internal 20Q3*\n",
    "readding two already released samples to the public list\n",
    "\n",
    "MAF file, added column (Variant_annotation) classifying each variant as either silent, damaging, other conserving, or other non-conserving, based on this mapping (old annotation from Variant_Classification column - new annotation):\n",
    "\n",
    "Silent - silent\n",
    "Splice_Site - damaging\n",
    "Missense_Mutation - other non-conserving\n",
    "Nonsense_Mutation - damaging\n",
    "De_novo_Start_OutOfFrame - damaging\n",
    "Nonstop_Mutation - other non-conserving\n",
    "Frame_Shift_Del - damaging\n",
    "Frame_Shift_Ins - damaging\n",
    "In_Frame_Del - other non-conserving\n",
    "In_Frame_Ins - other non-conserving\n",
    "Stop_Codon_Del - other non-conserving\n",
    "Stop_Codon_Ins - other non-conserving\n",
    "Start_Codon_SNP - damaging\n",
    "Start_Codon_Del - damaging\n",
    "Start_Codon_Ins - damaging\n",
    "5'Flank - other conserving\n",
    "Intron - other conserving\n",
    "IGR - other conserving\n",
    "3'UTR - other conserving\n",
    "5'UTR - other conserving\n",
    "Binary matrices:\n",
    "\n",
    "- damaging: if damaging\n",
    "- other: if other conserving or other non-conserving\n",
    "- hotspot: if it is not a silent mutation and is either TCGA or COSMIC hotspot\n",
    "- Rows: cell line, DepMap (arxspan) IDs\n",
    "\n",
    "Columns: Gene, HGNC symbol (Entrez ID)\n",
    "\n",
    "NEW LINES:\n",
    "\"\"\"+str(newlines)\n",
    "\n",
    "if len(ermgency_removed):\n",
    "    description+=\"\"\"\n",
    "    \n",
    "    !! WE REMOVED!!:\n",
    "    \"\"\"+str(ermgency_removed)\n",
    "\n",
    "tc.update_dataset(dataset_permaname=\"depmap-mutation-calls-9a1a\",\n",
    "                 upload_file_path_dict={'temp/depmap_'+release+'_mutation_calls.all': 'TableCSV',\n",
    "                                        'temp/damaging_mutation.all': 'NumericMatrixCSV',\n",
    "                                        'temp/other_mutation.all': 'NumericMatrixCSV',\n",
    "                                        'temp/hotspot_mutation.all': 'NumericMatrixCSV',\n",
    "                                       },#'temp/README': 'Raw'},\n",
    "                 dataset_description=description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CCLE_mutations', 'depmap-mutation-calls-9a1a.22/depmap_20Q3_mutation_calls'), ('CCLE_gene_cn', 'depmap-wes-cn-data-97cc.34/public_20Q3_gene_cn'), ('Achilles_gene_effect_unscaled', 'avana-public-tentative-20q3-3e73.5/gene_effect_unscaled'), ('Achilles_high_variance_genes', 'avana-public-tentative-20q3-3e73.5/high_variance_genes'), ('Achilles_guide_efficacy', 'avana-public-tentative-20q3-3e73.5/guide_efficacy'), ('CCLE_fusions_unfiltered', 'gene-fusions-6212.14/unfiltered_fusions_20Q3'), ('common_essentials', 'avana-public-tentative-20q3-3e73.5/essential_genes'), ('Achilles_logfold_change_failures', 'avana-public-tentative-20q3-3e73.5/logfold_change_failures'), ('CCLE_expression', 'depmap-rnaseq-expression-data-ccd0.25/public_20Q3_proteincoding_tpm'), ('Achilles_raw_readcounts', 'avana-public-tentative-20q3-3e73.5/raw_readcounts'), ('Achilles_raw_readcounts_failures', 'avana-public-tentative-20q3-3e73.5/raw_readcounts_failing'), ('README', 'public-20q3-3d35.22/README'), ('CCLE_segment_cn', 'depmap-wes-cn-data-97cc.34/public_20Q3_segs_cn'), ('CCLE_RNAseq_transcripts', 'depmap-rnaseq-expression-data-ccd0.25/public_20Q3_transcripts_tpm'), ('nonessentials', 'avana-public-tentative-20q3-3e73.5/nonessential_genes'), ('Achilles_guide_map', 'avana-public-tentative-20q3-3e73.5/guide_gene_map'), ('Achilles_dropped_guides', 'avana-public-tentative-20q3-3e73.5/dropped_guides'), ('CCLE_fusions', 'gene-fusions-6212.14/filtered_fusions_20Q3'), ('CCLE_RNAseq_reads', 'depmap-rnaseq-expression-data-ccd0.25/public_20Q3_counts'), ('CCLE_expression_full', 'depmap-rnaseq-expression-data-ccd0.25/public_20Q3_tpm'), ('Achilles_replicate_map', 'avana-public-tentative-20q3-3e73.5/replicate_map'), ('Achilles_gene_dependency', 'avana-public-tentative-20q3-3e73.5/gene_dependency'), ('Achilles_common_essentials', 'avana-public-tentative-20q3-3e73.5/pan_dependent_genes'), ('Achilles_logfold_change', 'avana-public-tentative-20q3-3e73.5/logfold_change'), ('Achilles_gene_effect', 'avana-public-tentative-20q3-3e73.5/gene_effect')]\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datafile/ad76ca2f1c2f43c1b57c7f84a1adaafe\n",
      "hitting https://cds.team/taiga/api/datasetVersion\n",
      "\n",
      "Dataset version with id f803fe38cdb04895b6285533dcc8b00b created. You can access to this dataset version directly with this url: https://cds.team/taiga/dataset_version/f803fe38cdb04895b6285533dcc8b00b\n"
     ]
    }
   ],
   "source": [
    "# To add to a virtual dataset\n",
    "AddToVirtual(virtual_public, 'depmap-mutation-calls-9a1a', [('CCLE_mutations', 'depmap_'+release+'_mutation_calls'),])#('README','README')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "277px",
    "width": "375px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "198.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "617.8px",
    "left": "1232.4px",
    "right": "20px",
    "top": "120px",
    "width": "262.8px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
