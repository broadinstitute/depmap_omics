# depmap_omics

__have a look at [DepMap](https://www.depmap.org)__

![](documentation/depmap-logo_white.png)

This repository contains code that processes data for the biannual DepMap data release. 

[Here](https://docs.google.com/presentation/d/1i0HI31dBejTYmzI9Cp6Ij7--t6eSR2r9vcC22TxSnNI/edit#slide=id.g525fd14bef_0_116) is an overview of the pipeline and the data it produces.

## Table of Contents
- [Getting Started](#quickstart)
  - [Installation](#installation)
- [Repository File Structure](#file-structure)
- [Running the Pipeline](#running-pipeline)
  - [Uploading and Preprocessing](#upload-preprocess)
  - [Running Terra Pipelines](#running-terra-pipelines)
  - [Downloading and Postprocessing](#downloading-postprocessing)
  - [QC, Groupding and Uploading](#qc-grouping-uploading)
- [Auxiliary Data for the Pipeline](#data)

## Getting Started <a name="quickstart"></a>

The processing pipeline rely on the following tools:

- [python](https://www.learnpython.org/)
- [R](https://www.codecademy.com/learn/learn-r)
- [jupyter](https://jupyter.org/index.html)
- [WDL](https://software.broadinstitute.org/wdl/documentation/)
- [gcp](https://cloud.google.com/sdk/docs/quickstart-macos)
- [docker](https://docs.docker.com/get-started/)
- [Terra](https://software.broadinstitute.org/firecloud/documentation/)
- [dalmatian](https://github.com/broadinstitute/dalmatian)
- [Terra and gcp](https://docs.google.com/document/d/1zTtaN-Px64f8JvgydZNdBbzBpFWyZzEpshSNxQh43Oc/edit#heading=h.dz5wh0l4bu9g)


### Installatiion <a name="installation"></a>

`git clone http://github.com/BroadInstitute/depmap_omics.git && cd depmap_omics`

`pip install -e .`

### :warning: this repository needs other repos

Some important data and code from the [genepy Library](https://github.com/broadinstitute/genepy).

Use the instructions in the genepy page to install the package.

### :warning: you would need the following R python packages

1. You will need to install jupyter notetbooks and google cloud sdk
  - install [Google Cloud SDK](https://cloud.google.com/sdk/docs/downloads-interactive).
  - authenticate my SDK account by running `gcloud auth application-default login` in the terminal, and follow the instrucion to log in.

2. and GSVA for ssGSEA in R `R` run `R -e 'if(!requireNamespace("BiocManager", quietly = TRUE)){install.packages("BiocManager")};BiocManager::install(c("GSEABase", "erccdashboard", "GSVA", "DESeq2"));'`

3. For Python use the requirements.txt file `pip install -r requirements.txt` 

4. Follow instructions [here](documentation/getting_started.md) to set up Terra and obtain access to services required for running the pipeline.

## Repository File Structure <a name="file-structure"></a>

For our 2 computation pipelines for depmap omics:
- Expression and Fusion (RNA)
- Copy number and Mutations (WGS)

Each:
- can be run in a jupyter notebook file,
- gets data from Terra workspace's gcp buckets managed by Broad's Genomics Platform + DevOps (internal only), 
- updates the sample TSVs in the processing workspace on terra with paths to the files, 
- computes the results for each samples by running workflows, 
- downloads the results and postprocesses them with additional local functions,
- performs QC and uploads them to taiga (internal only).

__ccle_tasks/__ Contains a notebook for each of the different additional processing that the CCLE team has to perform as well as one-off tasks run by the omics team

__data/__ Contains important information used for processing, including terra workspace configurations from past quarters

__depmapomics/__ Contains the core python code used in the pipeline and called by the processing jupyter notebooks

__\*\_pipeline/__ Contains some of the pipeline's workflows' wdl files and script files used by these workflows 

__temp/__ Contains the temp file that can get removed after processing (should be empty)

__documentation/__ Contains some additional files and diagrams for documenting the pipelines

__tests/__ Contains automated pytest functions used internally for development

## Pipeline Walkthrough <a name="running-pipeline"></a>

The processing pipeline is divided into four steps: uploading, running Terra pipelines, local postprocessing, and uploading. Here is a detailed walkthrough:

### 1. Uploading and Preprocessing (internal only) <a name="upload-preprocess"></a>

Currently, sequenced data for DepMap is generated by the Genomics Platform (GP) at the Broad who deposit them into several different Terra workspaces. Therefore, the first step of this pipeline is to look at these workspaces and

- identify new samples by looking at the bam files one by one and comparing them with bams we have onboarded
- remove duplicates and ones with broken file paths
- onboard new version of an old cell line's bam if we find any

**Remarks:**
- Although the loading function is for internal use only, external users should feel free to modify and use `createDatasetWithNewCellLines`, `GetNewCellLinesFromWorkspaces` or any other function in `loading.py` for their own needs when loading samples into Terra workspaces.

### 2. Running Terra Pipelines <a name="running-terra-pipelines"></a>

We are using Dalmatian to send requests to Terra, so before running this part, external users need to make sure that the dalmatian `WorkspaceManager` object is initialized with the right workspace and that the functions are taking the correct workflow names as inputs.
You can then run the RNAseq and/or WGS pipelines on your samples. The whole process may take 3-7 days.
For a more detailed documentation on the Terra pipelines, please refer to this [summary of DepMap processing pipeline](documentation/DepMap_processing_pipeline.md).

### 3. Downloading and Postprocessing (often called **on local** in the notebooks) <a name="downloading-postprocessing"></a>

This step will do a set of tasks:
- clean the workspaces by deleting large useless files, including unmapped bams.
- retrieve from the workspace interesting QC results.
- copy realigned bam files to our own data storage bucket.
- download the results.
- remove all duplicate samples from our downloaded file (keeping only the latest version of each sample).

_You would only be interested here at minima in the result downloading_
 
...and postprocessing tasks. The main postprocessing steps for each pipeline are as followed:

#### CN

`copynumbers.py` contains the main postprocessing function (`postProcess()` and their wrappers for internal use) responsible for postprocessing segments and creating gene-level CN files.

#### Mutation

`mutations.py` contains `postProcess()` (and its wrappers for internal use, including one for filtered and one for unfiltered mutation data), which is responsible for postprocessing aggregated MAF files and generating various types of mutation datasets.

#### RNA

`expressions.py` contains the main postprocessing function (`postProcess()` and their wrappers for internal use) responsible for postprocessing aggregated expression data from RSEM, which removes duplicates, renames genes, filters and log transforms entries, and generates protein-level expression data files.

##### Fusion

Functions responsible for postprocessing aggregated fusion data can be found in `fusions.py`. We want to apply filters to the fusion table to reduce the number of artifacts in the dataset. Specifically, we filter the following:

* Remove fusions involving mitochondrial chromosomes, or HLA genes, or immunoglobulin genes
* Remove red herring fusions (from STAR-Fusion annotations column)
* Remove recurrent in CCLE (>= 25 samples)
* Remove fusion with (SpliceType=" INCL_NON_REF_SPLICE" and LargeAnchorSupport="No" and FFPM < 0.1)
* Remove fusions with FFPM < 0.05 (STAR-Fusion suggests using 0.1, but looking at the translocation data, this looks like it might be too aggressive)


**Remarks:**
- in the RNAseq pipeline we have an additional sub-pipeline at the end of the notebook to process the fusion calls from starFusion
- to get the exact same results as in CCLE, be sure to run `genecn = genecn.apply(lambda x: np.log2(1+x))` to the genecn dataframe in the CNV pipeline (present at the end of the validation steps).
- we do not yet have integrated our germline calling in the mutation pipeline but you can still find the haplotypeCaller\|DeepVariant workflows and their parameters


### 4. QC, Grouping and Uploading to the Portal (internal use only) <a name="qc-grouping-uploading"></a>

We then perform the following QC tasks for each dataset. These tasks should not be very interesting for external user as they revolve around manual checks of the data, comparison to previous releases, etc.

#### CN

Once the CN files are saved, we load them back in python and do some validations, in brief:

- mean, max, var...
- to previous version: same mean, max, var...
- checkAmountOfSegments: flag any samples with a very high number of segments
- checkGeneChangeAccrossAll: flag any genes which stay at a similar value across all samples

#### Mutation

__Compare to previous release (broad only)__

We compare the results to the previous releases MAF. Namely:

- Count the total number of mutations per cell line, split by type (SNP, INS, DEL)
- Count the total number of mutations observed by position (group by chromosome, start position, end position and count the number of mutations)
- Look at specific differences between the two MAFs (join on DepMap_ID, Chromosome, Start position, End position, Variant_Type). This is done for WES only


#### RNA

Once the expression files are saved, we do the following validations:
- mean, max, var...
- to previous version: same mean, max, var...
- we QC on the amount of genes with 0 counts for each samples


> After QC, we are also preparing the data to be released to different groups, removing the samples per access category: Blacklist\|Internal\|DepMapConsortium\|Public.

We are then uploading the data to a server called taiga where it will be used in the depmap portal

## Auxiliary Data for the Pipeline <a name="data"></a>


### PONS

for CN pons are made from a set of ~400 normals from the GTEx project as they were sequenced in the same fashion as CCLE samples with the same set of baits. you can see the ID of the samples in `data/samples_for_pons.csv`.
We have created pons for each bait set and using XY only.
We have used workflow from the pipeline:
`gatk/CNV_Somatic_Panel_Workflow`


### targets

The data we are presenting comes from different WES targets/baits/intervals.

We are currently using Illumina ICE intervals and Agilent intervals. you can find their related PON files and interval files as parameters in our workspace files in `data/xQx.json`

__additional auxilliary data is used and listed in some of our workflow, like the CGA pipeline. You will be able to find them by looking at the wdl scripts of each pipelines and looking into the data/xQx.json` files for workspace data files.__


@[jkobject](https://www.jkobject.com)
@gkugener
@gmiller
@5im1z
@__[BroadInsitute](https://www.broadinstitute.org)

If you have any feedback or run into any issues, feel free to post an issue on the github repo.
