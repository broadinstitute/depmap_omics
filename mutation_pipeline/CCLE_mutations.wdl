# This workflow takes an input CRAM to call variants with HaplotypeCaller
# Then filters the calls with the CNNVariant neural net tool
# The site-level scores are added to the INFO field of the VCF.
# The architecture arguments, info_key and tensor type arguments MUST be in agreement
# (e.g. 2D models must have tensor_type of read_tensor and info_key CNN_2D, 1D models have tensor_type reference and info_key CNN_1D)
# The INFO field key will be "1D_CNN" or "2D_CNN" depending on the neural net architecture used for inference.
# The architecture arguments specify pre-trained networks.
# New networks can be trained by the GATK tools: CNNVariantWriteTensors and CNNVariantTrain
# The CRAM could be generated by the single-sample pipeline
# (https://github.com/gatk-workflows/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl)
# Also accepts a BAM as the input file in which case a BAM index is required as well.

import "https://api.firecloud.org/ga4gh/v1/tools/gatk:cnn-variant-common-tasks/versions/1/plain-WDL/descriptor" as CNNTasks

workflow CDS_MutationCalling {
    File tumor_file                  # Aligned CRAM file or Aligned BAM files
    File? tumor_file_index           # Index for an aligned BAM file if that is the input, unneeded if input is a CRAM
    Boolean? tumor_normal_mode
    File? normal_file_index           # Index for an aligned BAM file if that is the input, unneeded if input is a CRAM
    File? normal_file           # Index for an aligned BAM file if that is the input, unneeded if input is a CRAM
    File reference_fasta 
    File reference_dict
    File reference_fasta_index
    File resource_fofn               # File of VCF file names of resources of known SNPs and INDELs, (e.g. mills, gnomAD)
    File resource_fofn_index         # File of VCF file indices of resources
    File? architecture_json          # Neural Net configuration for CNNScoreVariants
    File? architecture_hd5           # Pre-Trained weights and architecture for CNNScoreVariants
    Int? inference_batch_size        # Batch size for python in CNNScoreVariants
    Int? transfer_batch_size         # Batch size for java in CNNScoreVariants
    Int? intra_op_threads            # Tensorflow threading within nodes
    Int? inter_op_threads            # Tensorflow threading between nodes
    String output_prefix             # Identifying string for this run will be used to name all output files
    String? tensor_type              # What kind of tensors the Neural Net expects (e.g. reference, read_tensor)
    String info_key                  # The score key for the info field of the vcf (e.g. CNN_1D, CNN_2D)
    String snp_tranches              # Filtering threshold(s) for SNPs in terms of sensitivity to overlapping known variants in resources
    String indel_tranches            # Filtering threshold(s) for INDELs in terms of sensitivity to overlapping known variants in resources
    File? gatk_override              # GATK Jar file to over ride the one included in gatk_docker
    String gatk_docker
    File calling_intervals
    Int scatter_count                # Number of shards for parallelization of HaplotypeCaller and CNNScoreVariants
    String extra_args                # Extra arguments for HaplotypeCaller

    # VCF format dbSNP file, used to exclude regions around known polymorphisms from analysis by some PROGRAMs;
    # PROGRAMs whose CLP doesn't allow for this argument will quietly ignore it
    File DB_SNP_VCF
    # index file of VCF file of DB SNP variants
    File DB_SNP_VCF_IDX

    # Runtime parameters
    Int? mem_gb
    Int? preemptible_attempts
    Float? disk_space_gb
    Int? cpu

    Boolean tumor_normal_mode = select_first([tumor_normal_mode, false])
    Int? increase_disk_size
    Int additional_disk = select_first([increase_disk_size, 20])
    Float ref_size = size(reference_fasta, "GB") + size(reference_fasta_index, "GB") + size(reference_dict, "GB")
    Int tumor_file_size   = ceil(size(tumor_file,   "GB"))
    Int normal_file_size  = ceil(size(normal_file,  "GB"))
    Int db_snp_vcf_size = ceil(size(DB_SNP_VCF, "GB"))


    # Clunky check to see if the input is a BAM or a CRAM
    if (basename(tumor_file) == basename(tumor_file, ".bam")){
        call CNNTasks.CramToBam {
            input:
              reference_fasta = reference_fasta,
              reference_dict = reference_dict,
              reference_fasta_index = reference_fasta_index,
              cram_file = tumor_file,
              output_prefix = output_prefix,
              
              disk_space_gb = round(4*size(tumor_file, "GB") + ref_size + additional_disk),
              preemptible_attempts = preemptible_attempts
        }
    }

    # Clunky check to see if the input is a BAM or a CRAM
    if tumor_normal_mode {
        if (basename(normal_file) == basename(normal_file, ".bam")){
            call CNNTasks.CramToBam as normalToBam {
                input:
                  reference_fasta = reference_fasta,
                  reference_dict = reference_dict,
                  reference_fasta_index = reference_fasta_index,
                  cram_file = normal_file,
                  output_prefix = output_prefix,
                  
                  disk_space_gb = round(4*size(normal_file, "GB") + ref_size + additional_disk),
                  preemptible_attempts = preemptible_attempts
            }
        }
    }

    call CrossCheckLaneFingerprints_Task {
        input:
            tumorBam=tumorBam,
            normalBam=normalBam,
            tumorBamIdx=tumorBamIdx,
            normalBamIdx=normalBamIdx,
            name=name,
            GATK4_JAR=GATK4_JAR,
            gatk4_jar_size=gatk4_jar_size,
            tumorBam_size=tumorBam_size,
            normalBam_size=normalBam_size,
    }

    call ContEST_Task {
        input:
            tumorBam=tumorBam,
            tumorBamIdx=tumorBamIdx,
            normalBam=normalBam,
            normalBamIdx=normalBamIdx,
            name=name,
            refFasta=refFasta,
            refFastaIdx=refFastaIdx,
            refFastaDict=refFastaDict,
            refFasta_size=refFasta_size,
            targetIntervals=targetIntervals,
            tumorBam_size=tumorBam_size,
            normalBam_size=normalBam_size,
    }

    call PicardMultipleMetrics_Task as tumorMM_Task {
            input:
                bam=tumorBam,
                bamIndex=tumorBamIdx,
                sampleName=caseName,
                refFasta=refFasta,
                DB_SNP_VCF=DB_SNP_VCF,
                DB_SNP_VCF_IDX=DB_SNP_VCF_IDX,
                GATK4_JAR=GATK4_JAR,
                targetIntervals=targetIntervals,
                baitIntervals=baitIntervals,
                refFasta_size=refFasta_size,
                gatk4_jar_size=gatk4_jar_size,
                db_snp_vcf_size=db_snp_vcf_size,
                bam_size=tumorBam_size,
    }

    call SplitIntervals {
        input:
            scatter_count = scatter_count,
            intervals = calling_intervals,
            ref_fasta = reference_fasta,
            ref_dict = reference_dict,
            ref_fai = reference_fasta_index,
            
            gatk_override = gatk_override,
            gatk_docker = gatk_docker,
            disk_space = round(additional_disk + ref_size)
    }

    String input_bam = select_first([CramToBam.output_bam, tumor_file])
    Float bam_size = size(input_bam, "GB")

    scatter (calling_interval in SplitIntervals.interval_files) {
        call CNNTasks.RunHC4 {
            input:
                input_bam = input_bam,
                input_bam_index = select_first([CramToBam.output_bam_index, tumor_file_index]),
                reference_fasta = reference_fasta,
                reference_dict = reference_dict,
                reference_fasta_index = reference_fasta_index,
                output_prefix = output_prefix,
                interval_list = calling_interval,
                
                gatk_docker = gatk_docker,
                gatk_override = gatk_override,
                preemptible_attempts = preemptible_attempts,
                extra_args = extra_args,
                disk_space_gb = round(bam_size + ref_size + additional_disk)
        }

        call CNNTasks.CNNScoreVariants {
            input:
                input_vcf = RunHC4.raw_vcf,
                input_vcf_index = RunHC4.raw_vcf_index,
                bam_file = RunHC4.bamout,
                bam_file_index = RunHC4.bamout_index,
                architecture_json = architecture_json,
                architecture_hd5 = architecture_hd5,
                reference_fasta = reference_fasta,
                tensor_type = tensor_type,
                inference_batch_size = inference_batch_size,
                transfer_batch_size = transfer_batch_size,
                intra_op_threads = intra_op_threads,
                inter_op_threads = inter_op_threads,
                reference_dict = reference_dict,
                reference_fasta_index = reference_fasta_index,               
                output_prefix = output_prefix,
                interval_list = calling_interval,
                
                gatk_override = gatk_override,
                gatk_docker = gatk_docker,
                preemptible_attempts = preemptible_attempts,
                mem_gb = mem_gb,
                disk_space_gb = round((bam_size/scatter_count) + ref_size + additional_disk)
        }
    }

    call CNNTasks.MergeVCFs as MergeVCF_HC4 {
        input: 
            input_vcfs = CNNScoreVariants.cnn_annotated_vcf,
            output_prefix = output_prefix,
            
            gatk_override = gatk_override,
            preemptible_attempts = preemptible_attempts,
            gatk_docker = gatk_docker,
            disk_space_gb = additional_disk
    }

    call CNNTasks.FilterVariantTranches {
        input:
            input_vcf = MergeVCF_HC4.merged_vcf,
            input_vcf_index = MergeVCF_HC4.merged_vcf_index,
            resource_fofn = resource_fofn,
            resource_fofn_index = resource_fofn_index,
            output_prefix = output_prefix,
            snp_tranches = snp_tranches,
            indel_tranches = indel_tranches,
            info_key = info_key,
            
            gatk_override = gatk_override,
            preemptible_attempts = preemptible_attempts,
            gatk_docker = gatk_docker,
            disk_space_gb = additional_disk
    }

    call CNNTasks.SamtoolsMergeBAMs {
        input:
            input_bams = RunHC4.bamout,
            output_prefix = output_prefix,
            disk_space_gb = round(bam_size + ref_size + additional_disk)
    }
    
    call VEP_Task {
        input:
            vcf=vcf
            vcf_id=vcf_id
            pairName=pairName,
            caseName=caseName,
            ctrlName=ctrlName,    
    }

    # Oncotator is a tool for annotating human genomic point mutations and indels with data relevant to cancer researchers.

    call Funcotate{
        input:
            gatk_docker               = gatk_docker,
            ref_fasta                 = ref_fasta,
            ref_fasta_index           = ref_fasta_index,
            ref_dict                  = ref_dict,
            input_vcf                 = variant_vcf_to_funcotate,
            input_vcf_idx             = variant_vcf_to_funcotate_index,
            reference_version         = reference_version,
            output_file_base_name     = output_file_base_name,
            output_format             = output_format,
            compress                  = compress,
            use_gnomad                = use_gnomad,

            interval_list             = interval_list,
            data_sources_tar_gz       = data_sources_tar_gz,
            transcript_selection_mode = transcript_selection_mode,
            transcript_selection_list = transcript_selection_list,
            annotation_defaults       = annotation_defaults,
            annotation_overrides      = annotation_overrides,
            extra_args                = funcotator_extra_args,

            gatk_override             = gatk4_jar_override
    }

    output {
        ####### QC Tasks Outputs #######
        # Copy Number QC Report files
        File tumor_bam_lane_list=CopyNumberReportQC_Task.tumorBamLaneList
        File normal_bam_lane_list=CopyNumberReportQC_Task.normalBamLaneList
        File tumor_bam_read_coverage_lane=CopyNumberReportQC_Task.tumorRCL
        File normal_bam_read_coverage_lane=CopyNumberReportQC_Task.normalRCL
        File copy_number_qc_report=CopyNumberReportQC_Task.CopyNumQCReport
        File copy_number_qc_report_png=CopyNumberReportQC_Task.CopyNumQCReportPNG
        File copy_number_qc_mix_ups=CopyNumberReportQC_Task.CopyNumQCMixUps
        # Picard Multiple Metrics Task - NORMAL BAM
        File? normal_bam_bam_validation=normalMM_Task.bam_validation
        File? normal_bam_alignment_summary_metrics=normalMM_Task.alignment_summary_metrics
        File? normal_bam_bait_bias_detail_metrics=normalMM_Task.bait_bias_detail_metrics
        File? normal_bam_bait_bias_summary_metrics=normalMM_Task.bait_bias_summary_metrics
        File? normal_bam_base_distribution_by_cycle=normalMM_Task.base_distribution_by_cycle
        File? normal_bam_base_distribution_by_cycle_metrics=normalMM_Task.base_distribution_by_cycle_metrics
        File? normal_bam_gc_bias_detail_metrics=normalMM_Task.gc_bias_detail_metrics
        File? normal_bam_gc_bias=normalMM_Task.gc_bias
        File? normal_bam_gc_bias_summary_metrics=normalMM_Task.gc_bias_summary_metrics
        File? normal_bam_insert_size_histogram=normalMM_Task.insert_size_histogram
        File? normal_bam_insert_size_metrics=normalMM_Task.insert_size_metrics        
        File? normal_bam_pre_adapter_detail_metrics=normalMM_Task.pre_adapter_detail_metrics
        File? normal_bam_pre_adapter_summary_metrics=normalMM_Task.pre_adapter_summary_metrics
        File? normal_bam_quality_by_cycle=normalMM_Task.quality_by_cycle
        File? normal_bam_quality_by_cycle_metrics=normalMM_Task.quality_by_cycle_metrics
        File? normal_bam_quality_distribution=normalMM_Task.quality_distribution
        File? normal_bam_quality_distribution_metrics=normalMM_Task.quality_distribution_metrics
        File? normal_bam_quality_yield_metrics=normalMM_Task.quality_yield_metrics
        File? normal_bam_converted_oxog_metrics=normalMM_Task.converted_oxog_metrics
        File? normal_bam_hybrid_selection_metrics=normalMM_Task.hsMetrics
        # Picard Multiple Metrics Task - TUMOR BAM
        File? tumor_bam_bam_validation=tumorMM_Task.bam_validation
        File? tumor_bam_alignment_summary_metrics=tumorMM_Task.alignment_summary_metrics
        File? tumor_bam_bait_bias_detail_metrics=tumorMM_Task.bait_bias_detail_metrics
        File? tumor_bam_bait_bias_summary_metrics=tumorMM_Task.bait_bias_summary_metrics
        File? tumor_bam_base_distribution_by_cycle=tumorMM_Task.base_distribution_by_cycle
        File? tumor_bam_base_distribution_by_cycle_metrics=tumorMM_Task.base_distribution_by_cycle_metrics
        File? tumor_bam_gc_bias_detail_metrics=tumorMM_Task.gc_bias_detail_metrics
        File? tumor_bam_gc_bias=tumorMM_Task.gc_bias
        File? tumor_bam_gc_bias_summary_metrics=tumorMM_Task.gc_bias_summary_metrics
        File? tumor_bam_insert_size_histogram=tumorMM_Task.insert_size_histogram
        File? tumor_bam_insert_size_metrics=tumorMM_Task.insert_size_metrics
        File? tumor_bam_pre_adapter_detail_metrics=tumorMM_Task.pre_adapter_detail_metrics
        File? tumor_bam_pre_adapter_summary_metrics=tumorMM_Task.pre_adapter_summary_metrics
        File? tumor_bam_quality_by_cycle=tumorMM_Task.quality_by_cycle
        File? tumor_bam_quality_by_cycle_metrics=tumorMM_Task.quality_by_cycle_metrics
        File? tumor_bam_quality_distribution=tumorMM_Task.quality_distribution
        File? tumor_bam_quality_distribution_metrics=tumorMM_Task.quality_distribution_metrics
        File? tumor_bam_quality_yield_metrics=tumorMM_Task.quality_yield_metrics
        File? tumor_bam_converted_oxog_metrics=tumorMM_Task.converted_oxog_metrics
        File? tumor_bam_hybrid_selection_metrics=tumorMM_Task.hsMetrics
        # Cross-Sample Contamination Task
        File contamination_data=ContEST_Task.contamDataFile
        File contestAFFile=ContEST_Task.contestAFFile
        File contest_base_report=ContEST_Task.contestBaseReport
        File contest_validation=ContEST_Task.validationOutput
        Float fracContam=ContEST_Task.fracContam
        # Cross Check Lane Fingerprints Task
        File cross_check_fingprt_metrics=CrossCheckLaneFingerprints_Task.crossCheckMetrics
        File cross_check_fingprt_report=CrossCheckLaneFingerprints_Task.crossCheckReport
        Float cross_check_fingprt_min_lod_value=CrossCheckLaneFingerprints_Task.crossCheckMinLODValue
        String cross_check_fingprt_min_lod_lanes=CrossCheckLaneFingerprints_Task.crossCheckMinLODLanes
        ####### Mutation calling #####
        

        ###### Annotated ####
    }
}


task SplitIntervals {
    # inputs
    File? intervals
    File ref_fasta
    File ref_fai
    File ref_dict
    Int scatter_count
    String? split_intervals_extra_args

    File? gatk_override

    # runtime
    String gatk_docker
    Int? mem
    Int? preemptible_attempts
    Int? disk_space
    Int? cpu

    # Mem is in units of GB but our command and memory runtime values are in MB
    Int machine_mem = if defined(mem) then mem * 1000 else 3500
    Int command_mem = machine_mem - 500

    command {
        set -e
        export GATK_LOCAL_JAR=${default="/root/gatk.jar" gatk_override}

        gatk --java-options "-Xmx${command_mem}m" \
            SplitIntervals \
            -R ${ref_fasta} \
            ${"-L " + intervals} \
            -scatter ${scatter_count} \
            -O ./ \
            ${split_intervals_extra_args}
    }

    runtime {
        docker: "${gatk_docker}"
        memory: machine_mem + " MB"
        disks: "local-disk " + select_first([disk_space, 100]) + " HDD"
        preemptible: select_first([preemptible_attempts, 10])
        cpu: select_first([cpu, 1])
        bootDiskSizeGb: "16"
    }

    output {
        Array[File] interval_files = glob("*.interval_list")
    }
}

task VEP_Task {

    # TASK INPUT PARAMS
    File MUTECT1_CS
    File MUTECT2_VCF
    File STRELKA_VCF
    String pairName
    String caseName
    String ctrlName
    File VEP_File    
    File GNOMAD_FILE
    File GNOMAD_FILE_IDX
    File oncoDBTarBall_JustRef

    # RUNTIME INPUT PARAMS
    String preemptible
    String diskGB_boot
    String diskGB_buffer
    String memoryGB
    String cpu

    # DEFAULT VALUES
    String default_cpu = "1"
    String default_memoryGB = "10"
    String default_preemptible = "1"
    String default_diskGB_boot = "15"
    String default_diskGB_buffer = "50"

    # COMPUTE MEMORY SIZE
    Int machine_memoryGB = if memoryGB != "" then memoryGB else default_memoryGB
    Int command_memoryGB = machine_memoryGB - 1

    # COMPUTE DISK SIZE
    Int machine_diskGB_buffer = if diskGB_buffer != "" then diskGB_buffer else default_diskGB_buffer
    Int diskGB = ceil(size(MUTECT1_CS, "G") + size(MUTECT2_VCF, "G") + size(STRELKA_VCF, "G")
                + size(GNOMAD_FILE, "G")*5 + size(GNOMAD_FILE_IDX, "G") + size(VEP_File, "G") * 5 
                + machine_diskGB_buffer)

    parameter_meta {
        MUTECT1_CS : ""
        MUTECT2_VCF : ""
        STRELKA_VCF : ""
        pairName: "a string for the name of the pair under analysis used for naming output files"
        caseName : "tumor sample name, prefix for output"
        ctrlName : "normal sample name, prefix for output"
        VEP_File : ""
        GNOMAD_FILE : ""      
    }

    command <<<

        set -x

        ############## Pre-process MuTect1 call stats #################################
        # Running Oncotator to Convert MuTect1 callstats to VCF 

        MUTECT1_CS_PASSED="${pairName}.MuTect1.call_stats.passed.txt"
        MUTECT1_CS_REJECTED="${pairName}.MuTect1.call_stats.rejected.txt"
        MUTECT1_CS_MAFLITE="${pairName}.MuTect1.call_stats.maflite"
        MUTECT1_VCF="${pairName}.MuTect1.call_stats.maflite.annotated.vcf"

        # Filter MuTect1 mutation calls that passed filter
        python /usr/local/bin/filter_passed_mutations.py ${MUTECT1_CS} $MUTECT1_CS_PASSED $MUTECT1_CS_REJECTED "KEEP"
        # Convert MuTect1 call stats to MafLite
        python /usr/local/bin/callstats_to_maflite.py $MUTECT1_CS_PASSED $MUTECT1_CS_MAFLITE

        ########################### Unzip Oncotator database ###############################

        #find TARBALL type 
        # TODO Find better way to get extension
        TYPE=`echo 'if("${oncoDBTarBall_JustRef}"=~m/z$/) { print "GZ" ; } else { print "TAR" ; } '| perl` ;

        #obtain the name of the directory for oncodb and unpack based on the TYPE
        if [ "$TYPE" == "GZ" ] ; then
        # TODO Find better way to get name of the file
            ONCO_DB_DIR_NAME=`gunzip -c ${oncoDBTarBall_JustRef} |tar -tf /dev/stdin|head -1` ;
            tar -xzf ${oncoDBTarBall_JustRef}
        else
            ONCO_DB_DIR_NAME=`tar -tf ${oncoDBTarBall_JustRef} |head -1` ;
            tar -xf ${oncoDBTarBall_JustRef} ;
        fi ;

        ################## Annotate MuTect1, MuTect2, Strelka call stats ###########################

        # Annotate MuTect1 call stats (MAFLITE to VCF)
        # --infer-onps \        
        /root/oncotator_venv/bin/oncotator \
        -i MAFLITE \
        -o VCF \
        --db-dir `pwd`/$ONCO_DB_DIR_NAME \
        --infer_genotypes yes \
        -a normal_barcode:${ctrlName} \
        -a tumor_barcode:${caseName} \
        $MUTECT1_CS_PASSED $MUTECT1_VCF hg19

        ############### Run Variant Effector Predictor (VEP)  #########################

        #make a link from the home directory to the current directory to avoid running out of disk space on the boot disk
        mkdir -v vep_data_dir
        #delete the existing directory first to make a successful link
        rm -rf ~/.vep
        ln -vs `pwd`/vep_data_dir ~/.vep

        #In either case unpack the data into the home directory where VEP expects to find it
        IS_ZIP=`echo ${VEP_File}|grep -Pic '\.zip$'` ;
        if [ "$IS_ZIP" -eq "1" ] ;
        then
            #it's a zip file
            unzip -d ~ ${VEP_File} ;
        else
            #tar ball
            tar -C ~ -xvzf ${VEP_File}
        fi ;

        # VEP for MuTect1
        MUTECT1_VEP="${pairName}.mutect1.vep_annotated.vcf"
        MUTECT1_VEP_filtered="${pairName}.mutect1.vep_annotated.filtered.vcf"
        /ensembl-tools-release-83/ensembl-tools-release-83/scripts/variant_effect_predictor/variant_effect_predictor.pl \
        --custom ${GNOMAD_FILE},gnomADg,vcf,exact,0,GT,AC,AF,AN,AF_AFR,AF_AMR,AF_ASJ,AF_EAS,AF_FIN,AF_NFE,AF_OTH \
        --tumor_file $MUTECT1_VCF \
        --output_file $MUTECT1_VEP \
        --vcf \
        --symbol \
        --cache \
        --offline \
        --failed 1

        python /usr/local/bin/vep_filter_germline.py $MUTECT1_VEP $MUTECT1_VEP_filtered

        # VEP for MuTect2
        MUTECT2_VEP="${pairName}.mutect2.vep_annotated.vcf"
        MUTECT2_VEP_filtered="${pairName}.mutect2.vep_annotated.filtered.vcf"
        /ensembl-tools-release-83/ensembl-tools-release-83/scripts/variant_effect_predictor/variant_effect_predictor.pl \
        --custom ${GNOMAD_FILE},gnomADg,vcf,exact,0,GT,AC,AF,AN,AF_AFR,AF_AMR,AF_ASJ,AF_EAS,AF_FIN,AF_NFE,AF_OTH \
        --tumor_file ${MUTECT2_VCF} \
        --output_file $MUTECT2_VEP \
        --vcf \
        --symbol \
        --cache \
        --offline \
        --failed 1

        python /usr/local/bin/vep_filter_germline.py $MUTECT2_VEP $MUTECT2_VEP_filtered

        # VEP for Strelka
        STRELKA_VEP="${pairName}.strelka.vep_annotated.vcf"
        STRELKA_VEP_filtered="${pairName}.strelka.vep_annotated.filtered.vcf"
        /ensembl-tools-release-83/ensembl-tools-release-83/scripts/variant_effect_predictor/variant_effect_predictor.pl \
        --custom ${GNOMAD_FILE},gnomADg,vcf,exact,0,GT,AC,AF,AN,AF_AFR,AF_AMR,AF_ASJ,AF_EAS,AF_FIN,AF_NFE,AF_OTH \
        --tumor_file ${STRELKA_VCF} \
        --output_file $STRELKA_VEP \
        --vcf \
        --symbol \
        --cache \
        --offline \
        --failed 1

        python /usr/local/bin/vep_filter_germline.py $STRELKA_VEP $STRELKA_VEP_filtered

    >>>

    runtime {
        docker: "gcr.io/broad-getzlab-workflows/cga_production_pipeline:v0.2.ccle"
        bootDiskSizeGb: if diskGB_boot != "" then diskGB_boot else default_diskGB_boot
        preemptible: if preemptible != "" then preemptible else default_preemptible
        cpu: if cpu != "" then cpu else default_cpu
        disks: "local-disk ${diskGB} HDD"
        memory: machine_memoryGB + "GB"
    }

    output {
        File MUTECT1_VEP_annotated_vcf="${pairName}.mutect1.vep_annotated.vcf"
        File MUTECT2_VEP_annotated_vcf="${pairName}.mutect2.vep_annotated.vcf"
        File STRELKA_VEP_annotated_vcf="${pairName}.strelka.vep_annotated.vcf"  
        File MUTECT1_VEP_annotated_filtered_vcf="${pairName}.mutect1.vep_annotated.filtered.vcf"
        File MUTECT2_VEP_annotated_filtered_vcf="${pairName}.mutect2.vep_annotated.filtered.vcf"
        File STRELKA_VEP_annotated_filtered_vcf="${pairName}.strelka.vep_annotated.filtered.vcf"       
    }
}

task PicardMultipleMetrics_Task {

    # TASK INPUT PARAMS
    File bam
    File bamIndex
    String sampleName
    File targetIntervals
    File baitIntervals
    File refFasta
    File DB_SNP_VCF
    File DB_SNP_VCF_IDX
    File GATK4_JAR

    String validationStringencyLevel
    String run_clean_sam

    # FILE SIZE
    Int bam_size
    Int refFasta_size
    Int gatk4_jar_size
    Int db_snp_vcf_size

    # RUNTIME INPUT PARAMS
    String preemptible
    String diskGB_boot
    String diskGB_buffer
    String memoryGB
    String cpu

    # DEFAULT VALUES
    String default_cpu = "1"
    String default_memoryGB = "10"
    String default_preemptible = "1"
    String default_diskGB_boot = "15"
    String default_diskGB_buffer = "20"
    String default_stringencyLevel = "LENIENT"
    String default_run_clean_sam = false

    # COMPUTE MEMORY SIZE
    Int machine_memoryGB = if memoryGB != "" then memoryGB else default_memoryGB
    Int command_memoryGB = machine_memoryGB - 1
    
    # COMPUTE DISK SIZE
    Int machine_diskGB_buffer = if diskGB_buffer != "" then diskGB_buffer else default_diskGB_buffer
    Int diskGB = ceil(bam_size + refFasta_size + gatk4_jar_size + db_snp_vcf_size + machine_diskGB_buffer)

    String stringencyLevel = if validationStringencyLevel != "" then validationStringencyLevel else default_stringencyLevel
    String clean_sam_flag = if run_clean_sam != "" then run_clean_sam else default_run_clean_sam

    parameter_meta {
        bam : "sample (normal or tumor) BAM file"
        bamIndex : "sample (normal or tumor) BAI file (BAM indexed)"
        sampleName : "sample (normal or tumor) name, prefix for output"
        refFasta : "FASTA file for the appropriate genome build (Reference sequence file)"
        DB_SNP_VCF : "VCF format dbSNP file, used to exclude regions around known polymorphisms from analysis by some PROGRAMs"
        DB_SNP_VCF_IDX : "dbSNP indexed file"
    }

    command <<<

        set -euxo pipefail

        /usr/local/jre1.8.0_73/bin/java "-Xmx${command_memoryGB}g" -jar ${GATK4_JAR} ValidateSamFile \
        --INPUT ${bam} \
        --OUTPUT "${sampleName}.bam_validation" \
        --MODE VERBOSE \
        --IGNORE_WARNINGS true \
        --REFERENCE_SEQUENCE ${refFasta} \
        --VALIDATION_STRINGENCY ${stringencyLevel}

        if [ "${clean_sam_flag}" = true ] ;
        then
            # Run bam through CleanSam to set MAPQ of unmapped reads to zero
            /usr/local/jre1.8.0_73/bin/java "-Xmx${command_memoryGB}g" -jar ${GATK4_JAR} CleanSam \
            --INPUT ${bam} \
            --OUTPUT ${sampleName}.unmapped_reads_cleaned.bam
        fi

        /usr/local/jre1.8.0_73/bin/java "-Xmx${command_memoryGB}g" -jar ${GATK4_JAR} CollectMultipleMetrics \
        --INPUT ${bam} \
        --OUTPUT ${sampleName}.multiple_metrics \
        --REFERENCE_SEQUENCE ${refFasta} \
        --DB_SNP ${DB_SNP_VCF} \
        --PROGRAM CollectAlignmentSummaryMetrics \
        --PROGRAM CollectInsertSizeMetrics \
        --PROGRAM QualityScoreDistribution \
        --PROGRAM MeanQualityByCycle \
        --PROGRAM CollectBaseDistributionByCycle \
        --PROGRAM CollectSequencingArtifactMetrics \
        --PROGRAM CollectQualityYieldMetrics \
        --PROGRAM CollectGcBiasMetrics
        
        #Extract OxoG metrics from generalized artifacts metrics. 
        # This tool extracts 8-oxoguanine (OxoG) artifact metrics from the output of CollectSequencingArtifactsMetrics 
        # (a tool that provides detailed information on a variety of
        # artifacts found in sequencing libraries) and converts them to the CollectOxoGMetrics tool's output format. This
        # conveniently eliminates the need to run CollectOxoGMetrics if we already ran CollectSequencingArtifactsMetrics in our
        # pipeline.
        /usr/local/jre1.8.0_73/bin/java -jar ${GATK4_JAR} ConvertSequencingArtifactToOxoG \
        --INPUT_BASE "${sampleName}.multiple_metrics" \
        --OUTPUT_BASE "${sampleName}.multiple_metrics.converted" \
        --REFERENCE_SEQUENCE ${refFasta} \
        --VALIDATION_STRINGENCY ${stringencyLevel}

        #zip up reports for QC Nozzle report
        zip picard_multiple_metrics.zip ${sampleName}.multiple_metrics.*

        # Collect WES HS metrics
        /usr/local/jre1.8.0_73/bin/java "-Xmx${command_memoryGB}g" -jar ${GATK4_JAR} CollectHsMetrics \
        --INPUT ${bam} \
        --BAIT_INTERVALS ${targetIntervals} \
        --TARGET_INTERVALS ${baitIntervals} \
        --OUTPUT "${sampleName}.HSMetrics.txt" \
        --VALIDATION_STRINGENCY ${stringencyLevel}

    >>>

    runtime {
        docker: "gcr.io/broad-getzlab-workflows/cga_production_pipeline:v0.2.ccle"
        bootDiskSizeGb: if diskGB_boot != "" then diskGB_boot else default_diskGB_boot
        preemptible: if preemptible != "" then preemptible else default_preemptible
        cpu: if cpu != "" then cpu else default_cpu
        disks: "local-disk ${diskGB} HDD"
        memory: machine_memoryGB + "GB"
    }

    output {
        File bam_validation="${sampleName}.bam_validation"
        File metricsReportsZip="picard_multiple_metrics.zip"
        File alignment_summary_metrics="${sampleName}.multiple_metrics.alignment_summary_metrics"
        File bait_bias_detail_metrics="${sampleName}.multiple_metrics.bait_bias_detail_metrics"
        File bait_bias_summary_metrics="${sampleName}.multiple_metrics.bait_bias_summary_metrics"
        File base_distribution_by_cycle="${sampleName}.multiple_metrics.base_distribution_by_cycle.pdf"
        File base_distribution_by_cycle_metrics="${sampleName}.multiple_metrics.base_distribution_by_cycle_metrics"
        File gc_bias_detail_metrics="${sampleName}.multiple_metrics.gc_bias.detail_metrics"
        File gc_bias="${sampleName}.multiple_metrics.gc_bias.pdf"
        File gc_bias_summary_metrics="${sampleName}.multiple_metrics.gc_bias.summary_metrics"
        File insert_size_histogram="${sampleName}.multiple_metrics.insert_size_histogram.pdf"
        File insert_size_metrics="${sampleName}.multiple_metrics.insert_size_metrics"
        #File oxog_metrics="${sampleName}.multiple_metrics.oxog_metrics"
        File pre_adapter_detail_metrics="${sampleName}.multiple_metrics.pre_adapter_detail_metrics"
        File pre_adapter_summary_metrics="${sampleName}.multiple_metrics.pre_adapter_summary_metrics"
        File quality_by_cycle="${sampleName}.multiple_metrics.quality_by_cycle.pdf"
        File quality_by_cycle_metrics="${sampleName}.multiple_metrics.quality_by_cycle_metrics"
        File quality_distribution="${sampleName}.multiple_metrics.quality_distribution.pdf"
        File quality_distribution_metrics="${sampleName}.multiple_metrics.quality_distribution_metrics"
        File quality_yield_metrics="${sampleName}.multiple_metrics.quality_yield_metrics"
        File converted_oxog_metrics="${sampleName}.multiple_metrics.converted.oxog_metrics"
        File hsMetrics="${sampleName}.HSMetrics.txt"
    }
}

task CrossCheckLaneFingerprints_Task {

    # TASK INPUT PARAMS
    File tumorBam
    File normalBam
    File tumorBamIdx
    File normalBamIdx
    String pairName
    File HaplotypeDBForCrossCheck
    File GATK4_JAR

    String validationStringencyLevel

    # FILE SIZE
    Int tumorBam_size
    Int normalBam_size
    Int gatk4_jar_size

    # RUNTIME INPUT PARAMS
    String preemptible
    String diskGB_boot
    String diskGB_buffer
    String memoryGB
    String cpu

    # DEFAULT VALUES
    String default_cpu = "1"
    String default_memoryGB = "3"
    String default_preemptible = "1"
    String default_diskGB_boot = "15"
    String default_diskGB_buffer = "20"
    String default_stringencyLevel = "LENIENT"

    # COMPUTE MEMORY SIZE
    Int machine_memoryGB = if memoryGB != "" then memoryGB else default_memoryGB
    Int command_memoryGB = machine_memoryGB - 1
    
    # COMPUTE DISK SIZE
    Int machine_diskGB_buffer = if diskGB_buffer != "" then diskGB_buffer else default_diskGB_buffer
    Int diskGB = ceil(tumorBam_size + normalBam_size + gatk4_jar_size + size(HaplotypeDBForCrossCheck, "G") 
                    + machine_diskGB_buffer)

    String stringencyLevel = if validationStringencyLevel != "" then validationStringencyLevel else default_stringencyLevel

    parameter_meta {
        tumorBam : "sample tumor BAM file"
        tumorBamIdx : "sample tumor BAI file (indexed BAM file)"
        normalBam : "sample normal BAM file"
        normalBamIdx : "sample normal BAI file (indexed BAM file)"
        pairName : "a string for the name of the pair under analysis used for naming output files"
        HaplotypeDBForCrossCheck : ""
        validationStringencyLevel : ""
    }

    command <<<

        set -euxo pipefail

        #drop from haplotypeDB seq entries which aren't in BAM if there are any found
        PREPPED_HAPLOTYPE_DB=PreppedHaplotypeDB.txt
        /usr/local/bin/filter_not_in_bam_dict.pl ${normalBam} ${HaplotypeDBForCrossCheck} $PREPPED_HAPLOTYPE_DB

        #CrosscheckLaneFingerprints[version=9]
        mkdir -v tmp
        /usr/local/jre1.8.0_73/bin/java "-Xmx${command_memoryGB}g" -jar ${GATK4_JAR} CrosscheckFingerprints \
        -I ${tumorBam} \
        -I ${normalBam} \
        -H $PREPPED_HAPLOTYPE_DB \
        --TMP_DIR `pwd`/tmp \
        --QUIET false \
        --EXIT_CODE_WHEN_MISMATCH 0 \
        --OUTPUT crosscheck.metrics \
        --VALIDATION_STRINGENCY ${stringencyLevel} 

        #Produce crosscheck.stats.txt file for making the html report
        grep -v "#" crosscheck.metrics | sed 1d > crosscheck.metrics.stripped
        
        python /usr/local/bin/crosscheck_report.py crosscheck.metrics.stripped

    >>>

    runtime {
        docker: "gcr.io/broad-getzlab-workflows/cga_production_pipeline:v0.2.ccle"
        bootDiskSizeGb: if diskGB_boot != "" then diskGB_boot else default_diskGB_boot
        preemptible: if preemptible != "" then preemptible else default_preemptible
        cpu: if cpu != "" then cpu else default_cpu
        disks: "local-disk ${diskGB} HDD"
        memory: machine_memoryGB + "GB"
    }

    output {
        File crossCheckMetrics="crosscheck.metrics"
        File crossCheckReport="report.html"
        Float crossCheckMinLODValue=read_float("crosscheck_min_lod_value.txt")
        String crossCheckMinLODLanes=read_string("crosscheck_min_lod_lanes.txt")
    }
}

task ContEST_Task {
        
    # TASK INPUT PARAMS
    File tumorBam
    File tumorBamIdx
    File normalBam
    File normalBamIdx
    File refFasta
    File refFastaIdx
    File refFastaDict
    File targetIntervals
    
    File SNP6Bed
    File HapMapVCF
    String pairName

    # FILE SIZE
    Int tumorBam_size
    Int normalBam_size
    Int refFasta_size

    # RUNTIME INPUT PARAMS
    String preemptible
    String diskGB_boot
    String diskGB_buffer
    String memoryGB
    String cpu

    # DEFAULT VALUES
    String default_cpu = "1"
    String default_memoryGB = "10"
    String default_preemptible = "1"
    String default_diskGB_boot = "15"
    String default_diskGB_buffer = "20"

    # COMPUTE MEMORY SIZE
    Int machine_memoryGB = if memoryGB != "" then memoryGB else default_memoryGB
    Int command_memoryGB = machine_memoryGB - 1
    
    # COMPUTE DISK SIZE
    Int machine_diskGB_buffer = if diskGB_buffer != "" then diskGB_buffer else default_diskGB_buffer
    Int diskGB = ceil(tumorBam_size + normalBam_size + refFasta_size
                + size(targetIntervals, "G") + size(SNP6Bed, "G") + size(HapMapVCF, "G")
                + machine_diskGB_buffer)

    parameter_meta {
        tumorBam : "sample tumor BAM file"
        tumorBamIdx : "sample tumor BAI file (indexed BAM file)"
        normalBam : "sample normal BAM file"
        normalBamIdx : "sample normal BAI file (indexed BAM file)"
        pairName : "sample name, prefix for output"
        refFasta : "the FASTA file for the appropriate genome build (Reference sequence file)"
        refFastaIdx : "FASTA file index for the reference genome"
        refFastaDict : "FASTA file dictionary for the reference genome"
        exomeIntervals : ""
        SNP6Bed : ""
        HapMapVCF : "the population allele frequencies for each SNP in HapMap"
    }

    command <<<

        set -euxo pipefail
         
        java "-Xmx${command_memoryGB}g" -Djava.io.tmpdir=/tmp -jar /usr/local/bin/GenomeAnalysisTK.jar \
        -T ContEst \
        -I:eval ${tumorBam} \
        -I:genotype ${normalBam} \
        -L ${targetIntervals} \
        -L ${SNP6Bed} \
        -isr INTERSECTION \
        -R ${refFasta} \
        -l INFO \
        -pf ${HapMapVCF} \
        -o contamination.af.txt \
        --trim_fraction 0.03 \
        --beta_threshold 0.05 \
        -br contamination.base_report.txt \
        -mbc 100 \
        --min_genotype_depth 30 \
        --min_genotype_ratio 0.8

        python /usr/local/bin/extract_contamination.py contamination.af.txt fraction_contamination.txt \
        contamination_validation.array_free.txt ${pairName}

        #Contamination validation/consensus
        python /usr/local/populateConsensusContamination_v26/contaminationConsensus.py \
        --pass_snp_qc false \
        --output contest_validation.output.tsv \
        --annotation contamination_percentage_consensus_capture \
        --array contamination_validation.array_free.txt \
        --noarray contamination_validation.array_free.txt

    >>>

    runtime {
        docker: "gcr.io/broad-getzlab-workflows/cga_production_pipeline:v0.2.ccle"
        bootDiskSizeGb: if diskGB_boot != "" then diskGB_boot else default_diskGB_boot
        preemptible: if preemptible != "" then preemptible else default_preemptible
        cpu: if cpu != "" then cpu else default_cpu
        disks: "local-disk ${diskGB} HDD"
        memory: machine_memoryGB + "GB"
    }

    output {
        File contamDataFile="contamination_validation.array_free.txt"
        File contestAFFile="contamination.af.txt"
        File contestBaseReport="contamination.base_report.txt"
        File validationOutput="contest_validation.output.tsv"
        Float fracContam=read_float("fraction_contamination.txt")
    }
}